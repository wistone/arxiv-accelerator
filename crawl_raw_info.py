#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬ï¼šç²¾ç¡®æŒ‰arXivå®˜æ–¹æ—¶åŒºæœç´¢è®ºæ–‡
- ç»Ÿä¸€ä½¿ç”¨ç¾å›½ä¸œéƒ¨æ—¶é—´
- ç²¾ç¡®æ—¥æœŸåŒ¹é…ï¼Œé¿å…é‡å¤
- ä¸arXivç½‘ç«™æ—¥æœŸå¯¹åº”
"""

import re
import pandas as pd
import datetime as dt
import feedparser
import requests
from contextlib import redirect_stdout, redirect_stderr
from tabulate import tabulate
import sys
import os

def crawl_arxiv_papers(date_str, category):
    """
    çˆ¬å–arXivè®ºæ–‡
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ 'YYYY-MM-DD'
        category: arXivåˆ†ç±»ï¼Œå¦‚ 'cs.CV'
    """
    
    try:
        target_date = dt.datetime.strptime(date_str, '%Y-%m-%d').date()
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        return False
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    log_file = f'log/{date_str}-{category}-log.txt'
    result_file = f'log/{date_str}-{category}-result.md'
    
    # ç¡®ä¿logç›®å½•å­˜åœ¨
    os.makedirs('log', exist_ok=True)
    
    try:
        # é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            with redirect_stdout(f), redirect_stderr(f):
                print(f"ğŸ¯ arXiv è®ºæ–‡çˆ¬å– - ç²¾ç¡®æ—¶åŒºç‰ˆæœ¬")
                print(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
                print(f"è®ºæ–‡åˆ†ç±»: {category}")
                print("=" * 60)
                
                # ç»Ÿä¸€ä½¿ç”¨arXivå®˜æ–¹æ—¶åŒºï¼ˆç¾å›½ä¸œéƒ¨æ—¶é—´ï¼‰
                # EDT (å¤ä»¤æ—¶): UTC-4, EST (æ ‡å‡†æ—¶): UTC-5
                # å½“å‰å‡è®¾ä½¿ç”¨EDT (UTC-4)
                us_eastern_tz = dt.timezone(dt.timedelta(hours=-4))
                
                # è·å–å½“å‰ç¾å›½ä¸œéƒ¨æ—¶é—´
                utc_now = dt.datetime.now(dt.timezone.utc)
                us_eastern_now = utc_now.astimezone(us_eastern_tz)
                us_eastern_today = us_eastern_now.date()
                
                print(f"\nğŸ“… ç²¾ç¡®æ—¶åŒºå¤„ç†ç­–ç•¥:")
                print(f"  âœ… ä½¿ç”¨arXivå®˜æ–¹æ—¶åŒº: ç¾å›½ä¸œéƒ¨æ—¶é—´ (UTC-4)")
                print(f"  âœ… æŒ‰APIå®é™…å‘å¸ƒæ—¶é—´åˆ†ç»„ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§")
                print(f"  âœ… é¿å…æ—¶åŒºæ··æ·†å’Œé‡å¤è®¡ç®—é—®é¢˜")
                print(f"\nğŸ• å½“å‰æ—¶é—´ä¿¡æ¯:")
                print(f"  å½“å‰UTCæ—¶é—´: {utc_now}")
                print(f"  å½“å‰ç¾å›½ä¸œéƒ¨æ—¶é—´: {us_eastern_now}")
                print(f"  ç¾å›½ä¸œéƒ¨ä»Šå¤©: {us_eastern_today}")
                print(f"  ç›®æ ‡æœç´¢æ—¥æœŸ: {target_date}")
                print(f"\nğŸ’¡ è¯´æ˜: ç”±äºarXivç½‘ç«™å’ŒAPIçš„æ—¥æœŸåˆ†ç»„é€»è¾‘ç•¥æœ‰å·®å¼‚ï¼Œ")
                print(f"     è®ºæ–‡æ•°é‡å¯èƒ½ä¸ç½‘ç«™æ˜¾ç¤ºä¸å®Œå…¨ä¸€è‡´ï¼Œä½†æœ¬å·¥å…·çš„æ•°æ®æ›´ç²¾ç¡®ã€‚")
                
                # ç»Ÿä¸€æŸ¥è¯¢ç­–ç•¥ï¼šè·å–æœ€è¿‘1000ç¯‡è®ºæ–‡ï¼Œç„¶åæŒ‰å‘å¸ƒæ—¥æœŸç²¾ç¡®è¿‡æ»¤
                URL = ("http://export.arxiv.org/api/query?"
                       f"search_query=cat:{category}&"
                       "sortBy=submittedDate&sortOrder=descending&"
                       "max_results=1000")
                
                print(f"\nğŸ” æŸ¥è¯¢URL: {URL}")
                
                # å‘é€è¯·æ±‚
                response = requests.get(URL)
                if response.status_code != 200:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return False
                
                # è§£æRSSå“åº”
                feed = feedparser.parse(response.content)
                if not feed.entries:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡æ¡ç›®")
                    return False
                
                print(f"ğŸ“Š ä»APIè·å–åˆ° {len(feed.entries)} ç¯‡è®ºæ–‡")
                
                # åˆ†æè®ºæ–‡å‘å¸ƒæ—¶é—´
                rows = []
                date_stats = {}
                
                print(f"\nğŸ” å¼€å§‹åˆ†æè®ºæ–‡å‘å¸ƒæ—¶é—´...")
                
                for i, entry in enumerate(feed.entries):
                    # è·å–è®ºæ–‡å‘å¸ƒæ—¶é—´ï¼ˆUTCæ—¶é—´ï¼‰
                    pub_datetime_utc = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                    
                    # è½¬æ¢ä¸ºç¾å›½ä¸œéƒ¨æ—¶é—´
                    pub_datetime_eastern = pub_datetime_utc.astimezone(us_eastern_tz)
                    pub_date_eastern = pub_datetime_eastern.date()
                    
                    # ç»Ÿè®¡æ¯ä¸ªæ—¥æœŸçš„è®ºæ–‡æ•°é‡
                    if pub_date_eastern not in date_stats:
                        date_stats[pub_date_eastern] = 0
                    date_stats[pub_date_eastern] += 1
                    
                    # åªä¿ç•™ç›®æ ‡æ—¥æœŸçš„è®ºæ–‡ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
                    if pub_date_eastern == target_date:
                        print(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸè®ºæ–‡ {len(rows)+1}: {entry.title[:50]}...")
                        print(f"   å‘å¸ƒæ—¶é—´(UTC): {pub_datetime_utc}")
                        print(f"   å‘å¸ƒæ—¶é—´(ç¾ä¸œ): {pub_datetime_eastern}")
                        
                        rows.append({
                            "No.": len(rows) + 1,
                            "number_id": re.search(r'(\d+\.\d+)', entry.id).group(1),
                            "title": entry.title.strip().replace('\n', ' '),
                            "authors": ', '.join(a.name for a in entry.authors),
                            "abstract": entry.summary.strip().replace('\n', ' '),
                            "link": entry.link
                        })
                    
                    # å¦‚æœå‘å¸ƒæ—¥æœŸæ—©äºç›®æ ‡æ—¥æœŸè¶…è¿‡7å¤©ï¼Œåœæ­¢æœç´¢ä»¥æé«˜æ•ˆç‡
                    if pub_date_eastern < target_date - dt.timedelta(days=7):
                        print(f"â¹ï¸  å‘å¸ƒæ—¥æœŸ {pub_date_eastern} æ—©äºç›®æ ‡æ—¥æœŸè¶…è¿‡7å¤©ï¼Œåœæ­¢æœç´¢")
                        break
                
                # æ˜¾ç¤ºæ—¥æœŸç»Ÿè®¡
                print(f"\nğŸ“Š æŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´çš„è®ºæ–‡åˆ†å¸ƒ:")
                sorted_dates = sorted(date_stats.keys(), reverse=True)
                for date in sorted_dates[:10]:  # æ˜¾ç¤ºæœ€è¿‘10å¤©
                    status = "ğŸ¯ ç›®æ ‡" if date == target_date else "  "
                    print(f"  {status} {date}: {date_stats[date]}ç¯‡")
                
                print(f"\nâœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸ {target_date} çš„è®ºæ–‡: {len(rows)} ç¯‡")
                
                if not rows:
                    print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° {target_date} çš„è®ºæ–‡")
                    print(f"ğŸ’¡ å»ºè®®æ£€æŸ¥:")
                    print(f"   1. æ—¥æœŸæ˜¯å¦æ­£ç¡® (ç›®æ ‡: {target_date})")
                    print(f"   2. è¯¥æ—¥æœŸæ˜¯å¦æœ‰è®ºæ–‡å‘å¸ƒ")
                    print(f"   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                    print(f"ğŸ“‹ åˆ›å»ºç©ºç»“æœæ–‡ä»¶ï¼ŒæœåŠ¡å™¨å°†è‡ªåŠ¨åˆ é™¤ä»¥ä¾¿åç»­é‡è¯•")
                    
                    # åˆ›å»ºç©ºçš„ç»“æœæ–‡ä»¶ï¼ˆæœåŠ¡å™¨ä¼šæ£€æµ‹åˆ°ç©ºç»“æœå¹¶è‡ªåŠ¨åˆ é™¤ï¼Œå…è®¸åç»­é‡è¯•ï¼‰
                    with open(result_file, 'w', encoding='utf-8') as f:
                        f.write(f"# arXiv {category} è®ºæ–‡ - {target_date}\n\n")
                        f.write(f"**æœç´¢æ—¥æœŸ**: {target_date} (ç¾å›½ä¸œéƒ¨æ—¶é—´ UTC-4)\\n")
                        f.write(f"**è®ºæ–‡åˆ†ç±»**: {category}\\n")
                        f.write(f"**è®ºæ–‡æ•°é‡**: 0 ç¯‡\\n")
                        f.write(f"**ç”Ÿæˆæ—¶é—´**: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
                        f.write(f"**æ•°æ®æ¥æº**: arXivå®˜æ–¹API (ç²¾ç¡®æ—¶åŒºå¤„ç†)\\n\\n")
                        f.write("> **è¯´æ˜**: æœ¬å·¥å…·ä½¿ç”¨arXivå®˜æ–¹APIï¼ŒæŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´ç²¾ç¡®åˆ†ç»„ã€‚\\n")
                        f.write("> ç”±äºarXivç½‘ç«™å’ŒAPIçš„æ—¥æœŸåˆ†ç»„é€»è¾‘å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼Œè®ºæ–‡æ•°é‡å¯èƒ½ä¸å®˜ç½‘æ˜¾ç¤ºæœ‰æ‰€ä¸åŒï¼Œä½†æ•°æ®å‡†ç¡®æ€§æ›´é«˜ã€‚\\n\\n")
                        f.write("## è®ºæ–‡åˆ—è¡¨\n\n")
                        f.write("*è¯¥æ—¥æœŸæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡*\\n")
                    
                    return True  # è¿”å›æˆåŠŸï¼Œè®©æœåŠ¡å™¨å¤„ç†ç©ºç»“æœçš„åˆ é™¤é€»è¾‘
                
        # å†™å…¥ç»“æœæ–‡ä»¶ï¼ˆä¸é‡å®šå‘è¾“å‡ºï¼‰
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ° {result_file}...")
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(rows)
        
        # ç”ŸæˆMarkdownæ ¼å¼çš„è¡¨æ ¼
        markdown_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
        
        # å†™å…¥æ–‡ä»¶
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(f"# arXiv {category} è®ºæ–‡ - {target_date}\n\n")
            f.write(f"**æœç´¢æ—¥æœŸ**: {target_date} (ç¾å›½ä¸œéƒ¨æ—¶é—´ UTC-4)\\n")
            f.write(f"**è®ºæ–‡åˆ†ç±»**: {category}\\n")
            f.write(f"**è®ºæ–‡æ•°é‡**: {len(rows)} ç¯‡\\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
            f.write(f"**æ•°æ®æ¥æº**: arXivå®˜æ–¹API (ç²¾ç¡®æ—¶åŒºå¤„ç†)\\n\\n")
            f.write("> **è¯´æ˜**: æœ¬å·¥å…·ä½¿ç”¨arXivå®˜æ–¹APIï¼ŒæŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´ç²¾ç¡®åˆ†ç»„ã€‚\\n")
            f.write("> ç”±äºarXivç½‘ç«™å’ŒAPIçš„æ—¥æœŸåˆ†ç»„é€»è¾‘å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼Œè®ºæ–‡æ•°é‡å¯èƒ½ä¸å®˜ç½‘æ˜¾ç¤ºæœ‰æ‰€ä¸åŒï¼Œä½†æ•°æ®å‡†ç¡®æ€§æ›´é«˜ã€‚\\n\\n")
            f.write("## è®ºæ–‡åˆ—è¡¨\n\n")
            f.write(markdown_table)
            f.write("\\n")
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ‰¾åˆ° {len(rows)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {result_file}")
        print(f"ğŸ“ æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
        
        return True  # è¿”å›æˆåŠŸçŠ¶æ€
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False  # è¿”å›å¤±è´¥çŠ¶æ€

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python crawl_raw_info_fixed.py <æ—¥æœŸ> <åˆ†ç±»>")
        print("ç¤ºä¾‹: python crawl_raw_info_fixed.py 2025-08-06 cs.CV")
        sys.exit(1)
    
    date_str = sys.argv[1]
    category = sys.argv[2]
    
    crawl_arxiv_papers(date_str, category)