#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬ï¼šç²¾ç¡®æŒ‰arXivå®˜æ–¹æ—¶åŒºæœç´¢è®ºæ–‡
- ç»Ÿä¸€ä½¿ç”¨ç¾å›½ä¸œéƒ¨æ—¶é—´
- ç²¾ç¡®æ—¥æœŸåŒ¹é…ï¼Œé¿å…é‡å¤
- ä¸arXivç½‘ç«™æ—¥æœŸå¯¹åº”
"""

import re
import pandas as pd
import datetime as dt
import feedparser
import requests
from contextlib import redirect_stdout, redirect_stderr
from tabulate import tabulate
import sys
import os

def crawl_arxiv_papers(date_str, category):
    """
    çˆ¬å–arXivè®ºæ–‡
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ 'YYYY-MM-DD'
        category: arXivåˆ†ç±»ï¼Œå¦‚ 'cs.CV'
    """
    
    try:
        target_date = dt.datetime.strptime(date_str, '%Y-%m-%d').date()
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        return False
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    log_file = f'log/{date_str}-{category}-log.txt'
    result_file = f'log/{date_str}-{category}-result.md'
    
    # ç¡®ä¿logç›®å½•å­˜åœ¨
    os.makedirs('log', exist_ok=True)
    
    try:
        # é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            with redirect_stdout(f), redirect_stderr(f):
                print(f"ğŸ¯ arXiv è®ºæ–‡çˆ¬å– - ç²¾ç¡®æ—¶åŒºç‰ˆæœ¬")
                print(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
                print(f"è®ºæ–‡åˆ†ç±»: {category}")
                print("=" * 60)
                
                # ç»Ÿä¸€ä½¿ç”¨arXivå®˜æ–¹æ—¶åŒºï¼ˆç¾å›½ä¸œéƒ¨æ—¶é—´ï¼‰
                # EDT (å¤ä»¤æ—¶): UTC-4, EST (æ ‡å‡†æ—¶): UTC-5
                # å½“å‰å‡è®¾ä½¿ç”¨EDT (UTC-4)
                us_eastern_tz = dt.timezone(dt.timedelta(hours=-4))
                
                # è·å–å½“å‰ç¾å›½ä¸œéƒ¨æ—¶é—´
                utc_now = dt.datetime.now(dt.timezone.utc)
                us_eastern_now = utc_now.astimezone(us_eastern_tz)
                us_eastern_today = us_eastern_now.date()
                
                print(f"\nğŸ“… ç²¾ç¡®æ—¶åŒºå¤„ç†ç­–ç•¥:")
                print(f"  âœ… ä½¿ç”¨arXivå®˜æ–¹æ—¶åŒº: ç¾å›½ä¸œéƒ¨æ—¶é—´ (UTC-4)")
                print(f"  âœ… æŒ‰APIå®é™…å‘å¸ƒæ—¶é—´åˆ†ç»„ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§")
                print(f"  âœ… é¿å…æ—¶åŒºæ··æ·†å’Œé‡å¤è®¡ç®—é—®é¢˜")
                print(f"\nğŸ• å½“å‰æ—¶é—´ä¿¡æ¯:")
                print(f"  å½“å‰UTCæ—¶é—´: {utc_now}")
                print(f"  å½“å‰ç¾å›½ä¸œéƒ¨æ—¶é—´: {us_eastern_now}")
                print(f"  ç¾å›½ä¸œéƒ¨ä»Šå¤©: {us_eastern_today}")
                print(f"  ç›®æ ‡æœç´¢æ—¥æœŸ: {target_date}")
                print(f"\nğŸ’¡ è¯´æ˜: ç”±äºarXivç½‘ç«™å’ŒAPIçš„æ—¥æœŸåˆ†ç»„é€»è¾‘ç•¥æœ‰å·®å¼‚ï¼Œ")
                print(f"     è®ºæ–‡æ•°é‡å¯èƒ½ä¸ç½‘ç«™æ˜¾ç¤ºä¸å®Œå…¨ä¸€è‡´ï¼Œä½†æœ¬å·¥å…·çš„æ•°æ®æ›´ç²¾ç¡®ã€‚")
                
                # åˆ†é¡µæŸ¥è¯¢ç­–ç•¥ï¼šé€é¡µè·å–ï¼Œç›´åˆ°è¦†ç›–ç›®æ ‡æ—¥æœŸçª—å£
                page_size = 200  # æ¯é¡µæ¡æ•°ï¼Œé¿å…å•æ¬¡è¿”å›ä¸è¶³
                start_index = 0
                max_pages = 30    # æœ€å¤šè¯·æ±‚ 30 é¡µï¼ˆä¸Šé™ 6000 ç¯‡ï¼‰
                entries_all = []
                
                for page in range(max_pages):
                    URL = (
                        "http://export.arxiv.org/api/query?"
                        f"search_query=cat:{category}&"
                        "sortBy=submittedDate&sortOrder=descending&"
                        f"start={start_index}&max_results={page_size}"
                    )
                    print(f"\nğŸ” æ‹‰å–ç¬¬ {page+1} é¡µ: start={start_index}, size={page_size}")
                    response = requests.get(URL)
                    if response.status_code != 200:
                        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        return False
                    feed = feedparser.parse(response.content)
                    if not feed.entries:
                        print("â¹ï¸ æ— æ›´å¤šæ¡ç›®ï¼Œåœæ­¢åˆ†é¡µæ‹‰å–")
                        break
                    entries_all.extend(feed.entries)
                    print(f"ğŸ“¥ æœ¬é¡µ {len(feed.entries)} æ¡ï¼Œç´¯è®¡ {len(entries_all)} æ¡")
                    
                    # åˆ¤æ–­æ˜¯å¦å·²è¦†ç›–åˆ°ç›®æ ‡æ—¥æœŸçª—å£ä»¥ä¸‹ï¼ˆæ¯”ç›®æ ‡æ—¥æœŸæ—©7å¤©ï¼‰
                    last_entry = feed.entries[-1]
                    last_pub_utc = dt.datetime(*last_entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                    last_pub_eastern = last_pub_utc.astimezone(us_eastern_tz)
                    if last_pub_eastern.date() < target_date - dt.timedelta(days=7):
                        print(f"â¹ï¸ å·²è¦†ç›–åˆ° {last_pub_eastern.date()} (< {target_date - dt.timedelta(days=7)})ï¼Œåœæ­¢åˆ†é¡µæ‹‰å–")
                        break
                    
                    start_index += page_size
                
                if not entries_all:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡æ¡ç›®")
                    return False
                
                print(f"ğŸ“Š ä»APIç´¯è®¡è·å–åˆ° {len(entries_all)} ç¯‡è®ºæ–‡ï¼ˆåˆ†é¡µï¼‰")
                
                # åˆ†æè®ºæ–‡å‘å¸ƒæ—¶é—´
                rows = []  # ç²¾ç¡®æŒ‰æ—¥æœŸåŒ¹é…çš„ç»“æœï¼ˆpub_date_eastern == target_dateï¼‰
                date_stats = {}
                
                print(f"\nğŸ” å¼€å§‹åˆ†æè®ºæ–‡å‘å¸ƒæ—¶é—´...")
                
                for i, entry in enumerate(entries_all):
                    # è·å–è®ºæ–‡å‘å¸ƒæ—¶é—´ï¼ˆUTCæ—¶é—´ï¼‰
                    pub_datetime_utc = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                    
                    # è½¬æ¢ä¸ºç¾å›½ä¸œéƒ¨æ—¶é—´
                    pub_datetime_eastern = pub_datetime_utc.astimezone(us_eastern_tz)
                    pub_date_eastern = pub_datetime_eastern.date()
                    
                    # ç»Ÿè®¡æ¯ä¸ªæ—¥æœŸçš„è®ºæ–‡æ•°é‡
                    if pub_date_eastern not in date_stats:
                        date_stats[pub_date_eastern] = 0
                    date_stats[pub_date_eastern] += 1
                    
                    # åªä¿ç•™ç›®æ ‡æ—¥æœŸçš„è®ºæ–‡ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
                    if pub_date_eastern == target_date:
                        print(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸè®ºæ–‡ {len(rows)+1}: {entry.title[:50]}...")
                        print(f"   å‘å¸ƒæ—¶é—´(UTC): {pub_datetime_utc}")
                        print(f"   å‘å¸ƒæ—¶é—´(ç¾ä¸œ): {pub_datetime_eastern}")
                        
                        rows.append({
                            "No.": len(rows) + 1,
                            "number_id": re.search(r'(\d+\.\d+)', entry.id).group(1),
                            "title": entry.title.strip().replace('\n', ' '),
                            "authors": ', '.join(a.name for a in entry.authors),
                            "abstract": entry.summary.strip().replace('\n', ' '),
                            "link": entry.link
                        })
                    
                    # å¦‚æœå‘å¸ƒæ—¥æœŸæ—©äºç›®æ ‡æ—¥æœŸè¶…è¿‡7å¤©ï¼Œåœæ­¢æœç´¢ä»¥æé«˜æ•ˆç‡
                    if pub_date_eastern < target_date - dt.timedelta(days=7):
                        print(f"â¹ï¸  å‘å¸ƒæ—¥æœŸ {pub_date_eastern} æ—©äºç›®æ ‡æ—¥æœŸè¶…è¿‡7å¤©ï¼Œåœæ­¢æœç´¢")
                        break
                
                # æ˜¾ç¤ºæ—¥æœŸç»Ÿè®¡
                print(f"\nğŸ“Š æŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´çš„è®ºæ–‡åˆ†å¸ƒ:")
                sorted_dates = sorted(date_stats.keys(), reverse=True)
                for date in sorted_dates[:10]:  # æ˜¾ç¤ºæœ€è¿‘10å¤©
                    status = "ğŸ¯ ç›®æ ‡" if date == target_date else "  "
                    print(f"  {status} {date}: {date_stats[date]}ç¯‡")
                
                print(f"\nâœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸ {target_date} çš„è®ºæ–‡: {len(rows)} ç¯‡")
                
                if not rows:
                    # å›é€€ç­–ç•¥ï¼šæŒ‰ç…§ arXiv å…¬å‘Šæ—¥çª—å£ç­›é€‰ï¼ˆç¾ä¸œ 20:00 ä¸ºç•Œï¼‰
                    print("\nâš ï¸ ç²¾ç¡®æŒ‰æ—¥æœŸæœªæ‰¾åˆ°è®ºæ–‡ï¼Œå¯ç”¨å›é€€ç­–ç•¥ï¼šæŒ‰å…¬å‘Šçª—å£(ç¾ä¸œ20:00ä¸ºç•Œ)ç­›é€‰â€¦")
                    window_start = dt.datetime.combine(
                        target_date - dt.timedelta(days=1), dt.time(hour=20, minute=0), tzinfo=us_eastern_tz
                    )
                    window_end = dt.datetime.combine(
                        target_date, dt.time(hour=20, minute=0), tzinfo=us_eastern_tz
                    )
                    print(f"å…¬å‘Šçª—å£: [{window_start} , {window_end}) (ç¾å›½ä¸œéƒ¨æ—¶é—´)")

                    fallback_rows = []
                    for entry in feed.entries:
                        pub_datetime_utc = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
                        pub_datetime_eastern = pub_datetime_utc.astimezone(us_eastern_tz)

                        if window_start <= pub_datetime_eastern < window_end:
                            fallback_rows.append({
                                "No.": len(fallback_rows) + 1,
                                "number_id": re.search(r'(\d+\.\d+)', entry.id).group(1),
                                "title": entry.title.strip().replace('\n', ' '),
                                "authors": ', '.join(a.name for a in entry.authors),
                                "abstract": entry.summary.strip().replace('\n', ' '),
                                "link": entry.link
                            })

                    if fallback_rows:
                        print(f"âœ… å›é€€ç­–ç•¥æ‰¾åˆ° {len(fallback_rows)} ç¯‡è®ºæ–‡ï¼ˆå…¬å‘Šçª—å£ï¼‰")
                        rows = fallback_rows
                    else:
                        print(f"ğŸ“­ å›é€€ç­–ç•¥ä»æœªæ‰¾åˆ° {target_date} çš„è®ºæ–‡ï¼Œåˆ›å»ºç©ºç»“æœæ–‡ä»¶ä»¥ä¾¿åç»­é‡è¯•â€¦")
                        # åˆ›å»ºç©ºçš„ç»“æœæ–‡ä»¶ï¼ˆæœåŠ¡å™¨ä¼šæ£€æµ‹åˆ°ç©ºç»“æœå¹¶è‡ªåŠ¨åˆ é™¤ï¼Œå…è®¸åç»­é‡è¯•ï¼‰
                        with open(result_file, 'w', encoding='utf-8') as f:
                            f.write(f"# arXiv {category} è®ºæ–‡ - {target_date}\n\n")
                            f.write(f"**æœç´¢æ—¥æœŸ**: {target_date} (ç¾å›½ä¸œéƒ¨æ—¶é—´ UTC-4)\\n")
                            f.write(f"**è®ºæ–‡åˆ†ç±»**: {category}\\n")
                            f.write(f"**è®ºæ–‡æ•°é‡**: 0 ç¯‡\\n")
                            f.write(f"**ç”Ÿæˆæ—¶é—´**: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
                            f.write(f"**æ•°æ®æ¥æº**: arXivå®˜æ–¹API (ç²¾ç¡®æ—¶åŒºå¤„ç†)\\n\\n")
                            f.write("> **è¯´æ˜**: æœ¬å·¥å…·ä½¿ç”¨arXivå®˜æ–¹APIï¼ŒæŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´ç²¾ç¡®åˆ†ç»„ã€‚\\n")
                            f.write("> å½“å¤©æœªæ‰¾åˆ°è®ºæ–‡ã€‚è‹¥ä¸å®˜ç½‘æœ‰å‡ºå…¥ï¼Œå¯èƒ½ç”±äºå®˜ç½‘æŒ‰å…¬å‘Šçª—å£åˆ†ç»„æˆ–å‘¨æœ«æœªæ›´æ–°ã€‚\\n\\n")
                            f.write("## è®ºæ–‡åˆ—è¡¨\n\n")
                            f.write("*è¯¥æ—¥æœŸæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡*\\n")
                        return True  # è¿”å›æˆåŠŸï¼Œè®©æœåŠ¡å™¨å¤„ç†ç©ºç»“æœçš„åˆ é™¤é€»è¾‘
                
        # å†™å…¥ç»“æœæ–‡ä»¶ï¼ˆä¸é‡å®šå‘è¾“å‡ºï¼‰
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ° {result_file}...")
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(rows)
        
        # ç”ŸæˆMarkdownæ ¼å¼çš„è¡¨æ ¼
        markdown_table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
        
        # å†™å…¥æ–‡ä»¶
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write(f"# arXiv {category} è®ºæ–‡ - {target_date}\n\n")
            f.write(f"**æœç´¢æ—¥æœŸ**: {target_date} (ç¾å›½ä¸œéƒ¨æ—¶é—´ UTC-4)\\n")
            f.write(f"**è®ºæ–‡åˆ†ç±»**: {category}\\n")
            f.write(f"**è®ºæ–‡æ•°é‡**: {len(rows)} ç¯‡\\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
            f.write(f"**æ•°æ®æ¥æº**: arXivå®˜æ–¹API (ç²¾ç¡®æ—¶åŒºå¤„ç†)\\n\\n")
            f.write("> **è¯´æ˜**: æœ¬å·¥å…·ä½¿ç”¨arXivå®˜æ–¹APIï¼ŒæŒ‰ç¾å›½ä¸œéƒ¨æ—¶é—´ç²¾ç¡®åˆ†ç»„ã€‚\\n")
            f.write("> ç”±äºarXivç½‘ç«™å’ŒAPIçš„æ—¥æœŸåˆ†ç»„é€»è¾‘å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼Œè®ºæ–‡æ•°é‡å¯èƒ½ä¸å®˜ç½‘æ˜¾ç¤ºæœ‰æ‰€ä¸åŒï¼Œä½†æ•°æ®å‡†ç¡®æ€§æ›´é«˜ã€‚\\n\\n")
            f.write("## è®ºæ–‡åˆ—è¡¨\n\n")
            f.write(markdown_table)
            f.write("\\n")
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ‰¾åˆ° {len(rows)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {result_file}")
        print(f"ğŸ“ æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
        
        return True  # è¿”å›æˆåŠŸçŠ¶æ€
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False  # è¿”å›å¤±è´¥çŠ¶æ€

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python crawl_raw_info_fixed.py <æ—¥æœŸ> <åˆ†ç±»>")
        print("ç¤ºä¾‹: python crawl_raw_info_fixed.py 2025-08-06 cs.CV")
        sys.exit(1)
    
    date_str = sys.argv[1]
    category = sys.argv[2]
    
    crawl_arxiv_papers(date_str, category)