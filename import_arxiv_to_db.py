#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä» arXiv æ‹‰å–æŒ‡å®šæ—¥æœŸ/åˆ†ç±»çš„è®ºæ–‡å¹¶å†™å…¥ Supabase æ•°æ®åº“ï¼šapp.papers ä¸ app.paper_categoriesã€‚

æ—¶é—´çª—å£è§„åˆ™ï¼šæŒ‰ ET(US/Eastern) çš„ 20:00 ä¸ºæ¯å¤©è¾¹ç•Œï¼Œç›®æ ‡æ—¥çš„çª—å£ä¸ºï¼š
  [ç›®æ ‡æ—¥å‰ä¸€æ—¥ 20:00, ç›®æ ‡æ—¥ 20:00] (é—­åŒºé—´)

å­—æ®µæ˜ å°„ï¼š
  - arxiv_id: ä¾‹å¦‚ "2508.05636v1"ï¼ˆä» entry.id æˆ– entry.link çš„ /abs/<id> æå–ï¼ŒåŒ…å«ç‰ˆæœ¬å·ï¼‰
  - title, authors, abstract, link
  - primary_category: arxiv:primary_category çš„ termï¼›æ— åˆ™ä» tags ç¬¬ä¸€é¡¹å›é€€
  - update_date: ç›®æ ‡æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
  - ingest_at: ç”±æ•°æ®åº“é»˜è®¤å€¼ç”Ÿæˆ

å†™åº“ç­–ç•¥ï¼š
  - papers: upsertï¼ˆæŒ‰ arxiv_id å”¯ä¸€ï¼‰
  - categories: å¯¹æ‰€æœ‰å‡ºç°çš„ category term upsert
  - paper_categories: å»ºç«‹ (paper_id, category_id) å…³è”ï¼ˆæŒ‰å¤šå¯¹å¤šï¼‰

æ—¥å¿—ï¼šæ¯æ¡å†™åº“æ“ä½œè¾“å‡ºåˆ°stdoutï¼Œä¾¿äºç¡®è®¤ã€‚
"""

import datetime as dt
import os
import re
from typing import List, Dict, Any, Optional, Tuple

import feedparser
import pytz
import requests

from db import repo as db_repo


def _extract_arxiv_id(entry: Any) -> Optional[str]:
    """ä»entry.idæˆ–entry.linkä¸­æå–åŒ…å«ç‰ˆæœ¬å·çš„arXiv IDï¼Œå¦‚ 2508.05636v1ã€‚
    è§„åˆ™ï¼šä¼˜å…ˆä» entry.id ä¸­è§£æ /abs/<id>ï¼›å¦åˆ™ä» entry.link è§£æã€‚
    """
    candidates = []
    if getattr(entry, "id", None):
        candidates.append(entry.id)
    if getattr(entry, "link", None):
        candidates.append(entry.link)

    for url in candidates:
        # å…¸å‹: http://arxiv.org/abs/2508.05636v1
        m = re.search(r"/abs/([0-9]{4}\.[0-9]{5}(v\d+)?)(?:[?#].*)?$", url)
        if m:
            return m.group(1)
    # æœ‰äº›feedå¯èƒ½æ˜¯https://arxiv.org/abs/YYMM.numberv1 å˜ä½“ï¼Œå…œåº•å†è¯•ä¸€æ¬¡å®½æ¾åŒ¹é…
    for url in candidates:
        m = re.search(r"([0-9]{4}\.[0-9]{5}v\d+)", url)
        if m:
            return m.group(1)
    return None


def _extract_primary_category(entry: Any) -> Optional[str]:
    # feedparserä¼šæŠŠarxiv:primary_categoryæ˜ å°„åˆ° entry.arxiv_primary_category
    pc = getattr(entry, "arxiv_primary_category", None)
    if isinstance(pc, dict) and pc.get("term"):
        return pc["term"]
    # å›é€€ï¼šä» tags åˆ—è¡¨å–ç¬¬ä¸€é¡¹term
    tags = getattr(entry, "tags", [])
    for t in tags:
        term = t.get("term") if isinstance(t, dict) else None
        if term:
            return term
    return None


def _extract_all_categories(entry: Any) -> List[str]:
    cats: List[str] = []
    tags = getattr(entry, "tags", [])
    for t in tags:
        term = t.get("term") if isinstance(t, dict) else None
        if term and term not in cats:
            cats.append(term)
    # ç¡®ä¿primaryä¹Ÿåœ¨åˆ—è¡¨ä¸­
    pc = _extract_primary_category(entry)
    if pc and pc not in cats:
        cats.append(pc)
    return cats


def import_arxiv_papers_to_db(
    target_date_str: str,
    category: str = "cs.CV",
    limit: Optional[int] = None,
    skip_if_exists: bool = True,
) -> Dict[str, Any]:
    import time
    target_date = dt.datetime.strptime(target_date_str, "%Y-%m-%d").date()
    et_tz = pytz.timezone("US/Eastern")

    start_et = et_tz.localize(dt.datetime.combine(target_date - dt.timedelta(days=1), dt.time(20, 0)))
    end_et = et_tz.localize(dt.datetime.combine(target_date, dt.time(20, 0)))

    start_utc = start_et.astimezone(dt.timezone.utc)
    end_utc = end_et.astimezone(dt.timezone.utc)

    start_date_str = start_utc.strftime("%Y%m%d%H%M%S")
    end_date_str = end_utc.strftime("%Y%m%d%H%M%S")

    base = os.getenv("ARXIV_API_BASE", "https://export.arxiv.org/api/query")
    url = (
        f"{base}?"
        f"search_query=cat:{category}+AND+submittedDate:[{start_date_str}+TO+{end_date_str}]&"
        "sortBy=submittedDate&sortOrder=descending&"
        "max_results=2000"
    )

    print(f"ç›®æ ‡æ—¥æœŸ(ET): {target_date} | åˆ†ç±»: {category}")
    print(f"çª—å£(ET): {start_et} ~ {end_et}")
    print(f"çª—å£(UTC): {start_utc} ~ {end_utc}")
    print(f"URL: {url}")

    start_time = time.time()
    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
        feed = feedparser.parse(resp.text)
    except Exception as e:
        print(f"requestsè·å–å¤±è´¥ï¼Œå°†å°è¯•feedparserç›´è¿: {e}")
        try:
            feed = feedparser.parse(url)
        except Exception as e2:
            # å°è¯•httpå›é€€
            http_url = url.replace("https://", "http://")
            print(f"feedparserç›´è¿å¤±è´¥ï¼Œå°è¯•HTTPå›é€€: {e2}")
            feed = feedparser.parse(http_url)
    api_time = time.time() - start_time if 'start_time' in locals() else 0
    print(f"â±ï¸  [å¯¼å…¥æ€§èƒ½] APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {api_time:.2f}s | è¿”å› {len(feed.entries)} æ¡")

    filter_start = time.time()
    kept: List[Any] = []
    for i, entry in enumerate(feed.entries):
        pub_utc = dt.datetime(*entry.published_parsed[:6], tzinfo=dt.timezone.utc)
        if start_utc <= pub_utc <= end_utc:
            kept.append(entry)

    filter_time = time.time() - filter_start
    print(f"â±ï¸  [å¯¼å…¥æ€§èƒ½] ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {filter_time:.2f}s | ä¿ç•™ {len(kept)} æ¡")
    
    if limit is not None:
        kept = kept[:limit]
        print(f"æŒ‰limitæˆªæ–­ä¸º {len(kept)} æ¡")
    total = len(kept)

    # ===== æ–°ï¼šæ‰¹å¤„ç†æé€Ÿè·¯å¾„ =====
    # 1) é¢„è§£æ entriesï¼Œæ„å»ºå¾…å†™å…¥è¡Œä¸ç±»åˆ«æ˜ å°„
    parse_start = time.time()
    parsed_items: List[Dict[str, Any]] = []
    arxiv_to_categories: Dict[str, List[str]] = {}
    errors = 0

    for idx, entry in enumerate(kept, start=1):
        try:
            arxiv_id = _extract_arxiv_id(entry)
            if not arxiv_id:
                print(f"[{idx}] è·³è¿‡ï¼šæ— æ³•è§£æarxiv_id | id={getattr(entry, 'id', '')}")
                continue
            title = (entry.title or "").strip().replace("\n", " ")
            authors = ", ".join(a.name for a in getattr(entry, "authors", []) if getattr(a, "name", None))
            abstract = (entry.summary or "").strip().replace("\n", " ")
            link = getattr(entry, "link", f"https://arxiv.org/abs/{arxiv_id}")
            primary_category = _extract_primary_category(entry)
            all_categories = _extract_all_categories(entry)

            upd_time = None
            if getattr(entry, "updated_parsed", None):
                t = dt.datetime(*entry.updated_parsed[:6])  # naive UTC
                upd_time = t.time().isoformat()

            row: Dict[str, Any] = {
                "arxiv_id": arxiv_id,
                "title": title,
                "authors": authors,
                "abstract": abstract,
                "link": link,
                "update_date": target_date_str,
                "primary_category": primary_category,
            }
            if upd_time:
                row["update_time"] = upd_time

            parsed_items.append(row)
            arxiv_to_categories[arxiv_id] = all_categories
        except Exception as e:
            errors += 1
            print(f"[{idx}] âŒ é¢„å¤„ç†å¤±è´¥: {e}")

    total = len(parsed_items)
    parse_time = time.time() - parse_start
    print(f"â±ï¸  [å¯¼å…¥æ€§èƒ½] è§£æå®Œæˆï¼Œè€—æ—¶: {parse_time:.2f}s | å¤„ç† {total} æ¡è®°å½•")

    # 2) å¤„ç†å·²å­˜åœ¨é€»è¾‘
    db_start = time.time()
    all_ids = [r["arxiv_id"] for r in parsed_items]
    existing_rows = db_repo.get_papers_by_arxiv_ids(all_ids)
    existing_set = {r["arxiv_id"] for r in existing_rows}

    items_for_write: List[Dict[str, Any]]
    if skip_if_exists:
        items_for_write = [r for r in parsed_items if r["arxiv_id"] not in existing_set]
    else:
        items_for_write = parsed_items  # è¦†ç›–æ›´æ–°

    # 3) æ‰¹é‡ upsert/insert papers
    arxiv_to_paper_id: Dict[str, int] = {}
    if items_for_write:
        if skip_if_exists:
            # ä»…è¡¥ç¼º
            arxiv_to_paper_id.update(db_repo.upsert_papers_bulk(items_for_write))
        else:
            # è¦†ç›–æ›´æ–°ï¼šç›´æ¥ upsert å…¨é‡ï¼Œå†æŸ¥è¯¢æ˜ å°„
            from db.client import app_schema
            app_schema().from_("papers").upsert(items_for_write, on_conflict="arxiv_id").execute()
            arxiv_to_paper_id.update({r["arxiv_id"]: r["paper_id"] for r in db_repo.get_papers_by_arxiv_ids(all_ids)})

    # å¯¹äºå·²å­˜åœ¨çš„ arxivï¼Œä»éœ€è¦ï¼š
    #  - æ›´æ–° update_date ä¸ºç›®æ ‡æ—¥ï¼ˆç¡®ä¿å½“å¤©åˆ—è¡¨ç»Ÿè®¡å‡†ç¡®ï¼‰
    #  - è¡¥å»ºåˆ†ç±»å…³è”
    if skip_if_exists and existing_set:
        try:
            from db.client import app_schema
            app_schema().from_("papers").update({"update_date": target_date_str}).in_("arxiv_id", list(existing_set)).execute()
        except Exception as e:
            print(f"è½»é‡æ›´æ–° existing papers.update_date å¤±è´¥: {e}")

    # ç»Ÿä¸€è·å– arxiv -> paper_id æ˜ å°„ï¼ˆåŒ…å«æ–°å†™å…¥ä¸å·²å­˜åœ¨ï¼‰
    try:
        final_rows = db_repo.get_papers_by_arxiv_ids(all_ids)
        arxiv_to_paper_id.update({r["arxiv_id"]: r["paper_id"] for r in final_rows})
    except Exception as e:
        print(f"è·å–paperæ˜ å°„å¤±è´¥: {e}")
    # å¯¹äº skip_if_exists=true çš„æƒ…å½¢ï¼Œå·²æœ‰çš„ä¹Ÿéœ€è¦æ˜ å°„ï¼ˆç”¨äºåç»­å…³è”æ—¶è‹¥ä½ å°†æ¥æƒ³ä¿ç•™ï¼Œä½†å½“å‰é€»è¾‘ä¿æŒä¸æ—§å®ç°ä¸€è‡´ï¼šè·³è¿‡å·²æœ‰ï¼Œä¸å†åšå…³è”ï¼‰

    # 4) ğŸš€ ä¼˜åŒ–ï¼šä»…ä¸ºæ–°å†™å…¥çš„è®ºæ–‡å»ºç«‹åˆ†ç±»å…³è”
    if items_for_write:  # åªæœ‰æ–°è®ºæ–‡æ—¶æ‰å¤„ç†åˆ†ç±»å…³è”
        new_arxiv_ids = {r["arxiv_id"] for r in items_for_write}
        print(f"â±ï¸  [å¯¼å…¥æ€§èƒ½] ä»…ä¸º {len(new_arxiv_ids)} æ¡æ–°è®ºæ–‡å»ºç«‹åˆ†ç±»å…³è”")
        
        # ä»…æ”¶é›†æ–°è®ºæ–‡çš„åˆ†ç±»
        new_category_names: List[str] = []
        for aid in new_arxiv_ids:
            new_category_names.extend(arxiv_to_categories.get(aid, []))
        cat_name_to_id = db_repo.upsert_categories_bulk(new_category_names) if new_category_names else {}

        # ä»…ä¸ºæ–°è®ºæ–‡åˆ›å»ºå…³è”
        pairs: List[Tuple[int, int]] = []  # (paper_id, category_id)
        for aid in new_arxiv_ids:
            pid = arxiv_to_paper_id.get(aid)
            if not pid:
                continue
            for cat in arxiv_to_categories.get(aid, []):
                cid = cat_name_to_id.get(cat)
                if cid:
                    pairs.append((pid, cid))
        
        if pairs:
            db_repo.upsert_paper_categories_bulk(pairs)
    else:
        # æ— æ–°è®ºæ–‡ï¼Œæ— éœ€å¤„ç†åˆ†ç±»å…³è”
        pairs = []
        print(f"âš¡ [å¯¼å…¥æ€§èƒ½] æ— æ–°è®ºæ–‡ï¼Œè·³è¿‡åˆ†ç±»å…³è”æ“ä½œ")
    
    # 5) è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_upsert = len(items_for_write)
    total_link = len(pairs)
    
    db_time = time.time() - db_start
    print(f"â±ï¸  [å¯¼å…¥æ€§èƒ½] æ•°æ®åº“æ“ä½œå®Œæˆï¼Œè€—æ—¶: {db_time:.2f}s | upsert={total_upsert} link={total_link}")
    
    # ä¼˜åŒ–ï¼šä»…åœ¨è°ƒè¯•æ¨¡å¼æˆ–æ•°é‡è¾ƒå°‘æ—¶è¾“å‡ºè¯¦ç»†æ—¥å¿—
    if total <= 20 or os.getenv('DEBUG_IMPORT', '').lower() == 'true':
        # ä¼˜åŒ–ï¼šæ„å»º arxiv_id -> parsed_item çš„æ˜ å°„ï¼Œé¿å…é‡å¤ç´¢å¼•æŸ¥æ‰¾
        arxiv_to_item = {r["arxiv_id"]: r for r in parsed_items}
        now = dt.datetime.now().isoformat(timespec='seconds')
        
        # è·å–æ‰€æœ‰è§£æçš„è®ºæ–‡IDç”¨äºæ—¥å¿—è¾“å‡º
        all_parsed_ids = {r["arxiv_id"] for r in parsed_items}
        for idx, aid in enumerate(sorted(all_parsed_ids), start=1):
            item = arxiv_to_item.get(aid, {})
            primary_category = item.get("primary_category")
            print(f"[{idx}/{total}] arxiv_id={aid} update_date={target_date_str} search_category={category} now={now}")
            for cat in arxiv_to_categories.get(aid, []):
                cid = cat_name_to_id.get(cat)
                if cid:
                    print(f"    + link category: {cat} (category_id={cid})")
    else:
        print(f"â­ï¸  [å¯¼å…¥æ€§èƒ½] è·³è¿‡è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆ{total}æ¡è®°å½•ï¼Œå¦‚éœ€æŸ¥çœ‹è®¾ç½® DEBUG_IMPORT=trueï¼‰")

    return {
        "total_upsert": total_upsert,
        "total_link": total_link,
        "errors": errors,
        "processed": len(kept),
    }


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Import arXiv papers into Supabase DB")
    parser.add_argument("date", help="YYYY-MM-DD")
    parser.add_argument("category", help="e.g. cs.CV")
    parser.add_argument("--limit", type=int, default=None, help="process only first N entries")
    args = parser.parse_args()

    stats = import_arxiv_papers_to_db(args.date, args.category, args.limit)
    print("SUMMARY:", stats)


if __name__ == "__main__":
    main()


