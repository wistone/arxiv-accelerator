#!/usr/bin/env python3
"""
ä½¿ç”¨è±†åŒ…APIè§£æArXivè®ºæ–‡ä½œè€…æœºæ„ä¿¡æ¯

è¾“å…¥ï¼šArXivè®ºæ–‡é“¾æ¥
è¾“å‡ºï¼šå»é‡çš„ä½œè€…æœºæ„åˆ—è¡¨

è¯¥æ–¹æ¡ˆä½¿ç”¨å¤§æ¨¡å‹æ™ºèƒ½è§£æè®ºæ–‡ç¬¬ä¸€é¡µå†…å®¹ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§å¤æ‚çš„è®ºæ–‡æ ¼å¼ã€‚
"""

import requests
import re
import sys
import json
import io
from typing import List
from pdfminer.high_level import extract_text_to_fp
from pdfminer.layout import LAParams
from doubao_client import DoubaoClient


def extract_arxiv_id_from_url(url: str) -> str:
    """
    ä»arxiv abstract URLä¸­æå–arxiv_id
    ä¾‹å¦‚: http://arxiv.org/abs/2507.23785v1 -> 2507.23785v1
    """
    url = url.strip()
    pattern = r'/abs/([^/\s]+)/?$'
    match = re.search(pattern, url)
    
    if match:
        return match.group(1)
    else:
        raise ValueError(f"æ— æ³•ä»URLä¸­æå–arxiv_id: {url}")


def download_arxiv_pdf(arxiv_url: str) -> bytes:
    """
    ä»ArXiv URLä¸‹è½½PDFæ–‡ä»¶ï¼ˆé«˜åº¦ä¼˜åŒ–æ€§èƒ½ï¼‰
    """
    arxiv_id = extract_arxiv_id_from_url(arxiv_url)
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}"
    
    print(f"[PDFä¸‹è½½] å¼€å§‹ä¸‹è½½: {pdf_url}")
    start_time = __import__('time').time()
    
    # é«˜æ€§èƒ½ä¸‹è½½é…ç½®
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/pdf',
        'Connection': 'keep-alive',
        'Accept-Encoding': 'gzip, deflate'  # å¯ç”¨å‹ç¼©
    }
    
    # ä½¿ç”¨sessionå¤ç”¨è¿æ¥ï¼Œè®¾ç½®æ›´ç§¯æçš„è¶…æ—¶
    with requests.Session() as session:
        session.headers.update(headers)
        
        # 1. å…ˆHEADè¯·æ±‚æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œç”¨äºæ—¥å¿—è®°å½•
        try:
            head_response = session.head(pdf_url, timeout=5)
            content_length = head_response.headers.get('content-length')
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                print(f"[PDFä¸‹è½½] é¢„æ£€æ–‡ä»¶å¤§å°: {size_mb:.1f}MB")
        except Exception as e:
            print(f"[PDFä¸‹è½½] âš ï¸  é¢„æ£€å¤±è´¥ï¼Œç»§ç»­ä¸‹è½½: {e}")
        
        # 2. æµå¼ä¸‹è½½ï¼Œè®¾ç½®è¾ƒçŸ­è¶…æ—¶
        response = session.get(pdf_url, timeout=10, stream=True)  # è¿›ä¸€æ­¥ç¼©çŸ­è¶…æ—¶
        response.raise_for_status()
        
        # 3. åˆ†å—ä¸‹è½½ï¼ˆæ— å¤§å°é™åˆ¶ï¼‰
        pdf_content = bytearray()
        
        for chunk in response.iter_content(chunk_size=8192):  # 8KBå—
            if chunk:
                pdf_content.extend(chunk)
    
        end_time = __import__('time').time()
        size_mb = len(pdf_content) / (1024 * 1024)
        speed_mbps = size_mb / (end_time - start_time) if (end_time - start_time) > 0 else 0
        print(f"[PDFä¸‹è½½] å®Œæˆï¼Œå¤§å°: {size_mb:.1f}MBï¼Œè€—æ—¶: {end_time - start_time:.2f}s (é€Ÿåº¦: {speed_mbps:.1f}MB/s)")
        
        return bytes(pdf_content)


def extract_first_page_text(pdf_content: bytes) -> str:
    """
    æå–PDFç¬¬ä¸€é¡µçš„æ–‡æœ¬å†…å®¹ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    """
    # ä¼˜åŒ–LAParamså‚æ•°ä»¥æé«˜é€Ÿåº¦
    laparams = LAParams(
        word_margin=0.1,
        char_margin=2.0,
        line_margin=0.5,
        boxes_flow=0.5,
        all_texts=False  # è·³è¿‡ä¸€äº›ä¸å¿…è¦çš„æ–‡æœ¬å¤„ç†
    )
    output = io.StringIO()
    
    try:
        # å¢åŠ è¶…æ—¶å’Œé”™è¯¯æŠ‘åˆ¶
        import logging
        logging.getLogger('pdfminer').setLevel(logging.ERROR)  # æŠ‘åˆ¶è­¦å‘Šä¿¡æ¯
        
        extract_text_to_fp(
            io.BytesIO(pdf_content),
            output,
            laparams=laparams,
            maxpages=1,  # åªæå–ç¬¬ä¸€é¡µ
            page_numbers=[0],
            codec='utf-8',
            caching=True,  # å¯ç”¨ç¼“å­˜
            check_extractable=True
        )
        return output.getvalue()
    except Exception as e:
        raise Exception(f"PDFæ–‡æœ¬æå–å¤±è´¥: {e}")


def load_prompt_template() -> str:
    """
    åŠ è½½promptæ¨¡æ¿
    """
    import os
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_path = os.path.join(script_dir, 'prompt', 'author_affliation_prompt.md')
    
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise Exception(f"è¯»å–promptæ–‡ä»¶å¤±è´¥: {e}")


def parse_affiliations_with_doubao(first_page_text: str) -> List[str]:
    """
    ä½¿ç”¨è±†åŒ…APIè§£æä½œè€…æœºæ„ä¿¡æ¯
    """
    # åŠ è½½promptæ¨¡æ¿
    system_prompt = load_prompt_template()
    
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_message = f"""
è¯·ä»ä»¥ä¸‹è®ºæ–‡ç¬¬ä¸€é¡µå†…å®¹ä¸­æå–æ‰€æœ‰ä½œè€…çš„æœºæ„ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼çš„æœºæ„åˆ—è¡¨ï¼š

è®ºæ–‡å†…å®¹ï¼š
{first_page_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„è¦æ±‚ï¼Œè¿”å›å»é‡çš„æœºæ„åç§°JSONæ•°ç»„ã€‚
"""
    
    # è°ƒç”¨è±†åŒ…API
    try:
        client = DoubaoClient()
        response = client.chat(
            message=user_message,
            system_prompt=system_prompt,
            verbose=False  # å…³é—­è¯¦ç»†è¾“å‡º
        )
        
        if response is None:
            raise Exception("è±†åŒ…APIè°ƒç”¨å¤±è´¥")
        
        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                affiliations_data = json.loads(json_match.group())
                if isinstance(affiliations_data, list):
                    return [str(affil).strip() for affil in affiliations_data if affil]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£æå…¶ä»–æ ¼å¼
            if "error" in response.lower():
                return []
            
            # å°è¯•æŒ‰è¡Œè§£æ
            lines = response.strip().split('\n')
            affiliations = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('```'):
                    # ç§»é™¤åˆ—è¡¨æ ‡è®°
                    clean_line = re.sub(r'^\d+\.\s*|^-\s*|^\*\s*', '', line).strip()
                    if clean_line and len(clean_line) > 2:
                        affiliations.append(clean_line)
            
            return affiliations[:20]  # é™åˆ¶æœ€å¤š20ä¸ªæœºæ„
            
        except json.JSONDecodeError:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»å“åº”ä¸­æå–æœºæ„åç§°
            print(f"JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response}")
            return []
            
    except Exception as e:
        raise Exception(f"è±†åŒ…APIè§£æå¤±è´¥: {e}")


# æ·»åŠ ç®€å•çš„å†…å­˜ç¼“å­˜
_AFFILIATION_CACHE = {}

def clear_affiliation_cache():
    """æ¸…ç©ºæœºæ„ä¿¡æ¯ç¼“å­˜"""
    global _AFFILIATION_CACHE
    _AFFILIATION_CACHE.clear()
    print("[ç¼“å­˜] ğŸ—‘ï¸ æœºæ„ä¿¡æ¯ç¼“å­˜å·²æ¸…ç©º")

def get_author_affiliations(arxiv_url: str, use_cache: bool = True, progress_callback=None) -> List[str]:
    """
    ä»ArXivè®ºæ–‡é“¾æ¥è·å–ä½œè€…æœºæ„ä¿¡æ¯åˆ—è¡¨ï¼ˆå»é‡ï¼‰
    
    Args:
        arxiv_url: ArXivè®ºæ–‡é“¾æ¥ï¼Œå¦‚ http://arxiv.org/abs/2507.23785v1
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
    Returns:
        List[str]: å»é‡çš„ä½œè€…æœºæ„åˆ—è¡¨
        
    Raises:
        Exception: å½“å¤„ç†å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    import time
    total_start = time.time()
    
    print(f"[æœºæ„è·å–] å¼€å§‹å¤„ç†: {arxiv_url}")
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and arxiv_url in _AFFILIATION_CACHE:
        cached_result = _AFFILIATION_CACHE[arxiv_url]
        print(f"[æœºæ„è·å–] ğŸš€ ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œæœºæ„æ•°: {len(cached_result)}")
        return cached_result
    
    try:
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨ä¸‹è½½PDFè·å–æœºæ„ä¿¡æ¯ï¼Œéœ€è¦10så·¦å³...")
        
        # 1. ä¸‹è½½PDFï¼ˆä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†ï¼‰
        step_start = time.time()
        pdf_content = download_arxiv_pdf(arxiv_url)
        download_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFä¸‹è½½å®Œæˆï¼Œè€—æ—¶: {download_time:.2f}s")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­æ”¹è®ºæ–‡ï¼Œæ­£åœ¨è§£æPDFæ–‡æœ¬...")
        
        # 2. æå–ç¬¬ä¸€é¡µæ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šåªæå–å‰2000å­—ç¬¦ç”¨äºæœºæ„è¯†åˆ«ï¼‰
        step_start = time.time()
        first_page_text = extract_first_page_text(pdf_content)
        
        # ä¼˜åŒ–ï¼šåªå–å‰2000å­—ç¬¦ï¼Œé€šå¸¸æœºæ„ä¿¡æ¯éƒ½åœ¨è®ºæ–‡å¼€å¤´
        if len(first_page_text) > 2000:
            first_page_text = first_page_text[:2000] + "\n[æ–‡æœ¬å·²æˆªæ–­ï¼Œä»…ä¿ç•™å‰2000å­—ç¬¦]"
            print(f"[æœºæ„è·å–] âœ‚ï¸ æ–‡æœ¬å·²æˆªæ–­è‡³2000å­—ç¬¦ä»¥ä¼˜åŒ–å¤„ç†é€Ÿåº¦")
        
        extract_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFè§£æå®Œæˆï¼Œè€—æ—¶: {extract_time:.2f}sï¼Œæ–‡æœ¬é•¿åº¦: {len(first_page_text)}")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨è°ƒç”¨AIæ¨¡å‹åˆ†ææœºæ„ä¿¡æ¯...")
        
        # 3. ä½¿ç”¨è±†åŒ…APIè§£ææœºæ„ä¿¡æ¯
        step_start = time.time()
        affiliations = parse_affiliations_with_doubao(first_page_text)
        api_time = time.time() - step_start
        print(f"[æœºæ„è·å–] è±†åŒ…APIåˆ†æå®Œæˆï¼Œè€—æ—¶: {api_time:.2f}sï¼Œæ‰¾åˆ°æœºæ„: {len(affiliations)}")
        
        # 4. å»é‡å¹¶è¿”å›
        unique_affiliations = []
        for affil in affiliations:
            if affil not in unique_affiliations:
                unique_affiliations.append(affil)
        
        # ç¼“å­˜ç»“æœ
        if use_cache:
            _AFFILIATION_CACHE[arxiv_url] = unique_affiliations
            print(f"[æœºæ„è·å–] ğŸ’¾ ç»“æœå·²ç¼“å­˜")
        
        total_time = time.time() - total_start
        print(f"[æœºæ„è·å–] å…¨æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s (ä¸‹è½½:{download_time:.1f}s, è§£æ:{extract_time:.1f}s, API:{api_time:.1f}s)ï¼Œæœ€ç»ˆæœºæ„æ•°: {len(unique_affiliations)}")
        
        return unique_affiliations
        
    except Exception as e:
        print(f"[æœºæ„è·å–] âŒ å¤„ç†å¤±è´¥: {e}")
        # å¯¹äºæŸäº›é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶è¿‡å¤§ï¼‰ï¼Œä¸ç¼“å­˜ç»“æœï¼Œå…è®¸åç»­é‡è¯•
        error_msg = str(e)
        should_cache_failure = False
        
        # åªæœ‰ç½‘ç»œé”™è¯¯ç­‰ä¸´æ—¶æ€§é—®é¢˜æ‰ç¼“å­˜å¤±è´¥ç»“æœ
        if "ç½‘ç»œ" in error_msg or "timeout" in error_msg.lower() or "connection" in error_msg.lower():
            should_cache_failure = True
            
        if use_cache and should_cache_failure:
            _AFFILIATION_CACHE[arxiv_url] = []
            print(f"[æœºæ„è·å–] ğŸ’¾ å¤±è´¥ç»“æœå·²ç¼“å­˜ï¼ˆä¸´æ—¶æ€§é”™è¯¯ï¼‰")
        else:
            print(f"[æœºæ„è·å–] ğŸš« å¤±è´¥ç»“æœæœªç¼“å­˜ï¼ˆå…è®¸é‡è¯•ï¼‰")
            
        raise e


if __name__ == "__main__":
    # ç®€å•çš„æµ‹è¯•ç”¨ä¾‹
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python parse_author_affli_from_doubao.py <arxiv_url>")
        print("ä¾‹å¦‚: python parse_author_affli_from_doubao.py \"http://arxiv.org/abs/2507.23785v1\"")
        exit(1)
    
    arxiv_url = sys.argv[1]
    try:
        print(f"æ­£åœ¨å¤„ç†è®ºæ–‡: {arxiv_url}")
        affiliations = get_author_affiliations(arxiv_url)
        print(f"è®ºæ–‡é“¾æ¥: {arxiv_url}")
        print(f"ä½œè€…æœºæ„ ({len(affiliations)}):")
        for i, affil in enumerate(affiliations, 1):
            print(f"  {i}. {affil}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")