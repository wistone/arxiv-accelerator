#!/usr/bin/env python3
"""
GROBIDæ‰¹é‡æµ‹è¯•è„šæœ¬

æµ‹è¯• parse_author_org_v3.py è„šæœ¬çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§
è¯»å– log/2025-07-31-cs.CV-result.md æ–‡ä»¶ä¸­çš„æ‰€æœ‰è®ºæ–‡é“¾æ¥
è¾“å‡ºæ¯ä¸ªè®ºæ–‡çš„æ ‡é¢˜å’Œä½œè€…æœºæ„ä¿¡æ¯
"""

import sys
import os
import re
import time
import xml.etree.ElementTree as ET
from typing import List, Dict, Tuple

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ä¸»è„šæœ¬
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import parse_author_org_v3
import grobid_tei_xml

def extract_paper_links_from_file(file_path: str) -> List[str]:
    """
    ä»markdownæ–‡ä»¶ä¸­æå–æ‰€æœ‰ArXivè®ºæ–‡é“¾æ¥
    """
    links = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ArXivé“¾æ¥
            pattern = r'http://arxiv\.org/abs/[^\s|)]+|https://arxiv\.org/abs/[^\s|)]+'
            found_links = re.findall(pattern, content)
            # å»é‡å¹¶æ¸…ç†é“¾æ¥
            for link in found_links:
                clean_link = link.strip('|).;:')
                if clean_link not in links:
                    links.append(clean_link)
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return links

def get_paper_title_from_grobid(xml_content: str) -> str:
    """
    ä»GROBID XMLä¸­æå–è®ºæ–‡æ ‡é¢˜
    """
    try:
        doc = grobid_tei_xml.parse_document_xml(xml_content)
        return doc.header.title if doc.header.title else "æ ‡é¢˜æå–å¤±è´¥"
    except:
        return "æ ‡é¢˜è§£æé”™è¯¯"

def process_paper_with_title(arxiv_url: str) -> Tuple[str, List[str]]:
    """
    å¤„ç†å•ç¯‡è®ºæ–‡ï¼Œè¿”å›æ ‡é¢˜å’Œæœºæ„åˆ—è¡¨
    """
    try:
        # ä¸‹è½½PDF
        pdf_content = parse_author_org_v3.download_arxiv_pdf(arxiv_url)
        
        # ä½¿ç”¨GROBIDå¤„ç†
        xml_content = parse_author_org_v3.process_pdf_with_grobid(pdf_content)
        
        # æå–æ ‡é¢˜
        title = get_paper_title_from_grobid(xml_content)
        
        # æå–æœºæ„
        affiliations = parse_author_org_v3.parse_grobid_xml(xml_content)
        
        return title, affiliations
        
    except Exception as e:
        return f"é”™è¯¯: {str(e)}", []

def run_batch_test(file_path: str, max_papers: int = None):
    """
    è¿è¡Œæ‰¹é‡æµ‹è¯•
    """
    print("=" * 80)
    print("ğŸ§ª GROBIDæ‰¹é‡æµ‹è¯•å¼€å§‹")
    print("=" * 80)
    
    # æå–é“¾æ¥
    links = extract_paper_links_from_file(file_path)
    if not links:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ArXivé“¾æ¥")
        return
    
    print(f"ğŸ“„ ä»æ–‡ä»¶ä¸­æå–åˆ° {len(links)} ä¸ªè®ºæ–‡é“¾æ¥")
    
    if max_papers:
        links = links[:max_papers]
        print(f"ğŸ”¢ é™åˆ¶æµ‹è¯•å‰ {max_papers} ç¯‡è®ºæ–‡")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(links),
        'success': 0,
        'failed': 0,
        'total_affiliations': 0
    }
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(links)} ç¯‡è®ºæ–‡...")
    print("=" * 80)
    
    for i, link in enumerate(links, 1):
        print(f"\n[{i}/{len(links)}] å¤„ç†è®ºæ–‡: {link}")
        arxiv_id = parse_author_org_v3.extract_arxiv_id_from_url(link)
        
        try:
            title, affiliations = process_paper_with_title(link)
            
            if title.startswith("é”™è¯¯:"):
                stats['failed'] += 1
                print(f"âŒ å¤±è´¥ - {title}")
            else:
                stats['success'] += 1
                stats['total_affiliations'] += len(affiliations)
                
                print(f"âœ… æˆåŠŸ")
                print(f"ğŸ“ æ ‡é¢˜: {title}")
                print(f"ğŸ¢ æœºæ„æ•°é‡: {len(affiliations)}")
                
                if affiliations:
                    print("ğŸ¢ ä½œè€…æœºæ„:")
                    for j, affil in enumerate(affiliations, 1):
                        print(f"    {j}. {affil}")
                else:
                    print("ğŸ¢ æœªæ‰¾åˆ°æœºæ„ä¿¡æ¯")
                    
        except Exception as e:
            stats['failed'] += 1
            print(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…æœåŠ¡å™¨è¿‡è½½
        if i < len(links):
            time.sleep(1)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“Š æ‰¹é‡æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 80)
    print(f"ğŸ“„ æ€»è®ºæ–‡æ•°: {stats['total']}")
    print(f"âœ… æˆåŠŸå¤„ç†: {stats['success']}")
    print(f"âŒ å¤„ç†å¤±è´¥: {stats['failed']}")
    print(f"ğŸ¢ æ€»æœºæ„æ•°: {stats['total_affiliations']}")
    
    if stats['total'] > 0:
        success_rate = (stats['success'] / stats['total']) * 100
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
        
        if stats['success'] > 0:
            avg_affiliations = stats['total_affiliations'] / stats['success']
            print(f"ğŸ“Š å¹³å‡æ¯ç¯‡è®ºæ–‡æœºæ„æ•°: {avg_affiliations:.2f}")
    
    print("=" * 80)

if __name__ == "__main__":
    # é»˜è®¤æµ‹è¯•æ–‡ä»¶è·¯å¾„
    default_file = "../log/2025-07-31-cs.CV-result.md"
    
    if len(sys.argv) > 1:
        if sys.argv[1].isdigit():
            # å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ•°å­—ï¼Œåˆ™ä¸ºæœ€å¤§è®ºæ–‡æ•°
            max_papers = int(sys.argv[1])
            file_path = default_file
        else:
            # å¦åˆ™ä¸ºæ–‡ä»¶è·¯å¾„
            file_path = sys.argv[1]
            max_papers = int(sys.argv[2]) if len(sys.argv) > 2 else None
    else:
        file_path = default_file
        max_papers = None
    
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {file_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        sys.exit(1)
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    run_batch_test(file_path, max_papers)