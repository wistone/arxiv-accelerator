|   No. |   analysis_result | title | authors | abstract | link |
|------:|:------------------|:------|:--------|:---------|:-----|
|     1 | {"pass_filter":false,"exclude_reason":"core features < 2 (no multi-modal, large-scale, unified framework, or novel paradigm)","raw_score":0,"norm_score":0,"reason":"Excluded: core features insufficient (no multi-modal, large-scale, unified framework, or novel paradigm)"} | Efficient Neural Combinatorial Optimization Solver for the Min-max   Heterogeneous Capacitated Vehicle Routing Problem | Xuan Wu, Di Wang, Chunguo Wu, Kaifang Qi, Chunyan Miao, Yubin Xiao, Jian Zhang, You Zhou | Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed to address Vehicle Routing Problems (VRPs). However, most of these solvers focus exclusively on single-vehicle VRP variants, overlooking the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), which involves multiple vehicles. Existing MMHCVRP solvers typically select a vehicle and its next node to visit at each decoding step, but often make myopic decoding decisions and overlook key properties of MMHCVRP, including local topological relationships, vehicle permutation invariance, and node symmetry, resulting in suboptimal performance. To better address these limitations, we propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed dual-modality node encoder to capture local topological relationships among nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in the preceding decoding step. Finally, leveraging vehicle permutation invariance and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to stabilize the Reinforcement Learning training process. To assess the performance of ECHO, we conduct extensive experiments. The experimental results demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying numbers of vehicles and nodes, and exhibits well-performing generalization across both scales and distribution patterns. Finally, ablation studies validate the effectiveness of all proposed methods. | http://arxiv.org/abs/2507.21386v1 |
|     2 | {"pass_filter":false,"exclude_reason":"single-modality (no multi-modal elements)","raw_score":0,"norm_score":0,"reason":"Excluded: single-modality (no multi-modal elements)"} | Reservoir Computation with Networks of Differentiating Neuron Ring   Oscillators | Alexander Yeung, Peter DelMastro, Arjun Karuvally, Hava Siegelmann, Edward Rietman, Hananel Hazan | Reservoir Computing is a machine learning approach that uses the rich repertoire of complex system dynamics for function approximation. Current approaches to reservoir computing use a network of coupled integrating neurons that require a steady current to maintain activity. Here, we introduce a small world graph of differentiating neurons that are active only when there are changes in input as an alternative to integrating neurons as a reservoir computing substrate. We find the coupling strength and network topology that enable these small world networks to function as an effective reservoir. We demonstrate the efficacy of these networks in the MNIST digit recognition task, achieving comparable performance of 90.65% to existing reservoir computing approaches. The findings suggest that differentiating neurons can be a potential alternative to integrating neurons and can provide a sustainable future alternative for power-hungry AI applications. | http://arxiv.org/abs/2507.21377v1 |
|     3 | {"pass_filter":false,"exclude_reason":"no core features met (multi_modal, large_scale, unified_framework, novel_paradigm all 0)","raw_score":0,"norm_score":0,"reason":"Excluded: no core features met (multi_modal, large_scale, unified_framework, novel_paradigm all 0)"} | Load Balancing for AI Training Workloads | Sarah McClure, Sylvia Ratnasamy, Scott Shenker | We investigate the performance of various load balancing algorithms for large-scale AI training workloads that are running on dedicated infrastructure. The performance of load balancing depends on both the congestion control and loss recovery algorithms, so our evaluation also sheds light on the appropriate choices for those designs as well. | http://arxiv.org/abs/2507.21372v1 |
|     4 | {"pass_filter":false,"exclude_reason":"single-modality time series classification","raw_score":0,"norm_score":0,"reason":"Excluded: single-modality time series classification"} | A Contrastive Diffusion-based Network (CDNet) for Time Series   Classification | Yaoyu Zhang, Chi-Guhn Lee | Deep learning models are widely used for time series classification (TSC) due to their scalability and efficiency. However, their performance degrades under challenging data conditions such as class similarity, multimodal distributions, and noise. To address these limitations, we propose CDNet, a Contrastive Diffusion-based Network that enhances existing classifiers by generating informative positive and negative samples via a learned diffusion process. Unlike traditional diffusion models that denoise individual samples, CDNet learns transitions between samples--both within and across classes--through convolutional approximations of reverse diffusion steps. We introduce a theoretically grounded CNN-based mechanism to enable both denoising and mode coverage, and incorporate an uncertainty-weighted composite loss for robust training. Extensive experiments on the UCR Archive and simulated datasets demonstrate that CDNet significantly improves state-of-the-art (SOTA) deep learning classifiers, particularly under noisy, similar, and multimodal conditions. | http://arxiv.org/abs/2507.21357v1 |
|     5 | {"pass_filter":true,"exclude_reason":"","core_features":{"multi_modal":1,"large_scale":1,"unified_framework":0,"novel_paradigm":1},"plus_features":{"new_benchmark":0,"sota":0,"fusion_arch":0,"real_world_app":0,"reasoning_planning":0,"scaling_modalities":0,"open_source":0},"raw_score":6,"norm_score":6,"reason":"满足多模态、大规模模型及新颖训练范式三大核心特征，无排除条件。"} | Group Relative Augmentation for Data Efficient Action Detection | Deep Anil Patel, Iain Melvin, Zachary Izzo, Martin Renqiang Min | Adapting large Video-Language Models (VLMs) for action detection using only a few examples poses challenges like overfitting and the granularity mismatch between scene-level pre-training and required person-centric understanding. We propose an efficient adaptation strategy combining parameter-efficient tuning (LoRA) with a novel learnable internal feature augmentation. Applied within the frozen VLM backbone using FiLM, these augmentations generate diverse feature variations directly relevant to the task. Additionally, we introduce a group-weighted loss function that dynamically modulates the training contribution of each augmented sample based on its prediction divergence relative to the group average. This promotes robust learning by prioritizing informative yet reasonable augmentations. We demonstrate our method's effectiveness on complex multi-label, multi-person action detection datasets (AVA, MOMA), achieving strong mAP performance and showcasing significant data efficiency for adapting VLMs from limited examples. | http://arxiv.org/abs/2507.21353v1 |
