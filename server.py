from flask import Flask, jsonify, request, send_from_directory, Response, abort
from flask_cors import CORS
import os
import re
import json
import time
import threading
import subprocess
import hmac
import hashlib
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()  # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
except ImportError:
    print("âš ï¸  python-dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

from crawl_raw_info import crawl_arxiv_papers
from paper_analysis_processor import analyze_paper, parse_markdown_table, generate_analysis_markdown, generate_analysis_fail_markdown
from doubao_client import DoubaoClient
from auto_commit_github_api import GitHubAutoCommit

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®é™æ€æ–‡ä»¶è·¯ç”±ï¼Œå…è®¸è®¿é—®jsç›®å½•ä¸‹çš„æ–‡ä»¶
@app.route('/js/<path:filename>')
def serve_js_files(filename):
    return send_from_directory('js', filename)

# é…ç½®é™æ€æ–‡ä»¶è·¯ç”±ï¼Œå…è®¸è®¿é—®cssç›®å½•ä¸‹çš„æ–‡ä»¶
@app.route('/css/<path:filename>')
def serve_css_files(filename):
    return send_from_directory('css', filename)

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªåˆ†æè¿›åº¦
analysis_progress = {}
analysis_lock = threading.Lock()

# è®¾ç½®é™æ€æ–‡ä»¶ç›®å½•
@app.route('/')
def index():
    return send_from_directory('.', 'arxiv_assistant.html')

@app.route('/api/search_articles', methods=['POST'])
def search_articles():
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')  # é»˜è®¤ä½¿ç”¨cs.CV
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Šå¤©
        today = datetime.now().date()
        selected_date_obj = datetime.strptime(selected_date, '%Y-%m-%d').date()
        is_today = selected_date_obj == today
        
        # æ„å»ºæ–‡ä»¶å
        date_obj = datetime.strptime(selected_date, '%Y-%m-%d')
        filename = f"{date_obj.strftime('%Y-%m-%d')}-{selected_category}-result.md"
        filepath = os.path.join('log', filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•çˆ¬å–
        if not os.path.exists(filepath):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}ï¼Œå¼€å§‹çˆ¬å–æ•°æ®...")
            
            # è°ƒç”¨çˆ¬è™«å‡½æ•°
            success = crawl_arxiv_papers(selected_date, selected_category)
            
            if not success:
                return jsonify({'error': f'çˆ¬å– {selected_date} çš„ {selected_category} æ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•'}), 500
            
            # é‡æ–°æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(filepath):
                return jsonify({'error': f'çˆ¬å–å®Œæˆä½†æœªæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ä»¶: {filepath}'}), 500
        
        # è¯»å–å¹¶è§£æmarkdownæ–‡ä»¶
        articles = parse_markdown_file(filepath, selected_category)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯å¹¶åˆ é™¤ç©ºæ–‡ä»¶ï¼Œè¿™æ ·ä¸‹æ¬¡å¯ä»¥é‡æ–°å°è¯•
        if len(articles) == 0:
            # åˆ é™¤ç©ºçš„logæ–‡ä»¶å’Œç»“æœæ–‡ä»¶
            log_file = os.path.join('log', f"{date_obj.strftime('%Y-%m-%d')}-{selected_category}-log.txt")
            if os.path.exists(log_file):
                os.remove(log_file)
                print(f"å·²åˆ é™¤ç©ºçš„æ—¥å¿—æ–‡ä»¶: {log_file}")
            if os.path.exists(filepath):
                os.remove(filepath)
                print(f"å·²åˆ é™¤ç©ºçš„ç»“æœæ–‡ä»¶: {filepath}")
            
            return jsonify({
                'error': f'å½“å¤©æ²¡æœ‰æ–°çš„{selected_category}è®ºæ–‡è¢«æäº¤åˆ°arXivã€‚è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œå› ä¸ºè®ºæ–‡æäº¤å’Œç´¢å¼•éœ€è¦æ—¶é—´ã€‚å¯èƒ½æ‚¨é€‰æ‹©äº†ä»Šå¤©çš„æ—¥æœŸï¼Œæˆ–è€…arXivå‘¨æœ«æœªæ›´æ–°ã€‚'
            }), 404
        
        return jsonify({
            'success': True,
            'articles': articles,
            'total': len(articles),
            'date': selected_date,
            'category': selected_category
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def parse_markdown_file(filepath, category_filter=''):
    """è§£æmarkdownæ–‡ä»¶å¹¶æå–æ–‡ç« ä¿¡æ¯"""
    articles = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²è¡¨æ ¼è¡Œ
        lines = content.strip().split('\n')
        
        # è·³è¿‡æ ‡é¢˜è¡Œå’Œåˆ†éš”è¡Œ
        data_lines = []
        for line in lines:
            if line.startswith('|') and not line.startswith('|------'):
                data_lines.append(line)
        
        # è§£ææ¯ä¸€è¡Œæ•°æ®
        for i, line in enumerate(data_lines[1:], 1):  # è·³è¿‡è¡¨å¤´
            parts = [part.strip() for part in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ |
            
            if len(parts) >= 6:
                try:
                    number = int(parts[0])
                    number_id = parts[1]
                    title = parts[2]
                    authors = parts[3]
                    abstract = parts[4]
                    link = parts[5]
                    
                    # ä¸å†è¿›è¡Œç±»åˆ«ç­›é€‰ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»æ˜¯æŒ‰ç±»åˆ«ç”Ÿæˆçš„
                    
                    articles.append({
                        'number': number,
                        'id': number_id,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'link': link
                    })
                except (ValueError, IndexError):
                    continue
    
    except Exception as e:
        print(f"è§£ææ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
    
    return articles

@app.route('/api/analyze_articles', methods=['POST'])
def analyze_articles():
    try:
        data = request.get_json()
        selected_date = data.get('date')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ„å»ºæ–‡ä»¶å
        date_obj = datetime.strptime(selected_date, '%Y-%m-%d')
        filename = f"{date_obj.strftime('%Y-%m-%d')}-result.md"
        filepath = os.path.join('log', filename)
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„æ•°æ®æ–‡ä»¶'}), 404
        
        # è¯»å–æ–‡ç« è¿›è¡Œç®€å•åˆ†æ
        articles = parse_markdown_file(filepath)
        
        # ç®€å•çš„åˆ†æé€»è¾‘
        analysis = analyze_articles_data(articles)
        
        return jsonify({
            'success': True,
            'analysis': analysis,
            'total_articles': len(articles)
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def analyze_articles_data(articles):
    """åˆ†ææ–‡ç« æ•°æ®"""
    if not articles:
        return {
            'top_keywords': [],
            'research_areas': [],
            'summary': 'æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ•°æ®'
        }
    
    # æå–å…³é”®è¯ï¼ˆç®€å•çš„å®ç°ï¼‰
    all_text = ' '.join([article['title'] + ' ' + article['abstract'] for article in articles])
    
    # å¸¸è§çš„ç ”ç©¶é¢†åŸŸå…³é”®è¯
    keywords = {
        'computer vision': ['vision', 'image', 'detection', 'segmentation', 'recognition'],
        'machine learning': ['learning', 'neural', 'network', 'model', 'training'],
        'deep learning': ['deep', 'neural', 'transformer', 'attention'],
        'robotics': ['robot', 'control', 'navigation', 'manipulation'],
        'natural language processing': ['language', 'text', 'nlp', 'translation'],
        'audio processing': ['audio', 'speech', 'sound', 'acoustic']
    }
    
    # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
    keyword_counts = {}
    for area, area_keywords in keywords.items():
        count = sum(1 for keyword in area_keywords if keyword.lower() in all_text.lower())
        if count > 0:
            keyword_counts[area] = count
    
    # æ’åºè·å–çƒ­é—¨é¢†åŸŸ
    top_areas = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]
    
    # æå–å¸¸è§è¯æ±‡
    words = re.findall(r'\b\w+\b', all_text.lower())
    word_freq = {}
    for word in words:
        if len(word) > 3 and word not in ['the', 'and', 'for', 'with', 'this', 'that', 'from', 'have', 'been', 'they', 'their']:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        'top_keywords': [word for word, count in top_keywords],
        'research_areas': [area for area, count in top_areas],
        'summary': f'åˆ†æäº† {len(articles)} ç¯‡æ–‡ç« ï¼Œä¸»è¦ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼š{", ".join([area for area, count in top_areas])}ã€‚çƒ­é—¨å…³é”®è¯ï¼š{", ".join([word for word, count in top_keywords[:5]])}ã€‚'
    }

@app.route('/api/check_analysis_exists', methods=['POST'])
def check_analysis_exists():
    """æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åˆ†ææ–‡ä»¶
        possible_files = [
            f"{selected_date}-{selected_category}-analysis.md",  # å…¨éƒ¨åˆ†æ
            f"{selected_date}-{selected_category}-analysis-top20.md",  # å‰20ç¯‡
            f"{selected_date}-{selected_category}-analysis-top10.md",  # å‰10ç¯‡
            f"{selected_date}-{selected_category}-analysis-top5.md",   # å‰5ç¯‡
        ]
        
        existing_files = []
        for filename in possible_files:
            filepath = os.path.join('log', filename)
            if os.path.exists(filepath):
                # æ ¹æ®æ–‡ä»¶åç¡®å®šåˆ†æèŒƒå›´
                if 'top5' in filename:
                    range_type = 'top5'
                    range_desc = 'å‰5ç¯‡'
                    test_count = 5
                elif 'top10' in filename:
                    range_type = 'top10'
                    range_desc = 'å‰10ç¯‡'
                    test_count = 10
                elif 'top20' in filename:
                    range_type = 'top20'
                    range_desc = 'å‰20ç¯‡'
                    test_count = 20
                else:
                    range_type = 'full'
                    range_desc = 'å…¨éƒ¨åˆ†æ'
                    test_count = None
                
                existing_files.append({
                    'filename': filename,
                    'filepath': filepath,
                    'range_type': range_type,
                    'range_desc': range_desc,
                    'test_count': test_count
                })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šå…¨éƒ¨åˆ†æ > å‰20ç¯‡ > å‰10ç¯‡ > å‰5ç¯‡
        existing_files.sort(key=lambda x: {
            'full': 0, 'top20': 1, 'top10': 2, 'top5': 3
        }[x['range_type']])
        
        # ç¡®å®šå¯ç”¨çš„åˆ†æé€‰é¡¹
        available_options = []
        if existing_files:
            best_file = existing_files[0]
            best_range = best_file['range_type']
            
            # æ ¹æ®æœ€ä½³æ–‡ä»¶ç¡®å®šå¯ç”¨çš„é€‰é¡¹
            if best_range == 'full':
                # æœ‰å…¨éƒ¨åˆ†æï¼Œåªèƒ½é€‰æ‹©å…¨éƒ¨åˆ†æ
                available_options = ['full']
            elif best_range == 'top20':
                # æœ‰å‰20ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top20', 'full']
            elif best_range == 'top10':
                # æœ‰å‰10ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰10ç¯‡ã€å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top10', 'top20', 'full']
            elif best_range == 'top5':
                # æœ‰å‰5ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰5ç¯‡ã€å‰10ç¯‡ã€å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top5', 'top10', 'top20', 'full']
        
        return jsonify({
            'exists': len(existing_files) > 0,
            'existing_files': existing_files,
            'best_file': existing_files[0] if existing_files else None,
            'available_options': available_options
        })
        
    except Exception as e:
        return jsonify({'error': f'æ£€æŸ¥æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500

@app.route('/api/analyze_papers', methods=['POST'])
def analyze_papers():
    """å¯åŠ¨è®ºæ–‡åˆ†æ"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        test_count = data.get('test_count')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ„å»ºè¾“å…¥æ–‡ä»¶è·¯å¾„
        filename = f"{selected_date}-{selected_category}-result.md"
        filepath = os.path.join('log', filename)
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„ {selected_category} æ•°æ®æ–‡ä»¶'}), 404
        
        # åˆ›å»ºåˆ†æä»»åŠ¡ID
        task_id = f"{selected_date}-{selected_category}"
        
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡
        thread = threading.Thread(
            target=run_analysis_task,
            args=(task_id, filepath, selected_date, selected_category, test_count)
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({'success': True, 'task_id': task_id})
        
    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'}), 500

def auto_commit_analysis_file(output_file, task_id):
    """
    è‡ªåŠ¨æäº¤åˆ†æç»“æœæ–‡ä»¶åˆ° GitHub
    
    Args:
        output_file: ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
        task_id: ä»»åŠ¡IDï¼Œç”¨äºæ—¥å¿—æ ‡è¯†
    
    Returns:
        bool: æäº¤æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨æäº¤æ–‡ä»¶åˆ° GitHub: {output_file}")
        
        # åˆå§‹åŒ– GitHub æäº¤å·¥å…·
        committer = GitHubAutoCommit()
        
        # æå–æ–‡ä»¶å 
        file_name = os.path.basename(output_file)
        
        # æäº¤æ–‡ä»¶
        result = committer.commit_file_by_name(file_name)
        
        if result["success"]:
            print(f"âœ… æˆåŠŸæäº¤æ–‡ä»¶åˆ° GitHub: {file_name}")
            print(f"ğŸ”— æäº¤é“¾æ¥: {result.get('commit_url', 'N/A')}")
            return True
        else:
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"âŒ GitHub æäº¤å¤±è´¥: {error_msg}")
            return False
    
    except Exception as e:
        print(f"âš ï¸  GitHub è‡ªåŠ¨æäº¤å¼‚å¸¸ (ä»»åŠ¡: {task_id}): {e}")
        print("ğŸ“ æç¤º: æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ GITHUB_TOKEN å’Œ GITHUB_REPO é…ç½®")
        return False

def run_analysis_task(task_id, input_file, selected_date, selected_category, test_count):
    """åå°è¿è¡Œåˆ†æä»»åŠ¡"""
    import sys
    
    # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºæµï¼Œç¡®ä¿æ—¥å¿—å®æ—¶æ˜¾ç¤º
    sys.stdout.flush()
    sys.stderr.flush()
    
    print(f"ğŸš€ å¼€å§‹åˆ†æä»»åŠ¡: {task_id}, æ–‡ä»¶: {input_file}, æµ‹è¯•æ•°é‡: {test_count}")
    print(f"ğŸ·ï¸  Instance æ ‡è¯†: {os.getenv('RENDER_INSTANCE_ID', 'local')}")
    print(f"â° ä»»åŠ¡å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    try:
        with analysis_lock:
            analysis_progress[task_id] = {
                'current': 0,
                'total': 0,
                'status': 'starting',
                'paper': None,
                'analysis_result': None
            }
        
        # è¯»å–system prompt
        system_prompt_file = "prompt/system_prompt.md"
        if not os.path.exists(system_prompt_file):
            raise Exception("system_prompt.mdæ–‡ä»¶ä¸å­˜åœ¨")
            
        with open(system_prompt_file, 'r', encoding='utf-8') as f:
            system_prompt = f.read().strip()
        
        # è§£æåŸå§‹markdownæ–‡ä»¶
        papers = parse_markdown_table(input_file)
        if not papers:
            raise Exception("æ— æ³•è§£æmarkdownæ–‡ä»¶")
        
        # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ•°é‡ï¼Œåªå¤„ç†å‰Nç¯‡
        if test_count:
            papers = papers[:test_count]
        
        with analysis_lock:
            analysis_progress[task_id]['total'] = len(papers)
            analysis_progress[task_id]['status'] = 'processing'
        
        # åˆ›å»ºdoubaoå®¢æˆ·ç«¯
        print(f"ğŸ“¡ åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯... ğŸ” Task ID: {task_id} - Instance å¼€å§‹å¤„ç†")
        client = DoubaoClient()
        print(f"âœ… è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - å‡†å¤‡å¼€å§‹AIåˆ†æ")
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        print(f"ğŸ“„ å¼€å§‹å¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
        
        # æ·»åŠ è®ºæ–‡åˆ†æç»Ÿè®¡
        success_count = 0
        error_count = 0
        
        # åˆå§‹åŒ– GitHub æäº¤çŠ¶æ€
        commit_success = False
        
        for i, paper in enumerate(papers):
            try:
                with analysis_lock:
                    analysis_progress[task_id]['current'] = i + 1
                    analysis_progress[task_id]['paper'] = paper
                    analysis_progress[task_id]['analysis_result'] = None
                
                # è°ƒç”¨è®ºæ–‡åˆ†æ
                print(f"ğŸ” åˆ†æç¬¬ {i+1}/{len(papers)} ç¯‡è®ºæ–‡: {paper['title'][:50]}...")
                sys.stdout.flush()  # ç«‹å³åˆ·æ–°è¾“å‡º
                start_time = time.time()
                
                analysis_result = analyze_paper(client, system_prompt, paper['title'], paper['abstract'])
                paper['analysis_result'] = analysis_result
                
                # æ£€æŸ¥æ˜¯å¦é€šè¿‡ç­›é€‰ï¼Œå¦‚æœé€šè¿‡åˆ™è·å–ä½œè€…æœºæ„ä¿¡æ¯
                paper['author_affiliation'] = ""  # é»˜è®¤ä¸ºç©º
                
                try:
                    # è§£æåˆ†æç»“æœJSON
                    import json
                    analysis_json = json.loads(analysis_result)
                    
                    # å¦‚æœé€šè¿‡ç­›é€‰ï¼Œè°ƒç”¨è±†åŒ…APIè·å–æœºæ„ä¿¡æ¯
                    if analysis_json.get('pass_filter', False):
                        print(f"ğŸ¢ è®ºæ–‡é€šè¿‡ç­›é€‰ï¼Œæ­£åœ¨è·å–ä½œè€…æœºæ„ä¿¡æ¯...")
                        from parse_author_affli_from_doubao import get_author_affiliations
                        
                        try:
                            affiliations = get_author_affiliations(paper['link'])
                            if affiliations:
                                # å°†æœºæ„åˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨
                                paper['author_affiliation'] = json.dumps(affiliations, ensure_ascii=False)
                                print(f"âœ… æˆåŠŸè·å– {len(affiliations)} ä¸ªä½œè€…æœºæ„: {affiliations}")
                            else:
                                paper['author_affiliation'] = "[]"  # ç©ºçš„JSONæ•°ç»„
                                print("âš ï¸ æœªæ‰¾åˆ°ä½œè€…æœºæ„ä¿¡æ¯")
                        except Exception as affil_error:
                            import traceback
                            print(f"âš ï¸ è·å–ä½œè€…æœºæ„å¤±è´¥: {affil_error}")
                            print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                            paper['author_affiliation'] = ""  # å‡ºé”™æ—¶ä¿æŒä¸ºç©º
                    else:
                        print("â­ï¸ è®ºæ–‡æœªé€šè¿‡ç­›é€‰ï¼Œè·³è¿‡æœºæ„ä¿¡æ¯è·å–")
                        
                except (json.JSONDecodeError, Exception) as e:
                    print(f"âš ï¸ è§£æåˆ†æç»“æœå¤±è´¥ï¼Œè·³è¿‡æœºæ„ä¿¡æ¯è·å–: {e}")
                    paper['author_affiliation'] = ""
                
                elapsed_time = time.time() - start_time
                
                # æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦åŒ…å«é”™è¯¯
                if '"error"' in analysis_result:
                    error_count += 1
                    print(f"âš ï¸  ç¬¬ {i+1} ç¯‡è®ºæ–‡åˆ†ææœ‰é”™è¯¯ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                else:
                    success_count += 1
                    print(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                
                with analysis_lock:
                    analysis_progress[task_id]['analysis_result'] = analysis_result
                    analysis_progress[task_id]['success_count'] = success_count
                    analysis_progress[task_id]['error_count'] = error_count
                
                # æ¯10ç¯‡è®ºæ–‡è¾“å‡ºä¸€æ¬¡è¿›åº¦æ‘˜è¦
                if (i + 1) % 10 == 0 or i == len(papers) - 1:
                    print(f"ğŸ“Š è¿›åº¦æ‘˜è¦: {i+1}/{len(papers)} å®Œæˆï¼ŒæˆåŠŸ: {success_count}ï¼Œé”™è¯¯: {error_count}")
                
                # ç®€å•å»¶æ—¶ä»¥ä¾¿å‰ç«¯èƒ½çœ‹åˆ°è¿›åº¦
                time.sleep(0.1)
                
            except Exception as e:
                error_count += 1
                print(f"âŒ ç¬¬ {i+1} ç¯‡è®ºæ–‡å¤„ç†å¼‚å¸¸: {e}")
                # ç»™å‡ºé»˜è®¤é”™è¯¯ç»“æœ
                paper['analysis_result'] = f'{{"error": "Processing exception: {str(e)}"}}'
                
                with analysis_lock:
                    analysis_progress[task_id]['analysis_result'] = paper['analysis_result']
                    analysis_progress[task_id]['error_count'] = error_count
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡
                continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„åˆ†æç»“æœ
        if success_count == 0:
            # å¦‚æœæ²¡æœ‰ä»»ä½•æˆåŠŸçš„åˆ†æï¼Œç”Ÿæˆå¤±è´¥æ–‡ä»¶
            if test_count:
                if test_count <= 5:
                    output_name = f"{selected_date}-{selected_category}-analysis-top5-fail.md"
                    completed_range_type = 'top5'
                elif test_count <= 10:
                    output_name = f"{selected_date}-{selected_category}-analysis-top10-fail.md"
                    completed_range_type = 'top10'
                elif test_count <= 20:
                    output_name = f"{selected_date}-{selected_category}-analysis-top20-fail.md"
                    completed_range_type = 'top20'
                else:
                    output_name = f"{selected_date}-{selected_category}-analysis-fail.md"
                    completed_range_type = 'full'
            else:
                output_name = f"{selected_date}-{selected_category}-analysis-fail.md"
                completed_range_type = 'full'
            
            output_file = os.path.join('log', output_name)
            generate_analysis_fail_markdown(papers, output_file, error_count)
            
            # è‡ªåŠ¨æäº¤å¤±è´¥åˆ†ææ–‡ä»¶åˆ° GitHub
            print(f"ğŸ“¤ å‡†å¤‡æäº¤å¤±è´¥åˆ†ææ–‡ä»¶åˆ° GitHub...")
            commit_success = auto_commit_analysis_file(output_file, task_id)
            if commit_success:
                print(f"âœ… å¤±è´¥åˆ†ææ–‡ä»¶å·²æˆåŠŸæäº¤åˆ° GitHub")
            else:
                print(f"âš ï¸  å¤±è´¥åˆ†ææ–‡ä»¶æäº¤åˆ° GitHub å¤±è´¥ï¼Œä½†ä¸å½±å“æœ¬åœ°åˆ†æç»“æœ")
            
            print(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥ï¼æ€»è®¡: {len(papers)} ç¯‡ï¼ŒæˆåŠŸ: {success_count} ç¯‡ï¼Œé”™è¯¯: {error_count} ç¯‡")
        else:
            # å¦‚æœæœ‰æˆåŠŸçš„åˆ†æï¼Œç”Ÿæˆæ­£å¸¸æ–‡ä»¶
            if test_count:
                if test_count <= 5:
                    output_name = f"{selected_date}-{selected_category}-analysis-top5.md"
                    completed_range_type = 'top5'
                elif test_count <= 10:
                    output_name = f"{selected_date}-{selected_category}-analysis-top10.md"
                    completed_range_type = 'top10'
                elif test_count <= 20:
                    output_name = f"{selected_date}-{selected_category}-analysis-top20.md"
                    completed_range_type = 'top20'
                else:
                    output_name = f"{selected_date}-{selected_category}-analysis.md"
                    completed_range_type = 'full'
            else:
                output_name = f"{selected_date}-{selected_category}-analysis.md"
                completed_range_type = 'full'
            
            output_file = os.path.join('log', output_name)
            generate_analysis_markdown(papers, output_file)
            
            # è‡ªåŠ¨æäº¤æˆåŠŸåˆ†ææ–‡ä»¶åˆ° GitHub
            print(f"ğŸ“¤ å‡†å¤‡æäº¤åˆ†æç»“æœæ–‡ä»¶åˆ° GitHub...")
            commit_success = auto_commit_analysis_file(output_file, task_id)
            if commit_success:
                print(f"âœ… åˆ†æç»“æœæ–‡ä»¶å·²æˆåŠŸæäº¤åˆ° GitHub")
            else:
                print(f"âš ï¸  åˆ†æç»“æœæ–‡ä»¶æäº¤åˆ° GitHub å¤±è´¥ï¼Œä½†ä¸å½±å“æœ¬åœ°åˆ†æç»“æœ")
            
            print(f"ğŸŠ åˆ†æä»»åŠ¡å®Œæˆï¼æ€»è®¡: {len(papers)} ç¯‡ï¼ŒæˆåŠŸ: {success_count} ç¯‡ï¼Œé”™è¯¯: {error_count} ç¯‡")
        
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'completed'
            analysis_progress[task_id]['output_file'] = output_file
            analysis_progress[task_id]['completed_range_type'] = completed_range_type
            analysis_progress[task_id]['final_success_count'] = success_count
            analysis_progress[task_id]['final_error_count'] = error_count
            # è®°å½• GitHub æäº¤çŠ¶æ€
            analysis_progress[task_id]['github_commit_success'] = commit_success
        
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {error_msg}")
        import traceback
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'error'
            analysis_progress[task_id]['error'] = error_msg

@app.route('/api/analysis_progress')
def analysis_progress_stream():
    """Server-Sent Eventsæµï¼Œç”¨äºå®æ—¶è·å–åˆ†æè¿›åº¦"""
    # åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
    date = request.args.get('date')
    category = request.args.get('category', 'cs.CV')
    task_id = f"{date}-{category}"
    
    def generate(task_id):
        last_current = -1
        last_status = None
        loop_count = 0
        max_loops = 1800  # æœ€å¤šå¾ªç¯30åˆ†é’Ÿï¼ˆ1800ç§’ï¼‰
        
        # ç«‹å³å‘é€åˆå§‹çŠ¶æ€
        yield f"data: {json.dumps({'status': 'connecting', 'current': 0, 'total': 0}, ensure_ascii=False)}\n\n"
        
        while loop_count < max_loops:
            loop_count += 1
            
            with analysis_lock:
                progress = analysis_progress.get(task_id, {})
            
            status = progress.get('status', 'unknown')
            current = progress.get('current', 0)
            
            # å‡å°‘è°ƒè¯•ä¿¡æ¯çš„é¢‘ç‡ï¼Œåªåœ¨çŠ¶æ€æˆ–è¿›åº¦å˜åŒ–æ—¶æ‰“å°
            if status != last_status or current != last_current:
                print(f"SSE Debug - task_id: {task_id}, status: {status}, current: {current}, loop: {loop_count}")
            
            # åªåœ¨è¿›åº¦æœ‰å˜åŒ–æ—¶å‘é€æ•°æ®ï¼Œæˆ–è€…æ˜¯ç‰¹æ®ŠçŠ¶æ€
            should_send = (
                current != last_current or 
                status != last_status or
                status in ['completed', 'error', 'starting']
            )
            
            if should_send:
                data = {
                    'current': current,
                    'total': progress.get('total', 0),
                    'status': status,
                    'paper': progress.get('paper'),
                    'analysis_result': progress.get('analysis_result')
                }
                
                print(f"SSE Sending data - current: {current}, status: {status}, has_result: {bool(data['analysis_result'])}")
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                last_current = current
                last_status = status
            
            if status == 'completed':
                # å‘é€å®Œæˆäº‹ä»¶
                completion_data = {
                    'summary': f'åˆ†æå®Œæˆï¼å…±å¤„ç† {progress.get("total", 0)} ç¯‡è®ºæ–‡',
                    'completed_range_type': progress.get('completed_range_type', 'full')
                }
                yield f"event: complete\ndata: {json.dumps(completion_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream completed for task_id: {task_id}")
                break
            elif status == 'error':
                error_data = {
                    'error': progress.get('error', 'æœªçŸ¥é”™è¯¯')
                }
                yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream error for task_id: {task_id}")
                break
            
            time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # å¦‚æœå¾ªç¯è¶…æ—¶ï¼Œå‘é€è¶…æ—¶é”™è¯¯
        if loop_count >= max_loops:
            timeout_data = {'error': 'SSE stream timeout'}
            yield f"event: error\ndata: {json.dumps(timeout_data, ensure_ascii=False)}\n\n"
            print(f"SSE stream timeout for task_id: {task_id}")
    
    return Response(generate(task_id), mimetype='text/event-stream')

@app.route('/api/get_analysis_results', methods=['POST'])
def get_analysis_results():
    """è·å–åˆ†æç»“æœ"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        selected_range = data.get('range_type', 'full') # é»˜è®¤å…¨éƒ¨åˆ†æ
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ ¹æ®é€‰æ‹©çš„èŒƒå›´æ„å»ºåˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        if selected_range == 'top5':
            filename = f"{selected_date}-{selected_category}-analysis-top5.md"
            fail_filename = f"{selected_date}-{selected_category}-analysis-top5-fail.md"
        elif selected_range == 'top10':
            filename = f"{selected_date}-{selected_category}-analysis-top10.md"
            fail_filename = f"{selected_date}-{selected_category}-analysis-top10-fail.md"
        elif selected_range == 'top20':
            filename = f"{selected_date}-{selected_category}-analysis-top20.md"
            fail_filename = f"{selected_date}-{selected_category}-analysis-top20-fail.md"
        else:
            filename = f"{selected_date}-{selected_category}-analysis.md"
            fail_filename = f"{selected_date}-{selected_category}-analysis-fail.md"
        
        filepath = os.path.join('log', filename)
        fail_filepath = os.path.join('log', fail_filename)
        
        # å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤±è´¥æ–‡ä»¶
        if os.path.exists(fail_filepath):
            # è§£æå¤±è´¥æ–‡ä»¶
            fail_info = parse_analysis_fail_file(fail_filepath)
            return jsonify({
                'success': False,
                'is_analysis_failed': True,
                'fail_info': fail_info,
                'date': selected_date,
                'category': selected_category,
                'range_type': selected_range
            })
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„ {selected_category} {selected_range} åˆ†æç»“æœæ–‡ä»¶'}), 404
        
        # è§£æåˆ†æç»“æœæ–‡ä»¶
        articles = parse_analysis_markdown_file(filepath)
        
        if len(articles) == 0:
            return jsonify({'error': f'åˆ†æç»“æœæ–‡ä»¶ä¸ºç©º'}), 404
        
        return jsonify({
            'success': True,
            'articles': articles,
            'total': len(articles),
            'date': selected_date,
            'category': selected_category,
            'range_type': selected_range
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def parse_analysis_fail_file(filepath):
    """è§£æåˆ†æå¤±è´¥markdownæ–‡ä»¶"""
    fail_info = {
        'total_papers': 0,
        'error_count': 0,
        'fail_time': '',
        'papers': []
    }
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–å¤±è´¥ä¿¡æ¯
        lines = content.split('\n')
        for line in lines:
            if '**æ€»è®¡è®ºæ–‡æ•°**:' in line:
                fail_info['total_papers'] = int(line.split(':')[1].strip())
            elif '**å¤±è´¥æ•°**:' in line:
                fail_info['error_count'] = int(line.split(':')[1].strip())
            elif '**å¤±è´¥æ—¶é—´**:' in line:
                fail_info['fail_time'] = line.split(':', 1)[1].strip()
        
        # è§£æè®ºæ–‡åˆ—è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        data_lines = []
        for line in lines:
            if line.startswith('|') and not line.startswith('|------') and 'é”™è¯¯ä¿¡æ¯' not in line:
                data_lines.append(line)
        
        # è§£æè®ºæ–‡æ•°æ®è¡Œ
        for line in data_lines:
            parts = [part.strip() for part in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ |
            if len(parts) >= 6:
                paper = {
                    'no': parts[0],
                    'error_msg': parts[1],
                    'title': parts[2],
                    'authors': parts[3],
                    'abstract': parts[4],
                    'link': parts[5]
                }
                fail_info['papers'].append(paper)
        
        return fail_info
        
    except Exception as e:
        print(f"è§£æå¤±è´¥æ–‡ä»¶å‡ºé”™: {e}")
        return fail_info

def parse_analysis_markdown_file(filepath):
    """è§£æåˆ†æç»“æœmarkdownæ–‡ä»¶"""
    articles = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²è¡¨æ ¼è¡Œ
        lines = content.strip().split('\n')
        
        # è·³è¿‡æ ‡é¢˜è¡Œå’Œåˆ†éš”è¡Œ
        data_lines = []
        for line in lines:
            if line.startswith('|') and not line.startswith('|------'):
                data_lines.append(line)
        
        # è§£ææ¯ä¸€è¡Œæ•°æ®
        for i, line in enumerate(data_lines[1:], 1):  # è·³è¿‡è¡¨å¤´
            parts = [part.strip() for part in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ |
            
            if len(parts) >= 6:
                try:
                    number = int(parts[0])
                    analysis_result = parts[1].replace('\\|', '|')  # è¿˜åŸè½¬ä¹‰çš„ç®¡é“ç¬¦
                    title = parts[2].replace('\\|', '|')
                    authors = parts[3].replace('\\|', '|')
                    abstract = parts[4].replace('\\|', '|')
                    link = parts[5].replace('\\|', '|')
                    
                    # å¤„ç†æ–°çš„ author_affiliation å­—æ®µï¼ˆç¬¬7åˆ—ï¼‰
                    author_affiliation = ""
                    if len(parts) >= 7:
                        author_affiliation = parts[6].replace('\\|', '|')
                    
                    articles.append({
                        'number': number,
                        'analysis_result': analysis_result,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'link': link,
                        'author_affiliation': author_affiliation
                    })
                except (ValueError, IndexError):
                    continue
    
    except Exception as e:
        print(f"è§£æåˆ†æç»“æœæ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
    
    return articles

@app.route('/api/available_dates', methods=['GET'])
def get_available_dates():
    """è·å–å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    try:
        log_dir = 'log'
        if not os.path.exists(log_dir):
            return jsonify({'dates': []})
        
        dates = []
        for filename in os.listdir(log_dir):
            if filename.endswith('-result.md'):
                # æå–æ—¥æœŸéƒ¨åˆ†
                date_part = filename.replace('-result.md', '')
                try:
                    # éªŒè¯æ—¥æœŸæ ¼å¼
                    datetime.strptime(date_part, '%Y-%m-%d')
                    dates.append(date_part)
                except ValueError:
                    continue
        
        # æŒ‰æ—¥æœŸæ’åº
        dates.sort(reverse=True)
        
        return jsonify({'dates': dates})
        
    except Exception as e:
        return jsonify({'error': f'è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}'}), 500





if __name__ == '__main__':
    import sys
    
    # å¼ºåˆ¶åˆ·æ–°æ ‡å‡†è¾“å‡ºï¼Œç¡®ä¿åœ¨Renderä¸­èƒ½çœ‹åˆ°æ—¥å¿—
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)
    
    print("å¯åŠ¨Arxivæ–‡ç« åˆç­›å°åŠ©æ‰‹æœåŠ¡å™¨...")
    print("è®¿é—®åœ°å€: http://localhost:8080")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¦ç”¨debugæ¨¡å¼ï¼Œä½†ä¿æŒæ—¥å¿—è¾“å‡º
    is_production = os.getenv('RENDER') is not None
    if is_production:
        print("ğŸŒ æ£€æµ‹åˆ°Renderç”Ÿäº§ç¯å¢ƒï¼Œä¼˜åŒ–æ—¥å¿—é…ç½®")
        app.run(debug=False, host='0.0.0.0', port=8080)
    else:
        print("ğŸ–¥ï¸  æœ¬åœ°å¼€å‘ç¯å¢ƒ")
        app.run(debug=True, host='0.0.0.0', port=8080) 