from flask import Flask, jsonify, request, send_from_directory, Response, abort
from flask_cors import CORS
import os
import re
import json
import time
import threading
import subprocess
import hmac
import hashlib
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()  # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
except ImportError:
    print("âš ï¸  python-dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

from crawl_raw_info import crawl_arxiv_papers
from paper_analysis_processor import analyze_paper, parse_markdown_table, generate_analysis_markdown
from doubao_client import DoubaoClient

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªåˆ†æè¿›åº¦
analysis_progress = {}
analysis_lock = threading.Lock()

# è®¾ç½®é™æ€æ–‡ä»¶ç›®å½•
@app.route('/')
def index():
    return send_from_directory('.', 'arxiv_assistant.html')

@app.route('/api/search_articles', methods=['POST'])
def search_articles():
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')  # é»˜è®¤ä½¿ç”¨cs.CV
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Šå¤©
        today = datetime.now().date()
        selected_date_obj = datetime.strptime(selected_date, '%Y-%m-%d').date()
        is_today = selected_date_obj == today
        
        # æ„å»ºæ–‡ä»¶å
        date_obj = datetime.strptime(selected_date, '%Y-%m-%d')
        filename = f"{date_obj.strftime('%Y-%m-%d')}-{selected_category}-result.md"
        filepath = os.path.join('log', filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•çˆ¬å–
        if not os.path.exists(filepath):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}ï¼Œå¼€å§‹çˆ¬å–æ•°æ®...")
            
            # è°ƒç”¨çˆ¬è™«å‡½æ•°
            success = crawl_arxiv_papers(selected_date, selected_category)
            
            if not success:
                return jsonify({'error': f'çˆ¬å– {selected_date} çš„ {selected_category} æ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•'}), 500
            
            # é‡æ–°æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(filepath):
                return jsonify({'error': f'çˆ¬å–å®Œæˆä½†æœªæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ä»¶: {filepath}'}), 500
        
        # è¯»å–å¹¶è§£æmarkdownæ–‡ä»¶
        articles = parse_markdown_file(filepath, selected_category)
        
        # å¦‚æœæ˜¯ä»Šå¤©ä¸”æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œè¿”å›ç‰¹æ®Šé”™è¯¯ä¿¡æ¯å¹¶åˆ é™¤logæ–‡ä»¶
        if is_today and len(articles) == 0:
            # åˆ é™¤å½“å¤©çš„logæ–‡ä»¶
            log_file = os.path.join('log', f"{date_obj.strftime('%Y-%m-%d')}-{selected_category}-log.txt")
            if os.path.exists(log_file):
                os.remove(log_file)
            if os.path.exists(filepath):
                os.remove(filepath)
            
            return jsonify({
                'error': f'ä»Šå¤©æ²¡æœ‰æ–°çš„{selected_category}è®ºæ–‡è¢«æäº¤åˆ°arXivã€‚è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œå› ä¸ºè®ºæ–‡æäº¤å’Œç´¢å¼•éœ€è¦æ—¶é—´ã€‚'
            }), 404
        
        # å¦‚æœä¸æ˜¯ä»Šå¤©ä¸”æ²¡æœ‰è®ºæ–‡ï¼Œè¿”å›ä¸€èˆ¬æ€§é”™è¯¯
        if len(articles) == 0:
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„ {selected_category} è®ºæ–‡æ•°æ®'}), 404
        
        return jsonify({
            'success': True,
            'articles': articles,
            'total': len(articles),
            'date': selected_date,
            'category': selected_category
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def parse_markdown_file(filepath, category_filter=''):
    """è§£æmarkdownæ–‡ä»¶å¹¶æå–æ–‡ç« ä¿¡æ¯"""
    articles = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²è¡¨æ ¼è¡Œ
        lines = content.strip().split('\n')
        
        # è·³è¿‡æ ‡é¢˜è¡Œå’Œåˆ†éš”è¡Œ
        data_lines = []
        for line in lines:
            if line.startswith('|') and not line.startswith('|------'):
                data_lines.append(line)
        
        # è§£ææ¯ä¸€è¡Œæ•°æ®
        for i, line in enumerate(data_lines[1:], 1):  # è·³è¿‡è¡¨å¤´
            parts = [part.strip() for part in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ |
            
            if len(parts) >= 6:
                try:
                    number = int(parts[0])
                    number_id = parts[1]
                    title = parts[2]
                    authors = parts[3]
                    abstract = parts[4]
                    link = parts[5]
                    
                    # ä¸å†è¿›è¡Œç±»åˆ«ç­›é€‰ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»æ˜¯æŒ‰ç±»åˆ«ç”Ÿæˆçš„
                    
                    articles.append({
                        'number': number,
                        'id': number_id,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'link': link
                    })
                except (ValueError, IndexError):
                    continue
    
    except Exception as e:
        print(f"è§£ææ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
    
    return articles

@app.route('/api/analyze_articles', methods=['POST'])
def analyze_articles():
    try:
        data = request.get_json()
        selected_date = data.get('date')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ„å»ºæ–‡ä»¶å
        date_obj = datetime.strptime(selected_date, '%Y-%m-%d')
        filename = f"{date_obj.strftime('%Y-%m-%d')}-result.md"
        filepath = os.path.join('log', filename)
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„æ•°æ®æ–‡ä»¶'}), 404
        
        # è¯»å–æ–‡ç« è¿›è¡Œç®€å•åˆ†æ
        articles = parse_markdown_file(filepath)
        
        # ç®€å•çš„åˆ†æé€»è¾‘
        analysis = analyze_articles_data(articles)
        
        return jsonify({
            'success': True,
            'analysis': analysis,
            'total_articles': len(articles)
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def analyze_articles_data(articles):
    """åˆ†ææ–‡ç« æ•°æ®"""
    if not articles:
        return {
            'top_keywords': [],
            'research_areas': [],
            'summary': 'æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ•°æ®'
        }
    
    # æå–å…³é”®è¯ï¼ˆç®€å•çš„å®ç°ï¼‰
    all_text = ' '.join([article['title'] + ' ' + article['abstract'] for article in articles])
    
    # å¸¸è§çš„ç ”ç©¶é¢†åŸŸå…³é”®è¯
    keywords = {
        'computer vision': ['vision', 'image', 'detection', 'segmentation', 'recognition'],
        'machine learning': ['learning', 'neural', 'network', 'model', 'training'],
        'deep learning': ['deep', 'neural', 'transformer', 'attention'],
        'robotics': ['robot', 'control', 'navigation', 'manipulation'],
        'natural language processing': ['language', 'text', 'nlp', 'translation'],
        'audio processing': ['audio', 'speech', 'sound', 'acoustic']
    }
    
    # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
    keyword_counts = {}
    for area, area_keywords in keywords.items():
        count = sum(1 for keyword in area_keywords if keyword.lower() in all_text.lower())
        if count > 0:
            keyword_counts[area] = count
    
    # æ’åºè·å–çƒ­é—¨é¢†åŸŸ
    top_areas = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]
    
    # æå–å¸¸è§è¯æ±‡
    words = re.findall(r'\b\w+\b', all_text.lower())
    word_freq = {}
    for word in words:
        if len(word) > 3 and word not in ['the', 'and', 'for', 'with', 'this', 'that', 'from', 'have', 'been', 'they', 'their']:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        'top_keywords': [word for word, count in top_keywords],
        'research_areas': [area for area, count in top_areas],
        'summary': f'åˆ†æäº† {len(articles)} ç¯‡æ–‡ç« ï¼Œä¸»è¦ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼š{", ".join([area for area, count in top_areas])}ã€‚çƒ­é—¨å…³é”®è¯ï¼š{", ".join([word for word, count in top_keywords[:5]])}ã€‚'
    }

@app.route('/api/check_analysis_exists', methods=['POST'])
def check_analysis_exists():
    """æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åˆ†ææ–‡ä»¶
        possible_files = [
            f"{selected_date}-{selected_category}-analysis.md",  # å…¨éƒ¨åˆ†æ
            f"{selected_date}-{selected_category}-analysis-top20.md",  # å‰20ç¯‡
            f"{selected_date}-{selected_category}-analysis-top10.md",  # å‰10ç¯‡
            f"{selected_date}-{selected_category}-analysis-top5.md",   # å‰5ç¯‡
        ]
        
        existing_files = []
        for filename in possible_files:
            filepath = os.path.join('log', filename)
            if os.path.exists(filepath):
                # æ ¹æ®æ–‡ä»¶åç¡®å®šåˆ†æèŒƒå›´
                if 'top5' in filename:
                    range_type = 'top5'
                    range_desc = 'å‰5ç¯‡'
                    test_count = 5
                elif 'top10' in filename:
                    range_type = 'top10'
                    range_desc = 'å‰10ç¯‡'
                    test_count = 10
                elif 'top20' in filename:
                    range_type = 'top20'
                    range_desc = 'å‰20ç¯‡'
                    test_count = 20
                else:
                    range_type = 'full'
                    range_desc = 'å…¨éƒ¨åˆ†æ'
                    test_count = None
                
                existing_files.append({
                    'filename': filename,
                    'filepath': filepath,
                    'range_type': range_type,
                    'range_desc': range_desc,
                    'test_count': test_count
                })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šå…¨éƒ¨åˆ†æ > å‰20ç¯‡ > å‰10ç¯‡ > å‰5ç¯‡
        existing_files.sort(key=lambda x: {
            'full': 0, 'top20': 1, 'top10': 2, 'top5': 3
        }[x['range_type']])
        
        # ç¡®å®šå¯ç”¨çš„åˆ†æé€‰é¡¹
        available_options = []
        if existing_files:
            best_file = existing_files[0]
            best_range = best_file['range_type']
            
            # æ ¹æ®æœ€ä½³æ–‡ä»¶ç¡®å®šå¯ç”¨çš„é€‰é¡¹
            if best_range == 'full':
                # æœ‰å…¨éƒ¨åˆ†æï¼Œåªèƒ½é€‰æ‹©å…¨éƒ¨åˆ†æ
                available_options = ['full']
            elif best_range == 'top20':
                # æœ‰å‰20ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top20', 'full']
            elif best_range == 'top10':
                # æœ‰å‰10ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰10ç¯‡ã€å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top10', 'top20', 'full']
            elif best_range == 'top5':
                # æœ‰å‰5ç¯‡åˆ†æï¼Œå¯ä»¥é€‰æ‹©å‰5ç¯‡ã€å‰10ç¯‡ã€å‰20ç¯‡æˆ–é‡æ–°ç”Ÿæˆå…¨éƒ¨åˆ†æ
                available_options = ['top5', 'top10', 'top20', 'full']
        
        return jsonify({
            'exists': len(existing_files) > 0,
            'existing_files': existing_files,
            'best_file': existing_files[0] if existing_files else None,
            'available_options': available_options
        })
        
    except Exception as e:
        return jsonify({'error': f'æ£€æŸ¥æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500

@app.route('/api/analyze_papers', methods=['POST'])
def analyze_papers():
    """å¯åŠ¨è®ºæ–‡åˆ†æ"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        test_count = data.get('test_count')
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ„å»ºè¾“å…¥æ–‡ä»¶è·¯å¾„
        filename = f"{selected_date}-{selected_category}-result.md"
        filepath = os.path.join('log', filename)
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„ {selected_category} æ•°æ®æ–‡ä»¶'}), 404
        
        # åˆ›å»ºåˆ†æä»»åŠ¡ID
        task_id = f"{selected_date}-{selected_category}"
        
        # å¯åŠ¨åå°åˆ†æä»»åŠ¡
        thread = threading.Thread(
            target=run_analysis_task,
            args=(task_id, filepath, selected_date, selected_category, test_count)
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({'success': True, 'task_id': task_id})
        
    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'}), 500

def run_analysis_task(task_id, input_file, selected_date, selected_category, test_count):
    """åå°è¿è¡Œåˆ†æä»»åŠ¡"""
    print(f"ğŸš€ å¼€å§‹åˆ†æä»»åŠ¡: {task_id}, æ–‡ä»¶: {input_file}, æµ‹è¯•æ•°é‡: {test_count}")
    try:
        with analysis_lock:
            analysis_progress[task_id] = {
                'current': 0,
                'total': 0,
                'status': 'starting',
                'paper': None,
                'analysis_result': None
            }
        
        # è¯»å–system prompt
        system_prompt_file = "prompt/system_prompt.md"
        if not os.path.exists(system_prompt_file):
            raise Exception("system_prompt.mdæ–‡ä»¶ä¸å­˜åœ¨")
            
        with open(system_prompt_file, 'r', encoding='utf-8') as f:
            system_prompt = f.read().strip()
        
        # è§£æåŸå§‹markdownæ–‡ä»¶
        papers = parse_markdown_table(input_file)
        if not papers:
            raise Exception("æ— æ³•è§£æmarkdownæ–‡ä»¶")
        
        # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ•°é‡ï¼Œåªå¤„ç†å‰Nç¯‡
        if test_count:
            papers = papers[:test_count]
        
        with analysis_lock:
            analysis_progress[task_id]['total'] = len(papers)
            analysis_progress[task_id]['status'] = 'processing'
        
        # åˆ›å»ºdoubaoå®¢æˆ·ç«¯
        print(f"ğŸ“¡ åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯...")
        client = DoubaoClient()
        print(f"âœ… è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        print(f"ğŸ“„ å¼€å§‹å¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
        
        # æ·»åŠ è®ºæ–‡åˆ†æç»Ÿè®¡
        success_count = 0
        error_count = 0
        
        for i, paper in enumerate(papers):
            try:
                with analysis_lock:
                    analysis_progress[task_id]['current'] = i + 1
                    analysis_progress[task_id]['paper'] = paper
                    analysis_progress[task_id]['analysis_result'] = None
                
                # è°ƒç”¨è®ºæ–‡åˆ†æ
                print(f"ğŸ” åˆ†æç¬¬ {i+1}/{len(papers)} ç¯‡è®ºæ–‡: {paper['title'][:50]}...")
                start_time = time.time()
                
                analysis_result = analyze_paper(client, system_prompt, paper['title'], paper['abstract'])
                paper['analysis_result'] = analysis_result
                
                elapsed_time = time.time() - start_time
                
                # æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦åŒ…å«é”™è¯¯
                if '"error"' in analysis_result:
                    error_count += 1
                    print(f"âš ï¸  ç¬¬ {i+1} ç¯‡è®ºæ–‡åˆ†ææœ‰é”™è¯¯ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                else:
                    success_count += 1
                    print(f"âœ… ç¬¬ {i+1} ç¯‡è®ºæ–‡åˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                
                with analysis_lock:
                    analysis_progress[task_id]['analysis_result'] = analysis_result
                    analysis_progress[task_id]['success_count'] = success_count
                    analysis_progress[task_id]['error_count'] = error_count
                
                # æ¯10ç¯‡è®ºæ–‡è¾“å‡ºä¸€æ¬¡è¿›åº¦æ‘˜è¦
                if (i + 1) % 10 == 0 or i == len(papers) - 1:
                    print(f"ğŸ“Š è¿›åº¦æ‘˜è¦: {i+1}/{len(papers)} å®Œæˆï¼ŒæˆåŠŸ: {success_count}ï¼Œé”™è¯¯: {error_count}")
                
                # ç®€å•å»¶æ—¶ä»¥ä¾¿å‰ç«¯èƒ½çœ‹åˆ°è¿›åº¦
                time.sleep(0.1)
                
            except Exception as e:
                error_count += 1
                print(f"âŒ ç¬¬ {i+1} ç¯‡è®ºæ–‡å¤„ç†å¼‚å¸¸: {e}")
                # ç»™å‡ºé»˜è®¤é”™è¯¯ç»“æœ
                paper['analysis_result'] = f'{{"error": "Processing exception: {str(e)}"}}'
                
                with analysis_lock:
                    analysis_progress[task_id]['analysis_result'] = paper['analysis_result']
                    analysis_progress[task_id]['error_count'] = error_count
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡
                continue
        
        # æ ¹æ®test_countç”Ÿæˆä¸åŒçš„è¾“å‡ºæ–‡ä»¶å
        if test_count:
            if test_count <= 5:
                output_name = f"{selected_date}-{selected_category}-analysis-top5.md"
                completed_range_type = 'top5'
            elif test_count <= 10:
                output_name = f"{selected_date}-{selected_category}-analysis-top10.md"
                completed_range_type = 'top10'
            elif test_count <= 20:
                output_name = f"{selected_date}-{selected_category}-analysis-top20.md"
                completed_range_type = 'top20'
            else:
                output_name = f"{selected_date}-{selected_category}-analysis.md"
                completed_range_type = 'full'
        else:
            output_name = f"{selected_date}-{selected_category}-analysis.md"
            completed_range_type = 'full'
        
        output_file = os.path.join('log', output_name)
        generate_analysis_markdown(papers, output_file)
        
        print(f"ğŸŠ åˆ†æä»»åŠ¡å®Œæˆï¼æ€»è®¡: {len(papers)} ç¯‡ï¼ŒæˆåŠŸ: {success_count} ç¯‡ï¼Œé”™è¯¯: {error_count} ç¯‡")
        
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'completed'
            analysis_progress[task_id]['output_file'] = output_file
            analysis_progress[task_id]['completed_range_type'] = completed_range_type
            analysis_progress[task_id]['final_success_count'] = success_count
            analysis_progress[task_id]['final_error_count'] = error_count
        
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {error_msg}")
        import traceback
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'error'
            analysis_progress[task_id]['error'] = error_msg

@app.route('/api/analysis_progress')
def analysis_progress_stream():
    """Server-Sent Eventsæµï¼Œç”¨äºå®æ—¶è·å–åˆ†æè¿›åº¦"""
    # åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
    date = request.args.get('date')
    category = request.args.get('category', 'cs.CV')
    task_id = f"{date}-{category}"
    
    def generate(task_id):
        last_current = -1
        last_status = None
        loop_count = 0
        max_loops = 1800  # æœ€å¤šå¾ªç¯30åˆ†é’Ÿï¼ˆ1800ç§’ï¼‰
        
        # ç«‹å³å‘é€åˆå§‹çŠ¶æ€
        yield f"data: {json.dumps({'status': 'connecting', 'current': 0, 'total': 0}, ensure_ascii=False)}\n\n"
        
        while loop_count < max_loops:
            loop_count += 1
            
            with analysis_lock:
                progress = analysis_progress.get(task_id, {})
            
            status = progress.get('status', 'unknown')
            current = progress.get('current', 0)
            
            # å‡å°‘è°ƒè¯•ä¿¡æ¯çš„é¢‘ç‡ï¼Œåªåœ¨çŠ¶æ€æˆ–è¿›åº¦å˜åŒ–æ—¶æ‰“å°
            if status != last_status or current != last_current:
                print(f"SSE Debug - task_id: {task_id}, status: {status}, current: {current}, loop: {loop_count}")
            
            # åªåœ¨è¿›åº¦æœ‰å˜åŒ–æ—¶å‘é€æ•°æ®ï¼Œæˆ–è€…æ˜¯ç‰¹æ®ŠçŠ¶æ€
            should_send = (
                current != last_current or 
                status != last_status or
                status in ['completed', 'error', 'starting']
            )
            
            if should_send:
                data = {
                    'current': current,
                    'total': progress.get('total', 0),
                    'status': status,
                    'paper': progress.get('paper'),
                    'analysis_result': progress.get('analysis_result')
                }
                
                print(f"SSE Sending data - current: {current}, status: {status}, has_result: {bool(data['analysis_result'])}")
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                last_current = current
                last_status = status
            
            if status == 'completed':
                # å‘é€å®Œæˆäº‹ä»¶
                completion_data = {
                    'summary': f'åˆ†æå®Œæˆï¼å…±å¤„ç† {progress.get("total", 0)} ç¯‡è®ºæ–‡',
                    'completed_range_type': progress.get('completed_range_type', 'full')
                }
                yield f"event: complete\ndata: {json.dumps(completion_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream completed for task_id: {task_id}")
                break
            elif status == 'error':
                error_data = {
                    'error': progress.get('error', 'æœªçŸ¥é”™è¯¯')
                }
                yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream error for task_id: {task_id}")
                break
            
            time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # å¦‚æœå¾ªç¯è¶…æ—¶ï¼Œå‘é€è¶…æ—¶é”™è¯¯
        if loop_count >= max_loops:
            timeout_data = {'error': 'SSE stream timeout'}
            yield f"event: error\ndata: {json.dumps(timeout_data, ensure_ascii=False)}\n\n"
            print(f"SSE stream timeout for task_id: {task_id}")
    
    return Response(generate(task_id), mimetype='text/event-stream')

@app.route('/api/get_analysis_results', methods=['POST'])
def get_analysis_results():
    """è·å–åˆ†æç»“æœ"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        selected_range = data.get('range_type', 'full') # é»˜è®¤å…¨éƒ¨åˆ†æ
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # æ ¹æ®é€‰æ‹©çš„èŒƒå›´æ„å»ºåˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        if selected_range == 'top5':
            filename = f"{selected_date}-{selected_category}-analysis-top5.md"
        elif selected_range == 'top10':
            filename = f"{selected_date}-{selected_category}-analysis-top10.md"
        elif selected_range == 'top20':
            filename = f"{selected_date}-{selected_category}-analysis-top20.md"
        else:
            filename = f"{selected_date}-{selected_category}-analysis.md"
        
        filepath = os.path.join('log', filename)
        
        if not os.path.exists(filepath):
            return jsonify({'error': f'æœªæ‰¾åˆ° {selected_date} çš„ {selected_category} {selected_range} åˆ†æç»“æœæ–‡ä»¶'}), 404
        
        # è§£æåˆ†æç»“æœæ–‡ä»¶
        articles = parse_analysis_markdown_file(filepath)
        
        if len(articles) == 0:
            return jsonify({'error': f'åˆ†æç»“æœæ–‡ä»¶ä¸ºç©º'}), 404
        
        return jsonify({
            'success': True,
            'articles': articles,
            'total': len(articles),
            'date': selected_date,
            'category': selected_category,
            'range_type': selected_range
        })
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

def parse_analysis_markdown_file(filepath):
    """è§£æåˆ†æç»“æœmarkdownæ–‡ä»¶"""
    articles = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²è¡¨æ ¼è¡Œ
        lines = content.strip().split('\n')
        
        # è·³è¿‡æ ‡é¢˜è¡Œå’Œåˆ†éš”è¡Œ
        data_lines = []
        for line in lines:
            if line.startswith('|') and not line.startswith('|------'):
                data_lines.append(line)
        
        # è§£ææ¯ä¸€è¡Œæ•°æ®
        for i, line in enumerate(data_lines[1:], 1):  # è·³è¿‡è¡¨å¤´
            parts = [part.strip() for part in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ |
            
            if len(parts) >= 6:
                try:
                    number = int(parts[0])
                    analysis_result = parts[1].replace('\\|', '|')  # è¿˜åŸè½¬ä¹‰çš„ç®¡é“ç¬¦
                    title = parts[2].replace('\\|', '|')
                    authors = parts[3].replace('\\|', '|')
                    abstract = parts[4].replace('\\|', '|')
                    link = parts[5].replace('\\|', '|')
                    
                    articles.append({
                        'number': number,
                        'analysis_result': analysis_result,
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'link': link
                    })
                except (ValueError, IndexError):
                    continue
    
    except Exception as e:
        print(f"è§£æåˆ†æç»“æœæ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
    
    return articles

@app.route('/api/available_dates', methods=['GET'])
def get_available_dates():
    """è·å–å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    try:
        log_dir = 'log'
        if not os.path.exists(log_dir):
            return jsonify({'dates': []})
        
        dates = []
        for filename in os.listdir(log_dir):
            if filename.endswith('-result.md'):
                # æå–æ—¥æœŸéƒ¨åˆ†
                date_part = filename.replace('-result.md', '')
                try:
                    # éªŒè¯æ—¥æœŸæ ¼å¼
                    datetime.strptime(date_part, '%Y-%m-%d')
                    dates.append(date_part)
                except ValueError:
                    continue
        
        # æŒ‰æ—¥æœŸæ’åº
        dates.sort(reverse=True)
        
        return jsonify({'dates': dates})
        
    except Exception as e:
        return jsonify({'error': f'è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/internal/backup', methods=['POST'])
def trigger_backup():
    """
    å†…éƒ¨å¤‡ä»½API - é€šè¿‡GitHub Actionsè§¦å‘
    è¿”å›éœ€è¦å¤‡ä»½çš„æ–‡ä»¶åˆ—è¡¨å’Œå†…å®¹ï¼Œç”±GitHub Actionsæ‰§è¡ŒGitæ“ä½œ
    """
    # è·å–å¤‡ä»½å¯†é’¥
    SECRET = os.getenv("BACKUP_SECRET", "change-me-please")
    
    # éªŒè¯ç­¾å
    sig = request.headers.get("X-Backup-Sign")
    if not sig:
        print("âŒ å¤‡ä»½è¯·æ±‚ç¼ºå°‘ç­¾å")
        abort(403)
    
    # è®¡ç®—æœŸæœ›çš„ç­¾å
    expected_sig = hmac.new(SECRET.encode(), b"run", hashlib.sha256).hexdigest()
    
    # å®‰å…¨æ¯”è¾ƒç­¾å
    if not hmac.compare_digest(sig, expected_sig):
        print(f"âŒ å¤‡ä»½è¯·æ±‚ç­¾åéªŒè¯å¤±è´¥")
        abort(403)
    
    print("ğŸ” å¤‡ä»½è¯·æ±‚ç­¾åéªŒè¯é€šè¿‡")
    
    try:
        print("ğŸš€ å¼€å§‹æ£€æŸ¥éœ€è¦å¤‡ä»½çš„æ–‡ä»¶...")
        
        # æ£€æŸ¥logç›®å½•
        log_dir = "log"
        if not os.path.exists(log_dir):
            return {
                "ok": True, 
                "message": "logç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½",
                "files": [],
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        
        # æŸ¥æ‰¾æ‰€æœ‰åˆ†ææ–‡ä»¶
        analysis_files = []
        for filename in os.listdir(log_dir):
            if "-analysis" in filename and filename.endswith(".md"):
                filepath = os.path.join(log_dir, filename)
                if os.path.isfile(filepath):
                    analysis_files.append(filename)
        
        if not analysis_files:
            return {
                "ok": True, 
                "message": "æ²¡æœ‰æ‰¾åˆ°åˆ†ææ–‡ä»¶ï¼Œæ— éœ€å¤‡ä»½",
                "files": [],
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        
        print(f"ğŸ“„ å‘ç° {len(analysis_files)} ä¸ªåˆ†ææ–‡ä»¶")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_contents = {}
        for filename in analysis_files:
            filepath = os.path.join(log_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                file_contents[filename] = {
                    "path": f"log/{filename}",
                    "content": content,
                    "size": len(content)
                }
                print(f"   âœ… è¯»å–æ–‡ä»¶: {filename} ({len(content)} å­—ç¬¦)")
            except Exception as e:
                print(f"   âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {filename} - {e}")
                continue
        
        if not file_contents:
            return {
                "ok": False, 
                "error": "æ— æ³•è¯»å–ä»»ä½•åˆ†ææ–‡ä»¶",
                "files": []
            }, 500
        
        print("âœ… æ–‡ä»¶å†…å®¹è¯»å–å®Œæˆ")
        return {
            "ok": True, 
            "message": f"æˆåŠŸè¯»å– {len(file_contents)} ä¸ªåˆ†ææ–‡ä»¶",
            "files": file_contents,
            "file_count": len(file_contents),
            "total_size": sum(f["size"] for f in file_contents.values()),
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
            
    except Exception as e:
        error_msg = f"å¤‡ä»½æ£€æŸ¥å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        import traceback
        traceback.print_exc()
        return {
            "ok": False, 
            "error": error_msg
        }, 500

if __name__ == '__main__':
    print("å¯åŠ¨Arxivæ–‡ç« åˆç­›å°åŠ©æ‰‹æœåŠ¡å™¨...")
    print("è®¿é—®åœ°å€: http://localhost:8080")
    app.run(debug=True, host='0.0.0.0', port=8080) 