from flask import Flask, jsonify, request, send_from_directory, Response, abort
from flask_cors import CORS
import os
import re
import json
import time
import threading
import subprocess
import hmac
import hashlib
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()  # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
except ImportError:
    print("âš ï¸  python-dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

# å¯¼å…¥é‡æ„åçš„æœåŠ¡å±‚
from backend.services.analysis_service import analyze_paper
from backend.services.arxiv_service import import_arxiv_papers
from backend.services.affiliation_service import get_author_affiliations, clear_affiliation_cache
from backend.services.concurrent_analysis_service import get_concurrent_service, run_performance_comparison
from backend.clients.ai_client import DoubaoClient
from backend.db import repo as db_repo

# å‘åå…¼å®¹çš„åˆ«å
import_arxiv_papers_to_db = import_arxiv_papers

# ğŸ“¦ ç®€å•çš„å†…å­˜ç¼“å­˜ï¼ˆç”Ÿäº§ç¯å¢ƒå¯ç”¨Redisï¼‰
_search_cache = {}
_cache_expiry = {}
CACHE_TTL = 300  # 5åˆ†é’Ÿç¼“å­˜

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®é™æ€æ–‡ä»¶è·¯ç”±ï¼Œå…è®¸è®¿é—®jsç›®å½•ä¸‹çš„æ–‡ä»¶
@app.route('/js/<path:filename>')
def serve_js_files(filename):
    return send_from_directory('frontend/js', filename)

# é…ç½®é™æ€æ–‡ä»¶è·¯ç”±ï¼Œå…è®¸è®¿é—®cssç›®å½•ä¸‹çš„æ–‡ä»¶
@app.route('/css/<path:filename>')
def serve_css_files(filename):
    return send_from_directory('frontend/css', filename)

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªåˆ†æè¿›åº¦
analysis_progress = {}
analysis_lock = threading.Lock()

# è®¾ç½®é™æ€æ–‡ä»¶ç›®å½•
@app.route('/')
def index():
    return send_from_directory('./frontend', 'index.html')

@app.route('/intro.html')
@app.route('/intro')
def intro():
    return send_from_directory('./frontend', 'intro.html')

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºRenderéƒ¨ç½²éªŒè¯"""
    return jsonify({
        'status': 'healthy',
        'service': 'arxiv-accelerator',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

@app.route('/api/search_articles', methods=['POST'])
def search_articles():
    import time
    
    try:
        total_start = time.time()
        
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')

        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400

        print(f"ğŸš€ [æœç´¢æ€§èƒ½] å¼€å§‹æœç´¢ | date={selected_date} category={selected_category}")

        # ğŸš€ æ–°ç­–ç•¥ï¼šåŸºäºæ—¶é—´çš„æ™ºèƒ½ç¼“å­˜ï¼ˆè€Œéå®Œå…¨è·³è¿‡APIï¼‰
        import_time = 0
        stats = {'processed': 0, 'total_upsert': 0}
        current_time = time.time()
        
        # æ£€æŸ¥æœ€è¿‘æ˜¯å¦å·²ç»å¯¼å…¥è¿‡ï¼ˆçŸ­æ—¶é—´ç¼“å­˜ï¼‰
        import_cache_key = f"import_{selected_date}_{selected_category}"
        should_skip_import = False
        
        if import_cache_key in _cache_expiry:
            if current_time < _cache_expiry[import_cache_key]:
                # 30åˆ†é’Ÿå†…å·²å¯¼å…¥è¿‡ï¼Œè·³è¿‡ArXiv API
                should_skip_import = True
                print(f"âš¡ [æœç´¢æ€§èƒ½] 30åˆ†é’Ÿå†…å·²å¯¼å…¥ï¼Œè·³è¿‡ArXiv APIè°ƒç”¨")
        
        # åˆå§‹åŒ–å˜é‡
        import_time = 0
        stats = {'processed': 0, 'total_upsert': 0}
        skip_db_read = False
        
        if not should_skip_import:
            # ğŸš€ æ–°ç­–ç•¥ï¼šæ™ºèƒ½æ£€æŸ¥æ˜¯å¦çœŸçš„éœ€è¦å¯¼å…¥
            smart_check_start = time.time()
            
            # 1) å…ˆè·å–ArXiv APIæ•°æ®ï¼ˆè½»é‡çº§ï¼Œåªè·å–IDåˆ—è¡¨ï¼‰
            arxiv_ids = db_repo.get_arxiv_ids_from_api(selected_date, selected_category)
            api_check_time = time.time() - smart_check_start
            print(f"â±ï¸  [æœç´¢æ€§èƒ½] ArXiv API IDæ£€æŸ¥å®Œæˆï¼Œè€—æ—¶: {api_check_time:.2f}s | ArXivè¿”å› {len(arxiv_ids)} æ¡")
            
            if not arxiv_ids:
                print(f"ğŸ“­ [æœç´¢æ€§èƒ½] ArXiv APIæ— æ•°æ®ï¼Œè·³è¿‡å¯¼å…¥")
                import_time = 0
            else:
                # ğŸš€ æ–°ä¼˜åŒ–ï¼šä¸€ä½“åŒ–æ£€æŸ¥+è¯»å–ï¼Œé¿å…ä¸¤æ¬¡DBæŸ¥è¯¢
                unified_start = time.time()
                result = db_repo.smart_check_and_read(selected_date, selected_category, arxiv_ids)
                unified_time = time.time() - unified_start
                
                existing_ids = result.get('existing_ids', [])
                cached_articles = result.get('articles', [])
                
                print(f"â±ï¸  [æœç´¢æ€§èƒ½] ä¸€ä½“åŒ–æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {unified_time:.2f}s | è¯¥åˆ†ç±»å·²æœ‰ {len(existing_ids)} æ¡")
                
                missing_ids = set(arxiv_ids) - set(existing_ids)
                
                # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç±»å…³è”çš„å®Œæ•´æ€§
                expected_linked_count = len(existing_ids)  # å·²å­˜åœ¨çš„è®ºæ–‡æ•°é‡
                actual_linked_count = len(cached_articles)  # å·²å»ºç«‹åˆ†ç±»å…³è”çš„è®ºæ–‡æ•°é‡
                
                if not missing_ids and expected_linked_count == actual_linked_count:
                    # æ‰€æœ‰æ•°æ®éƒ½å·²å­˜åœ¨ä¸”åˆ†ç±»å…³è”å®Œæ•´ï¼Œè·³è¿‡å¯¼å…¥
                    print(f"âš¡ [æœç´¢æ€§èƒ½] æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ä¸”åˆ†ç±»å…³è”å®Œæ•´({actual_linked_count}/{expected_linked_count})ï¼Œè·³è¿‡å¯¼å…¥")
                    import_time = 0
                    stats = {'processed': len(existing_ids), 'total_upsert': 0}
                    articles = cached_articles
                    db_time = 0.0
                    skip_db_read = True
                elif not missing_ids:
                    # è®ºæ–‡å·²å­˜åœ¨ä½†åˆ†ç±»å…³è”ä¸å®Œæ•´ï¼Œéœ€è¦è¡¥å»ºå…³è”
                    print(f"ğŸ”— [æœç´¢æ€§èƒ½] è®ºæ–‡å·²å­˜åœ¨ä½†åˆ†ç±»å…³è”ä¸å®Œæ•´({actual_linked_count}/{expected_linked_count})ï¼Œè¡¥å»ºå…³è”")
                    skip_db_read = False
                    try:
                        import_start = time.time()
                        stats = import_arxiv_papers_to_db(selected_date, selected_category, limit=None, skip_if_exists=True)
                        import_time = time.time() - import_start
                        print(f"â±ï¸  [æœç´¢æ€§èƒ½] è¡¥å»ºå…³è”å®Œæˆï¼Œè€—æ—¶: {import_time:.2f}s | processed={stats.get('processed', 0)} links={stats.get('total_link', 0)}")
                    except Exception as e:
                        import_time = time.time() - import_start if 'import_start' in locals() else 0
                        print(f"âŒ [æœç´¢æ€§èƒ½] è¡¥å»ºå…³è”å¤±è´¥ï¼Œè€—æ—¶: {import_time:.2f}s | é”™è¯¯: {e}")
                        skip_db_read = False
                elif missing_ids:
                    # å¯¼å…¥ç¼ºå¤±çš„æ•°æ®
                    print(f"ğŸ“¥ [æœç´¢æ€§èƒ½] å‘ç° {len(missing_ids)} æ¡æ–°æ•°æ®ï¼Œå¼€å§‹å¢é‡å¯¼å…¥")
                    skip_db_read = False
                    try:
                        import_start = time.time()
                        stats = import_arxiv_papers_to_db(selected_date, selected_category, limit=None, skip_if_exists=True)
                        import_time = time.time() - import_start
                        print(f"â±ï¸  [æœç´¢æ€§èƒ½] å¢é‡å¯¼å…¥å®Œæˆï¼Œè€—æ—¶: {import_time:.2f}s | processed={stats.get('processed', 0)} upserted={stats.get('total_upsert', 0)}")
                    except Exception as e:
                        import_time = time.time() - import_start if 'import_start' in locals() else 0
                        print(f"âŒ [æœç´¢æ€§èƒ½] å¯¼å…¥å¤±è´¥ï¼Œè€—æ—¶: {import_time:.2f}s | é”™è¯¯: {e}")
                        skip_db_read = False
                
                # è®¾ç½®å¯¼å…¥ç¼“å­˜ï¼ˆ30åˆ†é’Ÿï¼‰
                _cache_expiry[import_cache_key] = current_time + 1800

        # 2) ä»æ•°æ®åº“è¯»å–å¹¶è¿”å›ç»™å‰ç«¯ï¼ˆä¿æŒåŸåè®®å­—æ®µï¼‰
        # ğŸš€ ç¼“å­˜ç­–ç•¥ï¼šæ£€æŸ¥ç¼“å­˜
        cache_key = f"{selected_date}_{selected_category}"
        
        if cache_key in _search_cache and cache_key in _cache_expiry:
            if current_time < _cache_expiry[cache_key]:
                print(f"âš¡ [æœç´¢æ€§èƒ½] ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡DBæŸ¥è¯¢ | key={cache_key}")
                cached_data = _search_cache[cache_key]
                return jsonify({
                    'success': True,
                    'articles': cached_data['articles'],
                    'total': cached_data['total'],
                    'date': selected_date,
                    'category': selected_category,
                    'performance': {
                        'total_time': round(time.time() - total_start, 2),
                        'import_time': import_time,
                        'db_read_time': 0.0,  # ç¼“å­˜å‘½ä¸­
                        'cache_hit': True
                    },
                    'debug': cached_data.get('debug')
                })
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“ï¼ˆé™¤éå·²ç»æœ‰ä¸€ä½“åŒ–æŸ¥è¯¢çš„ç»“æœï¼‰
        try:
            if 'skip_db_read' in locals() and skip_db_read:
                # ä½¿ç”¨ä¸€ä½“åŒ–æŸ¥è¯¢çš„ç»“æœï¼Œæ— éœ€å†æ¬¡æŸ¥è¯¢
                print(f"âš¡ [æœç´¢æ€§èƒ½] ä½¿ç”¨ä¸€ä½“åŒ–æŸ¥è¯¢ç»“æœï¼Œè·³è¿‡é¢å¤–DBæŸ¥è¯¢")
                # articles å’Œ db_time å·²åœ¨ä¸Šé¢è®¾ç½®
            else:
                # æ­£å¸¸DBæŸ¥è¯¢
                db_start = time.time()
                print(f"ğŸ” [æœç´¢æ€§èƒ½] å¼€å§‹DBæŸ¥è¯¢ | key={cache_key}")
                articles = db_repo.list_papers_by_date_category(selected_date, selected_category)
                db_time = time.time() - db_start
            # é¢å¤–è°ƒè¯•æ—¥å¿—ï¼šå¯¹æ¯”å¯¼å…¥ç»Ÿè®¡ä¸DBè¿”å›æ•°é‡
            imported = stats.get('processed') if isinstance(locals().get('stats'), dict) else None
            upserted = stats.get('total_upsert') if isinstance(locals().get('stats'), dict) else None
            print(f"â±ï¸  [æœç´¢æ€§èƒ½] DBè¯»å–å®Œæˆï¼Œè€—æ—¶: {db_time:.2f}s | è·å– {len(articles)} æ¡è®°å½•")
            print(f"DBè¯»å– {len(articles)} æ¡ | date={selected_date} category={selected_category} | å¯¼å…¥processed={imported} upserted={upserted}")
            if imported is not None and len(articles) != imported:
                sample_ids = [a.get('id') for a in articles[:5]]
                print(f"æ•°é‡ä¸ä¸€è‡´: processed={imported} vs db={len(articles)}ã€‚å¯èƒ½åŸå› ï¼š1) PostgREST åˆ†é¡µå¯¼è‡´æˆªæ–­ï¼ˆå·²æ”¹ç”¨ range() å¼ºåˆ¶æ‰©å¤§ï¼‰ï¼›2) æ—¥æœŸçª—å£å·®å¼‚ï¼›3) åˆ†ç±»å…³è”ç¼ºå¤±ã€‚æ ·æœ¬arxiv_id={sample_ids}")
            # å¦‚æœæ•°é‡å¼‚å¸¸ï¼Œå‘å‰ç«¯æºå¸¦æ—¥å¿—ï¼Œä¾¿äºå¯è§†åŒ–
            debug_log = {
                'processed': imported,
                'db_count': len(articles),
                'category': selected_category,
                'date': selected_date
            }
        except Exception as e:
            return jsonify({'error': f'ä»æ•°æ®åº“è¯»å–å¤±è´¥: {e}'}), 500

        if len(articles) == 0:
            # æ„å»ºarXivæœç´¢URLä¾›ç”¨æˆ·ç›´æ¥æŸ¥çœ‹
            import datetime as dt
            import pytz
            target_date = dt.datetime.strptime(selected_date, "%Y-%m-%d").date()
            et_tz = pytz.timezone("US/Eastern")
            start_et = et_tz.localize(dt.datetime.combine(target_date - dt.timedelta(days=1), dt.time(20, 0)))
            end_et = et_tz.localize(dt.datetime.combine(target_date, dt.time(20, 0)))
            start_utc = start_et.astimezone(dt.timezone.utc)
            end_utc = end_et.astimezone(dt.timezone.utc)
            start_date_str = start_utc.strftime("%Y%m%d%H%M%S")
            end_date_str = end_utc.strftime("%Y%m%d%H%M%S")
            
            base = os.getenv("ARXIV_API_BASE", "https://export.arxiv.org/api/query")
            search_url = (
                f"{base}?"
                f"search_query=cat:{selected_category}+AND+submittedDate:[{start_date_str}+TO+{end_date_str}]&"
                "sortBy=submittedDate&sortOrder=descending&"
                "max_results=2000"
            )
            
            return jsonify({
                'error': f'å½“å¤©æ²¡æœ‰æ–°çš„{selected_category}è®ºæ–‡è¢«æäº¤åˆ°arXivï¼Œæˆ–æ•°æ®å°šæœªåŒæ­¥ã€‚è¯·ç¨åé‡è¯•ã€‚',
                'search_url': search_url
            }), 404

        # ğŸš€ æ›´æ–°ç¼“å­˜
        cache_data = {
            'articles': articles,
            'total': len(articles),
            'debug': debug_log if 'debug_log' in locals() else None
        }
        _search_cache[cache_key] = cache_data
        _cache_expiry[cache_key] = current_time + CACHE_TTL
        print(f"ğŸ“¦ [æœç´¢æ€§èƒ½] ç¼“å­˜å·²æ›´æ–° | key={cache_key} ttl={CACHE_TTL}s")

        total_time = time.time() - total_start
        print(f"ğŸ [æœç´¢æ€§èƒ½] æ€»è€—æ—¶: {total_time:.2f}s | å¯¼å…¥:{import_time:.2f}s + DBè¯»å–:{db_time:.2f}s")

        return jsonify({
            'success': True,
            'articles': articles,
            'total': len(articles),
            'date': selected_date,
            'category': selected_category,
            'performance': {
                'total_time': round(total_time, 2),
                'import_time': round(import_time, 2),
                'db_read_time': round(db_time, 2),
                'cache_hit': False
            },
            'debug': debug_log if 'debug_log' in locals() else None
        })

    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

# def parse_markdown_file(filepath, category_filter=''):
#     """âš ï¸ å·²åºŸå¼ƒï¼šè§£æmarkdownæ–‡ä»¶å¹¶æå–æ–‡ç« ä¿¡æ¯ï¼ˆå·²æ”¹ç”¨æ•°æ®åº“ï¼‰"""

# @app.route('/api/analyze_articles', methods=['POST'])
# def analyze_articles():
#     """âš ï¸ å·²åºŸå¼ƒï¼šåŸºäºMarkdownæ–‡ä»¶çš„æ—§åˆ†æè·¯ç”±ï¼ˆå·²æ”¹ç”¨ /api/analyze_papersï¼‰"""

# def analyze_articles_data(articles):
#     """âš ï¸ å·²åºŸå¼ƒï¼šç®€å•å…³é”®è¯ç»Ÿè®¡åˆ†æï¼ˆå·²æ”¹ç”¨AIæ¨¡å‹åˆ†æï¼‰"""

@app.route('/api/check_analysis_exists', methods=['POST'])
def check_analysis_exists():
    """æ£€æŸ¥æ•°æ®åº“ä¸­åˆ†æå®Œæˆçš„è¿›åº¦ï¼ˆæŒ‰å›ºå®š promptï¼‰ã€‚"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400

        # å›ºå®š promptï¼šä¼˜å…ˆæŒ‰åç§° multi-modal-llm æ‰¾åˆ° UUID
        prompt_id = db_repo.get_prompt_id_by_name('multi-modal-llm')
        if not prompt_id:
            return jsonify({'error': 'ç¼ºå°‘ prompt: multi-modal-llm'}), 500

        status = db_repo.get_analysis_status(selected_date, selected_category, prompt_id)
        resp = {
            'exists': status['completed'] > 0,
            'total': status['total'],
            'completed': status['completed'],
            'pending': status['pending'],
            'all_analyzed': status['completed'] >= status['total'] and status['total'] > 0
        }
        return jsonify(resp)
    except Exception as e:
        return jsonify({'error': f'æ£€æŸ¥è¿›åº¦å¤±è´¥: {str(e)}'}), 500

@app.route('/api/analyze_papers', methods=['POST'])
def analyze_papers():
    """å¯åŠ¨è®ºæ–‡åˆ†æï¼ˆä»…å¯¹æœªåˆ†æçš„ paperï¼Œè¡¥é½åˆ°æŒ‡å®šæ•°é‡ï¼‰ã€‚"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        range_type = data.get('range_type', 'full')

        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400

        # prompt: multi-modal-llm
        prompt_id = db_repo.get_prompt_id_by_name('multi-modal-llm')
        if not prompt_id:
            return jsonify({'error': 'ç¼ºå°‘ prompt: multi-modal-llm'}), 500

        # ä»»åŠ¡äº’æ–¥ï¼šå¦‚å·²å­˜åœ¨åŒæ—¥åŒç±»ä»»åŠ¡ï¼Œç›´æ¥è¿”å›å½“å‰è¿›åº¦
        task_id = f"{selected_date}-{selected_category}"
        with analysis_lock:
            if analysis_progress.get(task_id, {}).get('status') in ('starting','processing'):
                return jsonify({'success': True, 'task_id': task_id, 'message': 'å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œï¼Œè¿”å›å…¶è¿›åº¦'}), 200

        # è®¡ç®—ç›®æ ‡æ•°é‡ä¸è¡¥é½éœ€æ±‚
        target_map = {'top5': 5, 'top10': 10, 'top20': 20}
        target_n = target_map.get(range_type)

        status = db_repo.get_analysis_status(selected_date, selected_category, prompt_id)
        if target_n is not None:
            if status['completed'] >= target_n:
                return jsonify({'success': True, 'task_id': task_id, 'message': 'å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæ— éœ€å†æ¬¡åˆ†æ'}), 200
            need = target_n - status['completed']
        else:
            # fullï¼šå¯¹å…¨éƒ¨ pending
            need = status['pending']

        if need <= 0:
            return jsonify({'success': True, 'task_id': task_id, 'message': 'æ— éœ€åˆ†æ'}), 200

        # ä»…ç­›é€‰æœªåˆ†æçš„è®ºæ–‡ï¼Œéµå¾ªé¡ºåºä¸æœç´¢ä¸€è‡´
        pending = db_repo.list_unanalyzed_papers(selected_date, selected_category, prompt_id, limit=need)
        if not pending:
            return jsonify({'success': True, 'task_id': task_id, 'message': 'æ— å¾…åˆ†æè®ºæ–‡'}), 200

        # å¯åŠ¨åå°åˆ†æä»»åŠ¡ï¼ˆDBæºï¼‰ï¼Œä¸å†è¯»å– markdown
        thread = threading.Thread(
            target=run_db_analysis_task,
            args=(task_id, pending, selected_date, selected_category, prompt_id)
        )
        thread.daemon = True
        thread.start()

        return jsonify({'success': True, 'task_id': task_id, 'message': f'å¯åŠ¨åˆ†æï¼Œå…± {len(pending)} ç¯‡'})
    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/api/analyze_papers_concurrent', methods=['POST'])
def analyze_papers_concurrent():
    """å¯åŠ¨å¹¶å‘è®ºæ–‡åˆ†æï¼ˆ5è·¯å¹¶å‘ï¼‰"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        range_type = data.get('range_type', 'full')
        workers = data.get('workers', 5)  # å…è®¸å‰ç«¯æŒ‡å®šå¹¶å‘æ•°

        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400

        # prompt: multi-modal-llm
        prompt_id = db_repo.get_prompt_id_by_name('multi-modal-llm')
        if not prompt_id:
            return jsonify({'error': 'ç¼ºå°‘ prompt: multi-modal-llm'}), 500

        # ä»»åŠ¡äº’æ–¥ï¼šå¦‚å·²å­˜åœ¨åŒæ—¥åŒç±»ä»»åŠ¡ï¼Œç›´æ¥è¿”å›å½“å‰è¿›åº¦
        task_id = f"{selected_date}-{selected_category}-concurrent"
        with analysis_lock:
            if analysis_progress.get(task_id, {}).get('status') in ('starting','processing'):
                return jsonify({'success': True, 'task_id': task_id, 'message': 'å·²æœ‰å¹¶å‘ä»»åŠ¡åœ¨è¿è¡Œï¼Œè¿”å›å…¶è¿›åº¦'}), 200

        # è®¡ç®—ç›®æ ‡æ•°é‡ä¸è¡¥é½éœ€æ±‚
        target_map = {'top5': 5, 'top10': 10, 'top20': 20}
        target_n = target_map.get(range_type)

        status = db_repo.get_analysis_status(selected_date, selected_category, prompt_id)
        if target_n is not None:
            if status['completed'] >= target_n:
                return jsonify({'success': True, 'task_id': task_id, 'message': 'å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæ— éœ€å†æ¬¡åˆ†æ'}), 200
            need = target_n - status['completed']
        else:
            # fullï¼šå¯¹å…¨éƒ¨ pending
            need = status['pending']

        if need <= 0:
            return jsonify({'success': True, 'task_id': task_id, 'message': 'æ— éœ€åˆ†æ'}), 200

        # ä»…ç­›é€‰æœªåˆ†æçš„è®ºæ–‡ï¼Œéµå¾ªé¡ºåºä¸æœç´¢ä¸€è‡´
        pending = db_repo.list_unanalyzed_papers(selected_date, selected_category, prompt_id, limit=need)
        if not pending:
            return jsonify({'success': True, 'task_id': task_id, 'message': 'æ— å¾…åˆ†æè®ºæ–‡'}), 200

        # å¯åŠ¨åå°å¹¶å‘åˆ†æä»»åŠ¡
        thread = threading.Thread(
            target=run_concurrent_analysis_task,
            args=(task_id, pending, selected_date, selected_category, prompt_id, workers)
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True, 
            'task_id': task_id, 
            'message': f'å¹¶å‘åˆ†æä»»åŠ¡å·²å¯åŠ¨ ({workers}è·¯å¹¶å‘)',
            'workers': workers,
            'total_papers': len(pending)
        }), 200
    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨å¹¶å‘åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/api/performance_comparison', methods=['POST'])
def performance_comparison():
    """è¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆ1è·¯ vs 5è·¯å¹¶å‘ï¼‰"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        test_count = data.get('test_count', 10)  # é»˜è®¤æµ‹è¯•10ç¯‡

        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400

        # prompt: multi-modal-llm
        prompt_id = db_repo.get_prompt_id_by_name('multi-modal-llm')
        if not prompt_id:
            return jsonify({'error': 'ç¼ºå°‘ prompt: multi-modal-llm'}), 500

        # è·å–å¾…åˆ†æè®ºæ–‡
        pending = db_repo.list_unanalyzed_papers(selected_date, selected_category, prompt_id, limit=test_count)
        if len(pending) < test_count:
            return jsonify({'error': f'å¯ç”¨è®ºæ–‡æ•°é‡ä¸è¶³ï¼Œéœ€è¦{test_count}ç¯‡ï¼Œå®é™…{len(pending)}ç¯‡'}), 400

        # è¯»å–system prompt
        try:
            system_prompt = db_repo.get_system_prompt()
        except Exception as e:
            return jsonify({'error': f'è·å–ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {str(e)}'}), 500

        # è¿è¡Œæ€§èƒ½å¯¹æ¯”
        task_id = f"{selected_date}-{selected_category}-comparison"
        comparison_result = run_performance_comparison(
            task_id, pending, prompt_id, system_prompt, analysis_progress, test_count
        )

        return jsonify({
            'success': True,
            'comparison_result': comparison_result,
            'message': f'æ€§èƒ½å¯¹æ¯”å®Œæˆï¼Œæµ‹è¯•äº†{test_count}ç¯‡è®ºæ–‡'
        }), 200
        
    except Exception as e:
        return jsonify({'error': f'æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {str(e)}'}), 500

# def auto_commit_analysis_file(output_file, task_id):
#     """
#     âš ï¸  å·²åºŸå¼ƒï¼šè‡ªåŠ¨æäº¤åˆ†æç»“æœæ–‡ä»¶åˆ° GitHub
#     ç°åœ¨åˆ†æç»“æœç›´æ¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œä¸å†ç”ŸæˆMarkdownæ–‡ä»¶
#     """

# def run_analysis_task(task_id, input_file, selected_date, selected_category, test_count):
#     """åå°è¿è¡Œåˆ†æä»»åŠ¡
#     âš ï¸  å·²åºŸå¼ƒï¼šåŠŸèƒ½ï¼šåŸºäºMarkdownæ–‡ä»¶çš„æ—§åˆ†æä»»åŠ¡
#     """

def run_db_analysis_task(task_id, pending_papers, selected_date, selected_category, prompt_id):
    """åŸºäºæ•°æ®åº“å¾…åˆ†æé›†åˆçš„åå°ä»»åŠ¡ã€‚ä»…å¯¹æœªåˆ†æè®ºæ–‡è°ƒç”¨æ¨¡å‹å¹¶å†™å…¥DBã€‚"""
    import sys
    try:
        with analysis_lock:
            analysis_progress[task_id] = {
                'current': 0,
                'total': len(pending_papers),
                'status': 'processing',
                'paper': None,
                'analysis_result': None
            }

        # è¯»å–system prompt
        system_prompt = db_repo.get_system_prompt()

        # åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯
        print(f"ğŸ“¡ åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯... ğŸ” Task ID: {task_id}")
        client = DoubaoClient()
        print(f"âœ… è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - DB æºåˆ†æ")

        success_count = 0
        error_count = 0

        for i, m in enumerate(pending_papers):
            paper = {
                'paper_id': m['paper_id'],
                'title': m.get('title',''),
                'abstract': m.get('abstract',''),
                'authors': m.get('authors',''),
                'link': m.get('link',''),
                'author_affiliation': m.get('author_affiliation','')
            }
            try:
                with analysis_lock:
                    analysis_progress[task_id]['current'] = i + 1
                    analysis_progress[task_id]['paper'] = paper
                    analysis_progress[task_id]['analysis_result'] = None

                start_time = time.time()
                result = analyze_paper(client, system_prompt, paper['title'], paper['abstract'])

                # åºåˆ—åŒ–ç»“æœå¹¶å†™åº“ï¼ˆå¹‚ç­‰ï¼šå”¯ä¸€é”®ä¿è¯ï¼‰
                try:
                    import json as _json
                    ar = _json.loads(result)
                except Exception:
                    ar = { 'raw': result }

                db_repo.insert_analysis_result(
                    paper_id=paper['paper_id'],
                    prompt_id=prompt_id,
                    analysis_json=ar,
                    created_by=None,
                )

                # è‹¥é€šè¿‡ç­›é€‰ä¸” paper ä¸­æœºæ„ä¿¡æ¯ä¸ºç©ºï¼Œè‡ªåŠ¨è¡¥å……ä½œè€…æœºæ„å¹¶å†™å› papers.author_affiliation
                try:
                    pass_filter = bool(ar.get('pass_filter'))
                    has_aff = bool(paper.get('author_affiliation'))
                    if pass_filter and not has_aff:
                        # æ›´æ–°è¿›åº¦çŠ¶æ€ï¼Œå‘ŠçŸ¥å‰ç«¯æ­£åœ¨è·å–æœºæ„ä¿¡æ¯
                        with analysis_lock:
                            analysis_progress[task_id]['status'] = 'fetching_affiliations'
                            analysis_progress[task_id]['paper'] = {
                                **paper,
                                'status': 'æ­£åœ¨è·å–ä½œè€…æœºæ„...'
                            }
                        
                        # get_author_affiliations å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥
                        
                        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
                        def update_progress(message):
                            with analysis_lock:
                                analysis_progress[task_id]['paper'] = {
                                    **paper,
                                    'status': message
                                }
                        
                        affiliations = get_author_affiliations(paper['link'], progress_callback=update_progress)
                        if affiliations:
                            import json as _json
                            aff_json = _json.dumps(affiliations, ensure_ascii=False)
                            db_repo.update_paper_author_affiliation(paper['paper_id'], aff_json)
                        
                        # æ¢å¤å¤„ç†çŠ¶æ€
                        with analysis_lock:
                            analysis_progress[task_id]['status'] = 'processing'
                            analysis_progress[task_id]['paper'] = paper
                except Exception as _aff_e:
                    print(f"è·å–/å†™å…¥ä½œè€…æœºæ„å¤±è´¥: {_aff_e}")
                    # ç¡®ä¿çŠ¶æ€æ¢å¤
                    with analysis_lock:
                        analysis_progress[task_id]['status'] = 'processing'
                        analysis_progress[task_id]['paper'] = paper

                success_count += 1
                elapsed = time.time() - start_time
                print(f"âœ… DBåˆ†æ {i+1}/{len(pending_papers)} å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")

                with analysis_lock:
                    analysis_progress[task_id]['analysis_result'] = result
                    analysis_progress[task_id]['success_count'] = success_count
                    analysis_progress[task_id]['error_count'] = error_count
            except Exception as e:
                error_count += 1
                print(f"âŒ DBåˆ†æå¼‚å¸¸: {e}")
                with analysis_lock:
                    analysis_progress[task_id]['error_count'] = error_count
                continue

        with analysis_lock:
            analysis_progress[task_id]['status'] = 'completed'
            analysis_progress[task_id]['final_success_count'] = success_count
            analysis_progress[task_id]['final_error_count'] = error_count
    except Exception as e:
        print(f"âŒ run_db_analysis_task å¤±è´¥: {e}")
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'error'
            analysis_progress[task_id]['error'] = str(e)

def run_concurrent_analysis_task(task_id, pending_papers, selected_date, selected_category, prompt_id, workers=5):
    """è¿è¡Œå¹¶å‘åˆ†æä»»åŠ¡"""
    import sys
    try:
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        with analysis_lock:
            analysis_progress[task_id] = {
                'current': 0,
                'total': len(pending_papers),
                'status': 'starting',
                'paper': None,
                'analysis_result': None,
                'workers': workers,
                'start_time': time.time()
            }

        # è¯»å–system prompt
        system_prompt = db_repo.get_system_prompt()

        print(f"ğŸš€ [å¹¶å‘åˆ†æ] å¯åŠ¨ä»»åŠ¡ {task_id}ï¼Œ{workers}è·¯å¹¶å‘ï¼Œæ€»è®¡ {len(pending_papers)} ç¯‡è®ºæ–‡")

        # è·å–å¹¶å‘åˆ†ææœåŠ¡
        concurrent_service = get_concurrent_service(workers=workers)
        
        # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
        def update_progress_callback(task_id, completed, total):
            # è¿™ä¸ªå›è°ƒåœ¨å¹¶å‘æœåŠ¡ä¸­å·²ç»æ›´æ–°äº†è¿›åº¦ï¼Œè¿™é‡Œå¯ä»¥åšé¢å¤–çš„æ—¥å¿—
            pass

        # æ‰§è¡Œå¹¶å‘åˆ†æ
        result_stats = concurrent_service.analyze_papers_concurrent(
            task_id, pending_papers, prompt_id, system_prompt, 
            analysis_progress, update_progress_callback
        )

        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        with analysis_lock:
            analysis_progress[task_id].update({
                'status': 'completed',
                'final_stats': result_stats,
                'end_time': time.time()
            })

        print(f"ğŸ‰ [å¹¶å‘åˆ†æ] ä»»åŠ¡ {task_id} å®Œæˆï¼"
              f"æˆåŠŸ:{result_stats['success_count']}, å¤±è´¥:{result_stats['error_count']}, "
              f"æ€»è€—æ—¶:{result_stats['total_elapsed_time']:.1f}s")

    except Exception as e:
        print(f"âŒ [å¹¶å‘åˆ†æ] ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
        with analysis_lock:
            analysis_progress[task_id]['status'] = 'error'
            analysis_progress[task_id]['error'] = str(e)
@app.route('/api/analysis_progress')
def analysis_progress_stream():
    """Server-Sent Eventsæµï¼Œç”¨äºå®æ—¶è·å–åˆ†æè¿›åº¦"""
    # åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
    date = request.args.get('date')
    category = request.args.get('category', 'cs.CV')
    task_type = request.args.get('type', 'serial')  # serial or concurrent
    if task_type == 'concurrent':
        task_id = f"{date}-{category}-concurrent"
    else:
        task_id = f"{date}-{category}"
    
    def generate(task_id):
        import sys
        last_current = -1
        last_status = None
        loop_count = 0
        max_loops = 1800  # æœ€å¤šå¾ªç¯30åˆ†é’Ÿï¼ˆ1800ç§’ï¼‰
        
        # ç«‹å³å‘é€åˆå§‹çŠ¶æ€
        yield f"data: {json.dumps({'status': 'connecting', 'current': 0, 'total': 0}, ensure_ascii=False)}\n\n"
        
        while loop_count < max_loops:
            loop_count += 1
            
            with analysis_lock:
                progress = analysis_progress.get(task_id, {})
            
            status = progress.get('status', 'unknown')
            current = progress.get('current', 0)
            
            # å‡å°‘è°ƒè¯•ä¿¡æ¯çš„é¢‘ç‡ï¼Œåªåœ¨çŠ¶æ€æˆ–è¿›åº¦å˜åŒ–æ—¶è®°å½•åˆ°stderr
            if status != last_status or current != last_current:
                print(f"SSE Debug - task_id: {task_id}, status: {status}, current: {current}, loop: {loop_count}", file=sys.stderr)
            
            # åªåœ¨è¿›åº¦æœ‰å˜åŒ–æ—¶å‘é€æ•°æ®ï¼Œæˆ–è€…æ˜¯ç‰¹æ®ŠçŠ¶æ€
            should_send = (
                current != last_current or 
                status != last_status or
                status in ['completed', 'error', 'starting']
            )
            
            if should_send:
                data = {
                    'current': current,
                    'total': progress.get('total', 0),
                    'status': status,
                    'paper': progress.get('paper'),
                    'analysis_result': progress.get('analysis_result'),
                    'workers': progress.get('workers', 1),
                    'success_count': progress.get('success_count', 0),
                    'error_count': progress.get('error_count', 0),
                    'processing_papers': progress.get('processing_papers', []),
                    'last_completed_paper': progress.get('last_completed_paper')
                }
                
                print(f"SSE Sending data - current: {current}, status: {status}, has_result: {bool(data['analysis_result'])}", file=sys.stderr)
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                last_current = current
                last_status = status
            
            if status == 'completed':
                # å‘é€å®Œæˆäº‹ä»¶
                completion_data = {
                    'summary': f'åˆ†æå®Œæˆï¼å…±å¤„ç† {progress.get("total", 0)} ç¯‡è®ºæ–‡',
                    'completed_range_type': progress.get('completed_range_type', 'full')
                }
                yield f"event: complete\ndata: {json.dumps(completion_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream completed for task_id: {task_id}", file=sys.stderr)
                break
            elif status == 'error':
                error_data = {
                    'error': progress.get('error', 'æœªçŸ¥é”™è¯¯')
                }
                yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                print(f"SSE stream error for task_id: {task_id}", file=sys.stderr)
                break
            
            time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # å¦‚æœå¾ªç¯è¶…æ—¶ï¼Œå‘é€è¶…æ—¶é”™è¯¯
        if loop_count >= max_loops:
            timeout_data = {'error': 'SSE stream timeout'}
            yield f"event: error\ndata: {json.dumps(timeout_data, ensure_ascii=False)}\n\n"
            print(f"SSE stream timeout for task_id: {task_id}", file=sys.stderr)
    
    return Response(generate(task_id), mimetype='text/event-stream')

@app.route('/api/fetch_affiliations', methods=['POST'])
def fetch_affiliations_api():
    try:
        data = request.get_json()
        paper_id = data.get('paper_id')
        link = data.get('link')
        if not paper_id or not link:
            return jsonify({'error': 'ç¼ºå°‘paper_idæˆ–link'}), 400
        
        print(f"[API] å¼€å§‹è·å–ä½œè€…æœºæ„: paper_id={paper_id}, link={link}")
        
        # get_author_affiliations å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥
        aff = get_author_affiliations(link, use_cache=False)  # å¼ºåˆ¶ä¸ä½¿ç”¨ç¼“å­˜
        
        if aff and len(aff) > 0:
            import json as _json
            aff_json = _json.dumps(aff, ensure_ascii=False)
            db_repo.update_paper_author_affiliation(int(paper_id), aff_json)
            print(f"[API] âœ… æˆåŠŸè·å–å¹¶å†™å…¥æ•°æ®åº“: {len(aff)} ä¸ªæœºæ„")
            return jsonify({'success': True, 'affiliations': aff})
        else:
            print(f"[API] âš ï¸ æœªè·å–åˆ°æœºæ„ä¿¡æ¯")
            return jsonify({'success': False, 'error': 'æœªè·å–åˆ°æœºæ„ä¿¡æ¯', 'affiliations': []}), 200
    except Exception as e:
        error_msg = str(e)
        print(f"[API] âŒ è·å–æœºæ„å¤±è´¥: {error_msg}")
        
        # å¯¹äºæ–‡ä»¶è¿‡å¤§ç­‰é”™è¯¯ï¼Œè¿”å›å…·ä½“é”™è¯¯ä¿¡æ¯
        if "è¿‡å¤§" in error_msg or "é™åˆ¶" in error_msg:
            return jsonify({'success': False, 'error': f'æ–‡ä»¶è¿‡å¤§ï¼Œæ— æ³•å¤„ç†: {error_msg}', 'affiliations': []}), 200
        else:
            return jsonify({'error': error_msg}), 500

@app.route('/api/get_analysis_results', methods=['POST'])
def get_analysis_results():
    """è·å–åˆ†æç»“æœ"""
    try:
        data = request.get_json()
        selected_date = data.get('date')
        selected_category = data.get('category', 'cs.CV')
        selected_range = data.get('range_type', 'full') # é»˜è®¤å…¨éƒ¨åˆ†æ
        
        if not selected_date:
            return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'}), 400
        
        # ç›´æ¥ä»DBè¿”å›åˆ†æç»“æœï¼ˆä¼˜å…ˆä¸”é»˜è®¤ï¼‰
        try:
            prompt_id = db_repo.get_prompt_id_by_name("multi-modal-llm") or db_repo.get_prompt_id_by_name("system_default")
            if not prompt_id:
                return jsonify({'error': 'ç¼ºå°‘ prompt: multi-modal-llm'}), 500
            limit = 5 if selected_range == 'top5' else 10 if selected_range == 'top10' else 20 if selected_range == 'top20' else None
            time_filter = data.get('time_filter')  # æ”¯æŒæ—¶é—´ç­›é€‰å‚æ•°
            articles = db_repo.get_analysis_results(date=selected_date, category=selected_category, prompt_id=prompt_id, limit=limit, time_filter=time_filter)
            if len(articles) > 0:
                return jsonify({
                    'success': True,
                    'articles': articles,
                    'total': len(articles),
                    'date': selected_date,
                    'category': selected_category,
                    'range_type': selected_range
                })
        except Exception as e:
            print(f"ä»DBè¯»å–åˆ†æç»“æœå¤±è´¥: {e}")

        return jsonify({'success': True, 'articles': [], 'total': 0, 'date': selected_date, 'category': selected_category, 'range_type': selected_range})
        
    except Exception as e:
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

# def parse_analysis_fail_file(filepath):
#     """âš ï¸ å·²åºŸå¼ƒï¼šè§£æåˆ†æå¤±è´¥markdownæ–‡ä»¶ï¼ˆå·²æ”¹ç”¨æ•°æ®åº“ï¼‰"""

# def parse_analysis_markdown_file(filepath):
#     """âš ï¸ å·²åºŸå¼ƒï¼šè§£æåˆ†æç»“æœmarkdownæ–‡ä»¶ï¼ˆå·²æ”¹ç”¨æ•°æ®åº“ï¼‰"""

@app.route('/api/available_dates', methods=['GET'])
def get_available_dates():
    """è·å–å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    try:
        dates = db_repo.list_available_dates()
        return jsonify({'dates': dates})
        
    except Exception as e:
        return jsonify({'error': f'è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/clear_cache', methods=['POST'])
def clear_cache():
    """æ¸…ç†æœåŠ¡å™¨ç«¯ç¼“å­˜ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _search_cache, _cache_expiry
    
    data = request.get_json() or {}
    cache_type = data.get('type', 'all')
    
    if cache_type in ['all', 'search']:
        _search_cache.clear()
        search_keys = [k for k in _cache_expiry.keys() if not k.startswith('import_')]
        for key in search_keys:
            _cache_expiry.pop(key, None)
        print("ğŸ—‘ï¸  å·²æ¸…ç†æœç´¢ç¼“å­˜")
    
    if cache_type in ['all', 'import']:
        import_keys = [k for k in _cache_expiry.keys() if k.startswith('import_')]
        for key in import_keys:
            _cache_expiry.pop(key, None)
        print("ğŸ—‘ï¸  å·²æ¸…ç†å¯¼å…¥ç¼“å­˜")
    
    try:
        clear_affiliation_cache()
        print("ğŸ—‘ï¸  å·²æ¸…ç†æœºæ„ä¿¡æ¯ç¼“å­˜")
    except:
        pass
    
    return jsonify({
        'success': True, 
        'message': f'å·²æ¸…ç†{cache_type}ç¼“å­˜',
        'remaining_cache_keys': list(_cache_expiry.keys())
    })





if __name__ == '__main__':
    import sys
    
    # å¼ºåˆ¶åˆ·æ–°æ ‡å‡†è¾“å‡ºï¼Œç¡®ä¿åœ¨Renderä¸­èƒ½çœ‹åˆ°æ—¥å¿—
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)
    
    # æ­£ç¡®å¤„ç†Renderçš„PORTç¯å¢ƒå˜é‡
    port = int(os.getenv('PORT', 8080))  # Renderæ³¨å…¥PORTç¯å¢ƒå˜é‡ï¼Œæœ¬åœ°é»˜è®¤8080
    host = '0.0.0.0'  # å¿…é¡»ç»‘å®šåˆ°æ‰€æœ‰æ¥å£ï¼Œä¸èƒ½ç”¨localhost
    
    # æ¸…ç©ºæœºæ„ä¿¡æ¯ç¼“å­˜
    try:
        clear_affiliation_cache()
    except:
        pass
    
    print("å¯åŠ¨Arxivæ–‡ç« åˆç­›å°åŠ©æ‰‹æœåŠ¡å™¨...")
    print(f"ç¯å¢ƒPORTå˜é‡: {os.getenv('PORT', 'None (ä½¿ç”¨é»˜è®¤8080)')}")
    print(f"å®é™…ä½¿ç”¨ç«¯å£: {port}")
    print(f"ç»‘å®šåœ°å€: {host}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¦ç”¨debugæ¨¡å¼ï¼Œä½†ä¿æŒæ—¥å¿—è¾“å‡º
    is_production = os.getenv('RENDER') is not None
    if is_production:
        print("ğŸŒ æ£€æµ‹åˆ°Renderç”Ÿäº§ç¯å¢ƒï¼Œä¼˜åŒ–æ—¥å¿—é…ç½®")
        print(f"è®¿é—®åœ°å€: https://ä½ çš„renderåŸŸå")
        app.run(debug=False, host=host, port=port)
    else:
        print("ğŸ–¥ï¸  æœ¬åœ°å¼€å‘ç¯å¢ƒ")
        print(f"è®¿é—®åœ°å€: http://localhost:{port}")
        app.run(debug=True, host=host, port=port) 