import feedparser
import time
import re
import sys

def extract_arxiv_id_from_url(url: str) -> str:
    """
    ä»arxiv abstract URLä¸­æå–arxiv_id
    ä¾‹å¦‚: http://arxiv.org/abs/2507.23785v1 -> 2507.23785v1
    """
    # æ¸…ç†URLï¼Œç§»é™¤å¯èƒ½çš„ç©ºæ ¼
    url = url.strip()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–arxiv_id
    pattern = r'/abs/([^/\s]+)/?$'
    match = re.search(pattern, url)
    
    if match:
        return match.group(1)
    else:
        raise ValueError(f"æ— æ³•ä»URLä¸­æå–arxiv_id: {url}")

def arxiv_affil_via_api(url_or_id: str) -> list:
    """
    ä»arxiv URLæˆ–IDè·å–ä½œè€…æœºæ„ä¿¡æ¯
    """
    # å¦‚æœè¾“å…¥æ˜¯URLï¼Œå…ˆæå–arxiv_id
    if url_or_id.startswith('http'):
        arxiv_id = extract_arxiv_id_from_url(url_or_id)
    else:
        arxiv_id = url_or_id
    
    # ä½¿ç”¨ArXiv APIæŸ¥è¯¢
    api_url = f"https://export.arxiv.org/api/query?id_list={arxiv_id}"
    feed = feedparser.parse(api_url)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    if not feed.entries:
        return []
    
    # æå–ä½œè€…å’Œæœºæ„ä¿¡æ¯
    out = []
    entry = feed.entries[0]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä½œè€…ä¿¡æ¯
    if hasattr(entry, 'authors') and entry.authors:
        for author in entry.authors:
            author_name = author.get('name', 'Unknown Author')
            affiliation = author.get('affiliation', None)
            
            if affiliation:
                out.append((author_name, affiliation))
            else:
                out.append((author_name, 'No affiliation info'))
    
    return out

def extract_paper_links_from_file(file_path: str) -> list:
    """
    ä»markdownæ–‡ä»¶ä¸­æå–æ‰€æœ‰ArXivè®ºæ–‡é“¾æ¥
    """
    links = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ArXivé“¾æ¥
            pattern = r'http://arxiv\.org/abs/[^\s|)]+|https://arxiv\.org/abs/[^\s|)]+'
            found_links = re.findall(pattern, content)
            # å»é‡å¹¶æ¸…ç†é“¾æ¥
            for link in found_links:
                clean_link = link.strip('|).;:')
                if clean_link not in links:
                    links.append(clean_link)
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return links

def test_batch_papers(links: list) -> dict:
    """
    æ‰¹é‡æµ‹è¯•è®ºæ–‡é“¾æ¥ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ
    """
    results = {
        'total': len(links),
        'success_with_affiliation': 0,  # æˆåŠŸè·å–ä½œè€…ä¸”æœ‰æœºæ„ä¿¡æ¯
        'success_no_affiliation': 0,    # æˆåŠŸè·å–ä½œè€…ä½†æ— æœºæ„ä¿¡æ¯
        'failed': 0,                    # å®Œå…¨å¤±è´¥
        'details': []
    }
    
    print(f"å¼€å§‹æµ‹è¯• {len(links)} ç¯‡è®ºæ–‡...")
    print("=" * 80)
    
    for i, link in enumerate(links, 1):
        print(f"\n[{i}/{len(links)}] æµ‹è¯•: {link}")
        
        try:
            # è·å–ArXiv ID
            arxiv_id = extract_arxiv_id_from_url(link)
            
            # è·å–æœºæ„ä¿¡æ¯
            affiliations = arxiv_affil_via_api(link)
            
            if affiliations:
                # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„æœºæ„ä¿¡æ¯
                has_real_affiliation = any(affil != 'No affiliation info' for _, affil in affiliations)
                
                if has_real_affiliation:
                    results['success_with_affiliation'] += 1
                    print(f"âœ… æˆåŠŸ(æœ‰æœºæ„) - è®ºæ–‡ {arxiv_id}:")
                    for j, (author, affil) in enumerate(affiliations, 1):
                        print(f"  {j}. {author} - {affil}")
                    
                    results['details'].append({
                        'arxiv_id': arxiv_id,
                        'link': link,
                        'status': 'success_with_affiliation',
                        'affiliations': affiliations
                    })
                else:
                    results['success_no_affiliation'] += 1
                    print(f"ğŸ”¶ æˆåŠŸ(æ— æœºæ„) - è®ºæ–‡ {arxiv_id}:")
                    for j, (author, affil) in enumerate(affiliations, 1):
                        print(f"  {j}. {author} - (æ— æœºæ„ä¿¡æ¯)")
                    
                    results['details'].append({
                        'arxiv_id': arxiv_id,
                        'link': link,
                        'status': 'success_no_affiliation',
                        'affiliations': affiliations
                    })
            else:
                results['failed'] += 1
                print(f"âŒ å¤±è´¥ - è®ºæ–‡ {arxiv_id}: æœªæ‰¾åˆ°ä»»ä½•ä½œè€…ä¿¡æ¯")
                
                results['details'].append({
                    'arxiv_id': arxiv_id,
                    'link': link,
                    'status': 'no_authors',
                    'affiliations': []
                })
                
        except Exception as e:
            results['failed'] += 1
            print(f"âŒ é”™è¯¯ - {link}: {e}")
            
            results['details'].append({
                'arxiv_id': 'unknown',
                'link': link,
                'status': 'error',
                'error': str(e),
                'affiliations': []
            })
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
        time.sleep(0.5)
    
    return results

def print_summary(results: dict):
    """
    æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š ArXiv API æœºæ„ä¿¡æ¯è·å–æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"æ€»è®ºæ–‡æ•°: {results['total']}")
    print(f"âœ… æˆåŠŸè·å–ä½œè€…ä¸”æœ‰æœºæ„ä¿¡æ¯: {results['success_with_affiliation']}")
    print(f"ğŸ”¶ æˆåŠŸè·å–ä½œè€…ä½†æ— æœºæ„ä¿¡æ¯: {results['success_no_affiliation']}")
    print(f"âŒ å®Œå…¨å¤±è´¥(æ— ä½œè€…ä¿¡æ¯): {results['failed']}")
    
    if results['total'] > 0:
        success_total = results['success_with_affiliation'] + results['success_no_affiliation']
        api_success_rate = (success_total / results['total']) * 100
        affiliation_success_rate = (results['success_with_affiliation'] / results['total']) * 100
        
        print(f"\nğŸ“ˆ æˆåŠŸç‡ç»Ÿè®¡:")
        print(f"  - APIè°ƒç”¨æˆåŠŸç‡(èƒ½è·å–ä½œè€…): {api_success_rate:.2f}%")
        print(f"  - æœºæ„ä¿¡æ¯è·å–æˆåŠŸç‡: {affiliation_success_rate:.2f}%")
    
    # æŒ‰çŠ¶æ€åˆ†ç±»ç»Ÿè®¡
    status_count = {}
    for detail in results['details']:
        status = detail['status']
        status_count[status] = status_count.get(status, 0) + 1
    
    print(f"\nğŸ“‹ è¯¦ç»†çŠ¶æ€ç»Ÿè®¡:")
    status_descriptions = {
        'success_with_affiliation': 'æˆåŠŸè·å–ä½œè€…å’Œæœºæ„ä¿¡æ¯',
        'success_no_affiliation': 'æˆåŠŸè·å–ä½œè€…ä½†æ— æœºæ„ä¿¡æ¯',
        'no_authors': 'æ— æ³•è·å–ä½œè€…ä¿¡æ¯',
        'error': 'APIè°ƒç”¨é”™è¯¯'
    }
    
    for status, count in status_count.items():
        desc = status_descriptions.get(status, status)
        print(f"  - {desc}: {count}")
    
    print(f"\nğŸ’¡ ç»“è®º:")
    if results['success_with_affiliation'] == 0:
        print("  - ArXiv APIé€šå¸¸ä¸æä¾›æœºæ„ä¿¡æ¯ï¼Œä½†èƒ½æˆåŠŸè·å–ä½œè€…å§“å")
        print("  - å»ºè®®ä½¿ç”¨PDFè§£ææ–¹æ³•æ¥è·å–æœºæ„ä¿¡æ¯")
    else:
        print(f"  - æœ‰ {results['success_with_affiliation']} ç¯‡è®ºæ–‡æä¾›äº†æœºæ„ä¿¡æ¯")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("1. æµ‹è¯•å•ä¸ªURL: python parse_author_org_v2.py <arxiv_url>")
        print("2. æ‰¹é‡æµ‹è¯•æ–‡ä»¶ä¸­çš„é“¾æ¥: python parse_author_org_v2.py <file_path>")
        print("3. ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡ä»¶: python parse_author_org_v2.py default")
        sys.exit(1)
    
    arg = sys.argv[1]
    
    if arg == "default":
        # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡ä»¶
        file_path = "log/2025-07-31-cs.CV-result.md"
        print(f"ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡ä»¶: {file_path}")
        
        # æå–é“¾æ¥
        links = extract_paper_links_from_file(file_path)
        if not links:
            print("æœªæ‰¾åˆ°ä»»ä½•ArXivé“¾æ¥")
            sys.exit(1)
        
        print(f"ä»æ–‡ä»¶ä¸­æå–åˆ° {len(links)} ä¸ªè®ºæ–‡é“¾æ¥")
        
        # æ‰¹é‡æµ‹è¯•
        results = test_batch_papers(links)
        
        # æ‰“å°æ±‡æ€»
        print_summary(results)
        
    elif arg.startswith('http'):
        # æµ‹è¯•å•ä¸ªURL
        print(f"æµ‹è¯•å•ä¸ªURL: {arg}")
        try:
            affiliations = arxiv_affil_via_api(arg)
            arxiv_id = extract_arxiv_id_from_url(arg)
            
            print(f"\nè®ºæ–‡ {arxiv_id} çš„ä½œè€…æœºæ„ä¿¡æ¯:")
            if affiliations:
                for i, (author, affil) in enumerate(affiliations, 1):
                    print(f"  {i}. {author} - {affil}")
            else:
                print("  æœªæ‰¾åˆ°æœºæ„ä¿¡æ¯")
        except Exception as e:
            print(f"å¤„ç†URLæ—¶å‡ºé”™: {e}")
            
    else:
        # æµ‹è¯•æŒ‡å®šæ–‡ä»¶ä¸­çš„é“¾æ¥
        file_path = arg
        print(f"ä»æ–‡ä»¶æå–é“¾æ¥å¹¶æ‰¹é‡æµ‹è¯•: {file_path}")
        
        # æå–é“¾æ¥
        links = extract_paper_links_from_file(file_path)
        if not links:
            print("æœªæ‰¾åˆ°ä»»ä½•ArXivé“¾æ¥")
            sys.exit(1)
        
        print(f"ä»æ–‡ä»¶ä¸­æå–åˆ° {len(links)} ä¸ªè®ºæ–‡é“¾æ¥")
        
        # æ‰¹é‡æµ‹è¯•
        results = test_batch_papers(links)
        
        # æ‰“å°æ±‡æ€»
        print_summary(results)
