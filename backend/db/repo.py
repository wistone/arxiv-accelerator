from __future__ import annotations

import datetime as dt
from typing import Any, Dict, List, Optional, Tuple
import json
import time

from .client import app_schema, get_client


def _ensure_date(value: str | dt.date) -> str:
    if isinstance(value, dt.date):
        return value.isoformat()
    return value


def get_or_create_prompt(prompt_name: str) -> str:
    db = app_schema()
    # Try get
    res = db.from_("prompts").select("prompt_id").eq("prompt_name", prompt_name).limit(1).execute()
    if res.data:
        return res.data[0]["prompt_id"]
    # create empty placeholder; caller can update content later
    db.from_("prompts").insert({"prompt_name": prompt_name, "prompt_content": ""}).execute()
    res = db.from_("prompts").select("prompt_id").eq("prompt_name", prompt_name).limit(1).execute()
    return res.data[0]["prompt_id"]


def upsert_category(category_name: str) -> int:
    db = app_schema()
    # try select
    res = db.from_("categories").select("category_id").eq("category_name", category_name).limit(1).execute()
    if res.data:
        return res.data[0]["category_id"]
    # insert
    db.from_("categories").insert({"category_name": category_name}).execute()
    res = db.from_("categories").select("category_id").eq("category_name", category_name).limit(1).execute()
    return res.data[0]["category_id"]


def upsert_paper(
    *,
    arxiv_id: str,
    title: str,
    authors: str,
    abstract: str,
    link: str,
    update_date: str | dt.date,
    primary_category: Optional[str] = None,
    author_affiliation: Optional[str] = None,
) -> int:
    db = app_schema()
    # check exists
    res = db.from_("papers").select("paper_id").eq("arxiv_id", arxiv_id).limit(1).execute()
    if res.data:
        paper_id = res.data[0]["paper_id"]
        # update lightweight fields (idempotent)
        db.from_("papers").update({
            "title": title,
            "authors": authors,
            "abstract": abstract,
            "link": link,
            "update_date": _ensure_date(update_date),
            "primary_category": primary_category,
            "author_affiliation": author_affiliation,
        }).eq("paper_id", paper_id).execute()
        return paper_id
    # insert
    db.from_("papers").insert({
        "arxiv_id": arxiv_id,
        "title": title,
        "authors": authors,
        "abstract": abstract,
        "link": link,
        "update_date": _ensure_date(update_date),
        "primary_category": primary_category,
        "author_affiliation": author_affiliation,
    }).execute()
    res = db.from_("papers").select("paper_id").eq("arxiv_id", arxiv_id).limit(1).execute()
    return res.data[0]["paper_id"]


def get_paper_id_by_arxiv_id(arxiv_id: str) -> Optional[int]:
    db = app_schema()
    res = db.from_("papers").select("paper_id").eq("arxiv_id", arxiv_id).limit(1).execute()
    if res.data:
        return res.data[0]["paper_id"]
    return None


def analysis_exists(paper_id: int, prompt_id: str) -> bool:
    db = app_schema()
    res = db.from_("analysis_results").select("analysis_id").eq("paper_id", paper_id).eq("prompt_id", prompt_id).limit(1).execute()
    return bool(res.data)


def update_paper_author_affiliation(paper_id: int, author_affiliation: str) -> None:
    db = app_schema()
    db.from_("papers").update({"author_affiliation": author_affiliation}).eq("paper_id", paper_id).execute()


def link_paper_category(paper_id: int, category_name: str) -> None:
    db = app_schema()
    category_id = upsert_category(category_name)
    # try exists
    res = db.from_("paper_categories").select("paper_id").eq("paper_id", paper_id).eq("category_id", category_id).limit(1).execute()
    if res.data:
        return
    db.from_("paper_categories").insert({"paper_id": paper_id, "category_id": category_id}).execute()


def count_papers_by_date_category(date: str | dt.date, category: str) -> int:
    """å¿«é€Ÿè®¡æ•°ï¼šä»…æ£€æŸ¥papersè¡¨ä¸­æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°é‡ï¼ˆç”¨äºä¼˜åŒ–æœç´¢æ€§èƒ½ï¼‰"""
    db = app_schema()
    date_str = _ensure_date(date)
    
    try:
        # ğŸš€ ä¼˜åŒ–ï¼šåªæŸ¥papersè¡¨ï¼Œä¸join categoriesï¼Œå¤§å¹…æé€Ÿ
        result = (
            db.from_("papers")
            .select("*", count="exact")
            .eq("update_date", date_str)
            .execute()
        )
        return result.count or 0
    except Exception:
        return 0


def get_arxiv_ids_from_api(date: str | dt.date, category: str) -> List[str]:
    """è½»é‡çº§ArXiv APIè°ƒç”¨ï¼šåªè·å–arxiv_idåˆ—è¡¨ï¼ˆç”¨äºæ™ºèƒ½å¯¼å…¥åˆ¤æ–­ï¼‰"""
    import feedparser
    import requests
    import datetime as dt
    import pytz
    
    target_date = dt.datetime.strptime(str(date), "%Y-%m-%d").date() if isinstance(date, str) else date
    et_tz = pytz.timezone("US/Eastern")
    
    start_et = et_tz.localize(dt.datetime.combine(target_date - dt.timedelta(days=1), dt.time(20, 0)))
    end_et = et_tz.localize(dt.datetime.combine(target_date, dt.time(20, 0)))
    start_utc = start_et.astimezone(dt.timezone.utc)
    end_utc = end_et.astimezone(dt.timezone.utc)
    
    start_date_str = start_utc.strftime("%Y%m%d%H%M%S")
    end_date_str = end_utc.strftime("%Y%m%d%H%M%S")
    
    url = (
        f"https://export.arxiv.org/api/query?"
        f"search_query=cat:{category}+AND+submittedDate:[{start_date_str}+TO+{end_date_str}]&"
        "sortBy=submittedDate&sortOrder=descending&max_results=2000"
    )
    
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        feed = feedparser.parse(resp.text)
        
        arxiv_ids = []
        for entry in feed.entries:
            # å¿«é€Ÿè§£æIDï¼Œä¸å¤„ç†å…¶ä»–å­—æ®µ
            import re
            candidates = [getattr(entry, "id", ""), getattr(entry, "link", "")]
            for candidate in candidates:
                m = re.search(r"/abs/([0-9]{4}\.[0-9]{5}(v\d+)?)(?:[?#].*)?$", candidate)
                if m:
                    arxiv_ids.append(m.group(1))
                    break
        
        print(f"[æ™ºèƒ½å¯¼å…¥] ArXiv APIè¿”å› {len(arxiv_ids)} ä¸ªID")
        return arxiv_ids
    except Exception as e:
        print(f"[æ™ºèƒ½å¯¼å…¥] ArXiv APIè°ƒç”¨å¤±è´¥: {e}")
        return []


def get_existing_arxiv_ids_by_date(date: str | dt.date, arxiv_ids: List[str]) -> List[str]:
    """é«˜æ•ˆæ£€æŸ¥ï¼šæŒ‡å®šæ—¥æœŸä¸‹å·²å­˜åœ¨çš„arxiv_idåˆ—è¡¨"""
    if not arxiv_ids:
        return []
    
    db = app_schema()
    date_str = _ensure_date(date)
    
    try:
        # åˆ†æ‰¹æŸ¥è¯¢ï¼Œé¿å…INå­å¥è¿‡é•¿
        existing_ids = []
        chunk_size = 100
        for i in range(0, len(arxiv_ids), chunk_size):
            chunk = arxiv_ids[i:i + chunk_size]
            result = (
                db.from_("papers")
                .select("arxiv_id")
                .eq("update_date", date_str)
                .in_("arxiv_id", chunk)
                .execute()
                .data
            )
            existing_ids.extend([r["arxiv_id"] for r in result])
        
        print(f"[æ™ºèƒ½å¯¼å…¥] DBä¸­å·²å­˜åœ¨ {len(existing_ids)}/{len(arxiv_ids)} ä¸ªID")
        return existing_ids
    except Exception as e:
        print(f"[æ™ºèƒ½å¯¼å…¥] DBæ£€æŸ¥å¤±è´¥: {e}")
        return []


def smart_check_and_read(date: str | dt.date, category: str, arxiv_ids: List[str]) -> Dict[str, Any]:
    """ğŸš€ ä¸€ä½“åŒ–æ“ä½œï¼šæ£€æŸ¥å­˜åœ¨æ€§+è¯»å–å®Œæ•´æ•°æ®ï¼Œé¿å…ä¸¤æ¬¡DBæŸ¥è¯¢
    
    ä¿®å¤ï¼šåˆ†ä¸¤æ­¥æ£€æŸ¥
    1. æ£€æŸ¥è®ºæ–‡æ˜¯å¦åœ¨æ•°æ®åº“ä¸­å­˜åœ¨ï¼ˆæŒ‰date+arxiv_idï¼‰
    2. æ£€æŸ¥æ˜¯å¦å·²å»ºç«‹è¯¥åˆ†ç±»å…³è”ï¼Œå¦‚æœæ²¡æœ‰åˆ™éœ€è¦è¡¥å»º
    """
    import time
    
    if not arxiv_ids:
        return {'existing_ids': [], 'articles': []}
    
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    try:
        print(f"[ä¸€ä½“åŒ–] å¼€å§‹è”åˆæŸ¥è¯¢ï¼šå­˜åœ¨æ€§æ£€æŸ¥+å®Œæ•´æ•°æ®è¯»å–")
        start_time = time.time()
        
        # ğŸ”§ ä¿®å¤ï¼šç¬¬ä¸€æ­¥ï¼Œæ£€æŸ¥è¯¥æ—¥æœŸä¸‹æ‰€æœ‰å·²å­˜åœ¨çš„è®ºæ–‡ï¼ˆä¸é™åˆ†ç±»å…³è”ï¼‰
        all_existing_papers = []
        chunk_size = 100
        for i in range(0, len(arxiv_ids), chunk_size):
            chunk = arxiv_ids[i:i + chunk_size]
            
            # æŸ¥è¯¢è¯¥æ—¥æœŸä¸‹æ‰€æœ‰å·²å­˜åœ¨çš„è®ºæ–‡
            papers_result = (
                db.from_("papers")
                .select("paper_id, arxiv_id, title, authors, abstract, link, author_affiliation")
                .eq("update_date", date_str)
                .in_("arxiv_id", chunk)
                .order("arxiv_id", desc=True)
                .execute()
                .data
            )
            all_existing_papers.extend(papers_result)
        
        existing_paper_ids = [p["paper_id"] for p in all_existing_papers]
        existing_arxiv_ids = [p["arxiv_id"] for p in all_existing_papers]
        
        # ğŸ”§ ä¿®å¤ï¼šç¬¬äºŒæ­¥ï¼Œæ£€æŸ¥å“ªäº›è®ºæ–‡å·²ç»å»ºç«‹äº†è¯¥åˆ†ç±»å…³è”
        linked_paper_ids = []
        if existing_paper_ids:
            for i in range(0, len(existing_paper_ids), chunk_size):
                chunk_ids = existing_paper_ids[i:i + chunk_size]
                linked_result = (
                    db.from_("paper_categories")
                    .select("paper_id")
                    .eq("category_id", category_id)
                    .in_("paper_id", chunk_ids)
                    .execute()
                    .data
                )
                linked_paper_ids.extend([r["paper_id"] for r in linked_result])
        
        # æ‰¾å‡ºå·²å»ºç«‹åˆ†ç±»å…³è”çš„è®ºæ–‡æ•°æ®
        linked_papers = [p for p in all_existing_papers if p["paper_id"] in linked_paper_ids]
        
        query_time = time.time() - start_time
        print(f"[ä¸€ä½“åŒ–] è”åˆæŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {query_time:.2f}s | è¯¥æ—¥æœŸå·²æœ‰ {len(existing_arxiv_ids)} æ¡ï¼Œè¯¥åˆ†ç±»ä¸‹å·²å…³è” {len(linked_papers)} æ¡")
        
        # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼ï¼ˆåªè¿”å›å·²å»ºç«‹åˆ†ç±»å…³è”çš„è®ºæ–‡ï¼‰
        articles = []
        for idx, r in enumerate(linked_papers, start=1):
            articles.append({
                "number": idx,
                "id": r["arxiv_id"],
                "title": r["title"],
                "authors": r.get("authors") or "",
                "abstract": r.get("abstract") or "",
                "link": r.get("link") or "",
                "author_affiliation": r.get("author_affiliation") or "",
            })
        
        return {
            'existing_ids': existing_arxiv_ids,  # ğŸ”§ è¿”å›æ‰€æœ‰å·²å­˜åœ¨çš„è®ºæ–‡IDï¼ˆç”¨äºå¯¼å…¥åˆ¤æ–­ï¼‰
            'articles': articles,  # ğŸ”§ è¿”å›å·²å»ºç«‹åˆ†ç±»å…³è”çš„è®ºæ–‡ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
            'query_time': query_time
        }
        
    except Exception as e:
        print(f"[ä¸€ä½“åŒ–] è”åˆæŸ¥è¯¢å¤±è´¥: {e}")
        return {'existing_ids': [], 'articles': []}


def list_papers_by_date_category_reliable(date: str | dt.date, category: str) -> List[Dict[str, Any]]:
    """
    å¯é ç‰ˆæœ¬ï¼šä½¿ç”¨åˆ†æ­¥æŸ¥è¯¢ç¡®ä¿ä¸ä¼šä¸¢å¤±æ•°æ®
    1. å…ˆè·å–æ‰€æœ‰è¯¥åˆ†ç±»çš„paper_idï¼ˆåˆ†é¡µï¼‰
    2. å†æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„papersï¼ˆåˆ†å—ï¼‰
    """
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    print(f"[repo-reliable] å¼€å§‹æŸ¥è¯¢ date={date_str}, category={category}")
    
    # 1. è·å–è¯¥åˆ†ç±»çš„æ‰€æœ‰paper_idï¼ˆåˆ†é¡µé¿å…æˆªæ–­ï¼‰
    all_paper_ids = []
    page_size = 1000
    offset = 0
    while True:
        batch = (
            db.from_("paper_categories")
            .select("paper_id")
            .eq("category_id", category_id)
            .range(offset, offset + page_size - 1)
            .execute()
            .data
        )
        if not batch:
            break
        all_paper_ids.extend([r["paper_id"] for r in batch])
        offset += page_size
        if len(batch) < page_size:
            break
    
    print(f"[repo-reliable] è¯¥åˆ†ç±»æ€»paper_idæ•°: {len(all_paper_ids)}")
    
    # 2. åˆ†å—æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„papers
    papers = []
    chunk_size = 500
    for i in range(0, len(all_paper_ids), chunk_size):
        chunk_ids = all_paper_ids[i:i + chunk_size]
        chunk_papers = (
            db.from_("papers")
            .select("paper_id, arxiv_id, title, authors, abstract, link, author_affiliation")
            .in_("paper_id", chunk_ids)
            .eq("update_date", date_str)
            .order("arxiv_id", desc=True)
            .execute()
            .data
        )
        papers.extend(chunk_papers)
    
    print(f"[repo-reliable] æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°: {len(papers)}")
    
    # 3. è½¬æ¢ä¸ºarticlesæ ¼å¼
    articles = []
    for idx, paper in enumerate(papers, start=1):
        articles.append({
            "number": idx,
            "id": paper["arxiv_id"],
            "title": paper["title"],
            "authors": paper.get("authors") or "",
            "abstract": paper.get("abstract") or "",
            "link": paper.get("link") or "",
            "author_affiliation": paper.get("author_affiliation") or "",
        })
    
    return articles

def list_papers_by_date_category(date: str | dt.date, category: str) -> List[Dict[str, Any]]:
    db = app_schema()
    date_str = _ensure_date(date)
    # ä¼˜å…ˆèµ°ä¸€æ¬¡è¯·æ±‚çš„å†…è”JOINæŸ¥è¯¢ï¼ˆé€šè¿‡å¤–é”®è‡ªåŠ¨å…³ç³»ï¼‰ï¼Œæ˜¾è‘—å‡å°‘ç½‘ç»œå¾€è¿”
    category_id = upsert_category(category)
    try:
        # ä½¿ç”¨rangeæ›¿ä»£limitä»¥é¿å…Supabaseçš„é»˜è®¤é™åˆ¶ï¼Œå¹¶å¢åŠ æ›´å¤§çš„èŒƒå›´
        joined_rows = (
            db.from_("paper_categories")
            .select("papers!inner(paper_id, arxiv_id, title, authors, abstract, link, author_affiliation)")
            .eq("category_id", category_id)
            .eq("papers.update_date", date_str)
            .order("arxiv_id", foreign_table="papers", desc=True)
            .range(0, 9999)  # å¢åŠ åˆ°10000æ¡è®°å½•ï¼Œç¡®ä¿ä¸ä¼šæˆªæ–­
            .execute()
            .data
        )
        rows: List[Dict[str, Any]] = []
        for r in joined_rows:
            p = r.get("papers") or {}
            if p:
                rows.append(p)
        # è°ƒè¯•æ—¥å¿—
        try:
            print(f"[repo] fast-join rows for date={date_str}, category={category}: {len(rows)}")
            print(f"[repo] joined_rowsåŸå§‹æ•°é‡: {len(joined_rows)}, è¿‡æ»¤åçš„rowsæ•°é‡: {len(rows)}")
            if len(joined_rows) != len(rows):
                print(f"[repo] æ³¨æ„ï¼šjoined_rowså’Œrowsæ•°é‡ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨ç©ºpaperså­—æ®µ")
            
            # å¦‚æœè¿”å›çš„æ•°æ®çœ‹èµ·æ¥è¢«æˆªæ–­äº†ï¼ˆæ¯”å¦‚æ­£å¥½æ˜¯5000æˆ–10000ï¼‰ï¼Œæˆ–è€…æ•°é‡æ˜¾è‘—åå°‘ï¼Œä½¿ç”¨å¯é ç‰ˆæœ¬
            if len(rows) in [5000, 10000]:
                print(f"[repo] æ£€æµ‹åˆ°å¯èƒ½çš„æˆªæ–­ï¼ˆ{len(rows)}æ¡ï¼‰ï¼Œåˆ‡æ¢åˆ°å¯é ç‰ˆæœ¬")
                return list_papers_by_date_category_reliable(date, category)
            
            # å¦‚æœJOINæŸ¥è¯¢çš„ç»“æœæ˜æ˜¾å°‘äºé¢„æœŸï¼ˆæ¯”å¦‚åªæœ‰129æ¡ä½†å®é™…åº”è¯¥æœ‰132æ¡ï¼‰ï¼Œä¹Ÿåˆ‡æ¢åˆ°å¯é ç‰ˆæœ¬
            if len(joined_rows) > 0 and len(rows) != len(joined_rows):
                print(f"[repo] JOINç»“æœä¸ä¸€è‡´ï¼ˆåŸå§‹{len(joined_rows)}ï¼Œè¿‡æ»¤å{len(rows)}ï¼‰ï¼Œåˆ‡æ¢åˆ°å¯é ç‰ˆæœ¬")
                return list_papers_by_date_category_reliable(date, category)
                
        except Exception:
            pass
    except Exception as _join_err:
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å¯é ç‰ˆæœ¬
        print(f"[repo] JOINæŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨å¯é ç‰ˆæœ¬: {_join_err}")
        return list_papers_by_date_category_reliable(date, category)
    # map to legacy articles structure
    articles: List[Dict[str, Any]] = []
    for idx, r in enumerate(rows, start=1):
        articles.append({
            "number": idx,
            "id": r["arxiv_id"],
            "title": r["title"],
            "authors": r.get("authors") or "",
            "abstract": r.get("abstract") or "",
            "link": r.get("link") or "",
            "author_affiliation": r.get("author_affiliation") or "",
        })
    return articles


def get_prompt_id_by_name(prompt_name: str = "system_default") -> Optional[str]:
    db = app_schema()
    res = db.from_("prompts").select("prompt_id").eq("prompt_name", prompt_name).limit(1).execute()
    if res.data:
        return res.data[0]["prompt_id"]
    return None


def get_prompt_content_by_name(prompt_name: str = "multi-modal-llm") -> Optional[str]:
    """æ ¹æ®æç¤ºè¯åç§°è·å–å†…å®¹ã€‚"""
    db = app_schema()
    res = db.from_("prompts").select("prompt_content").eq("prompt_name", prompt_name).limit(1).execute()
    if res.data:
        return res.data[0]["prompt_content"]
    return None


def get_system_prompt() -> str:
    """è·å–ç³»ç»Ÿåˆ†ææç¤ºè¯ï¼ˆä¼˜å…ˆä»æ•°æ®åº“è¯»å–ï¼Œå›é€€åˆ°æ–‡ä»¶ï¼‰ã€‚"""
    # ä¼˜å…ˆä»æ•°æ®åº“è¯»å– multi-modal-llm prompt
    db_prompt = get_prompt_content_by_name("multi-modal-llm")
    if db_prompt:
        print("ğŸ“‹ [æç¤ºè¯] ä»æ•°æ®åº“è¯»å– multi-modal-llm æç¤ºè¯")
        return db_prompt
    
    # å›é€€æ–¹æ¡ˆï¼šä»æ–‡ä»¶è¯»å–ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    import os
    fallback_file = "prompt/multi-modal-llm-judger-example.md"
    if os.path.exists(fallback_file):
        print(f"âš ï¸  [æç¤ºè¯] æ•°æ®åº“ä¸­æœªæ‰¾åˆ° multi-modal-llmï¼Œå›é€€åˆ°æ–‡ä»¶: {fallback_file}")
        with open(fallback_file, 'r', encoding='utf-8') as f:
            return f.read().strip()
    
    raise Exception("æ— æ³•æ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯ï¼šæ•°æ®åº“ä¸­æ—  multi-modal-llm è®°å½•ï¼Œä¸”æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")


def list_unanalyzed_papers(date: str | dt.date, category: str, prompt_id: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    db = app_schema()
    date_str = _ensure_date(date)
    # paper ids by category - ä½¿ç”¨åˆ†é¡µé¿å…æˆªæ–­
    category_id = upsert_category(category)
    paper_ids = []
    page_size = 1000
    offset = 0
    while True:
        rows = (
            db.from_("paper_categories")
            .select("paper_id")
            .eq("category_id", category_id)
            .range(offset, offset + page_size - 1)
            .execute()
            .data
        )
        if not rows:
            break
        paper_ids.extend([r["paper_id"] for r in rows])
        offset += page_size
        if len(rows) < page_size:
            break
    
    if not paper_ids:
        return []

    # already analyzed for this prompt - åˆ†å—æŸ¥è¯¢
    analyzed_ids = set()
    chunk_size = 50
    for i in range(0, len(paper_ids), chunk_size):
        chunk_ids = paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("analysis_results")
            .select("paper_id")
            .eq("prompt_id", prompt_id)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        analyzed_ids.update(r["paper_id"] for r in chunk_rows)

    # candidate papers (by date) - åˆ†å—æŸ¥è¯¢
    candidates = []
    for i in range(0, len(paper_ids), chunk_size):
        chunk_ids = paper_ids[i:i + chunk_size]
        chunk_candidates = (
            db.from_("papers")
            .select("paper_id, arxiv_id, title, authors, abstract, link, author_affiliation")
            .eq("update_date", date_str)
            .in_("paper_id", chunk_ids)
            .order("arxiv_id")
            .execute()
            .data
        )
        candidates.extend(chunk_candidates)

    pending = [r for r in candidates if r["paper_id"] not in analyzed_ids]
    if limit:
        pending = pending[:limit]
    return pending


def insert_analysis_result(
    *, paper_id: int, prompt_id: str, analysis_json: Dict[str, Any], created_by: Optional[str]
) -> Optional[int]:
    db = app_schema()
    try:
        db.from_("analysis_results").insert({
            "paper_id": paper_id,
            "prompt_id": prompt_id,
            "analysis_result": analysis_json,
            "created_by": created_by,
        }).execute()
        # fetch id
        res = db.from_("analysis_results").select("analysis_id").eq("paper_id", paper_id).eq("prompt_id", prompt_id).limit(1).execute()
        if res.data:
            return res.data[0]["analysis_id"]
        return None
    except Exception:
        # likely unique violation (paper_id, prompt_id) â†’ ignore
        return None


def get_analysis_results(
    *, date: str | dt.date, category: str, prompt_id: str, limit: Optional[int] = None, order_by: str = "arxiv_id.asc",
    time_filter: Optional[str] = None, batch_filter: Optional[List[int]] = None
) -> List[Dict[str, Any]]:
    """æŒ‰â€œå½“å¤© + åˆ†ç±» + promptâ€è¿”å›åˆ†æç»“æœã€‚

    - ä»…è¿”å›å½“å¤©è¯¥åˆ†ç±»çš„è®ºæ–‡åˆ†æï¼ˆä¸ä¼šæ··å…¥å…¶å®ƒæ—¥æœŸï¼‰
    - å…ƒæ•°æ®ä» papers è¡¨å–ï¼ˆtitle/authors/abstract/link/author_affiliationï¼‰
    - é¡ºåºä¸æœç´¢é¡µä¸€è‡´ï¼ˆæŒ‰ arxiv_id å‡åºï¼‰
    - limit è‹¥æä¾›ï¼Œåˆ™åœ¨æœ€ç»ˆé¡ºåºä¸Šæˆªæ–­
    """
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)

    # 1) å–å½“å¤©+åˆ†ç±»çš„ papers åˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰
    #    å…ˆå–è¯¥åˆ†ç±»ä¸‹å…¨éƒ¨ paper_idï¼ˆåˆ†é¡µé˜²æ­¢æˆªæ–­ï¼‰
    all_category_paper_ids: List[int] = []
    page_size = 1000
    offset = 0
    while True:
        rows = (
            db.from_("paper_categories").select("paper_id").eq("category_id", category_id)
            .range(offset, offset + page_size - 1).execute().data
        )
        if not rows:
            break
        all_category_paper_ids.extend([r["paper_id"] for r in rows])
        offset += page_size
        if len(rows) < page_size:
            break
    if not all_category_paper_ids:
        return []

    # å½“å¤©è¯¥åˆ†ç±»çš„ papersï¼ˆä¿åºï¼‰
    papers_rows: List[Dict[str, Any]] = []
    # åˆ†å—é¿å… in() è¿‡é•¿
    chunk = 1000
    temp_rows: List[Dict[str, Any]] = []
    for i in range(0, len(all_category_paper_ids), chunk):
        part_ids = all_category_paper_ids[i:i + chunk]
        query = (
            db.from_("papers")
            .select("paper_id, arxiv_id, title, authors, abstract, link, author_affiliation, update_time")
            .eq("update_date", date_str)
            .in_("paper_id", part_ids)
        )
        
        # åº”ç”¨æ—¶é—´ç­›é€‰ï¼ˆå¦‚æœæä¾›ï¼‰- ä¿ç•™å‘åå…¼å®¹
        if time_filter == "after_18":
            query = query.gte("update_time", "18:00:00").lte("update_time", "23:59:59")
        
        # åº”ç”¨æ‰¹æ¬¡ç­›é€‰ï¼ˆå¦‚æœæä¾›ï¼‰
        if batch_filter:
            query = query.in_("paper_id", batch_filter)
        
        part = query.order("arxiv_id").execute().data
        temp_rows.extend(part)
    # å¯èƒ½é¡ºåºå·²æŒ‰ arxiv_idï¼›ä¸ºç¨³å¦¥ï¼Œç»Ÿä¸€å†æ’ä¸€æ¬¡
    # æœç´¢é¡µæŒ‰ arxiv_id é™åºï¼ˆæœ€æ–°ç¼–å·é å‰ï¼‰ï¼Œä¿æŒä¸€è‡´
    papers_rows = sorted(temp_rows, key=lambda r: r.get("arxiv_id", ""), reverse=True)
    if not papers_rows:
        return []

    date_paper_ids_ordered = [r["paper_id"] for r in papers_rows]

    # 2) è¯»å–è¯¥ prompt çš„ analysis ç»“æœï¼ˆä»…å½“å¤©papersï¼‰â†’ å»ºæ˜ å°„
    analyzed_map: Dict[int, Dict[str, Any]] = {}
    # åˆ†é¡µè¯»å– analysis_resultsï¼Œé¿å…é»˜è®¤åˆ†é¡µ
    offset = 0
    page_size = 1000
    id_set = set(date_paper_ids_ordered)
    while True:
        rows = (
            db.from_("analysis_results")
            .select("paper_id, analysis_result, norm_score")
            .eq("prompt_id", prompt_id)
            .range(offset, offset + page_size - 1)
            .order("norm_score", desc=True)
            .execute().data
        )
        if not rows:
            break
        for r in rows:
            pid = r.get("paper_id")
            if pid in id_set and pid not in analyzed_map:
                analyzed_map[pid] = r
        offset += page_size
        if len(rows) < page_size:
            break

    # 3) ç»„è£…è¾“å‡ºï¼šæŒ‰ papers_rows é¡ºåºï¼Œä»…åŒ…å«å·²åˆ†æçš„ï¼›åº”ç”¨ limit
    articles: List[Dict[str, Any]] = []
    for idx, p in enumerate(papers_rows, start=1):
        row = analyzed_map.get(p["paper_id"])  # è‹¥æ— åˆ†æåˆ™è·³è¿‡
        if not row:
            continue
        articles.append({
            "number": len(articles) + 1,
            "id": p.get("arxiv_id", ""),  # æ·»åŠ idå­—æ®µä¾›å‰ç«¯ä½¿ç”¨
            "paper_id": p.get("paper_id"),
            "analysis_result": json.dumps(row["analysis_result"], ensure_ascii=False, separators=(",", ":")),
            "title": p.get("title", ""),
            "authors": p.get("authors", ""),
            "abstract": p.get("abstract", ""),
            "link": p.get("link", ""),
            "author_affiliation": p.get("author_affiliation", ""),
            "update_time": p.get("update_time", ""),
        })
        if limit and len(articles) >= limit:
            break
    return articles


def get_ingest_batches(date: str | dt.date, category: str) -> Dict[str, Any]:
    """è·å–æŒ‡å®šæ—¥æœŸå’Œåˆ†ç±»çš„ingestæ‰¹æ¬¡ä¿¡æ¯"""
    from datetime import datetime, timedelta
    
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    try:
        # è·å–è¯¥æ—¥æœŸè¯¥åˆ†ç±»çš„æ‰€æœ‰è®ºæ–‡ingest_atä¿¡æ¯
        papers_with_ingest = (
            db.from_("paper_categories")
            .select("papers!inner(paper_id, arxiv_id, ingest_at)")
            .eq("category_id", category_id)
            .eq("papers.update_date", date_str)
            .order("ingest_at", foreign_table="papers")
            .range(0, 9999)
            .execute()
            .data
        )
        
        # å±•å¼€paperså­—æ®µ
        papers = []
        for item in papers_with_ingest:
            paper = item.get("papers", {})
            if paper:
                papers.append(paper)
        
        if not papers:
            return {
                'date': date_str,
                'category': category,
                'total_papers': 0,
                'batch_count': 0,
                'batches': []
            }
        
        # æŒ‰ingest_atæ—¶é—´è¿›è¡Œæ‰¹æ¬¡åˆ†ç»„
        # è®¾ç½®æ‰¹æ¬¡é—´éš”é˜ˆå€¼ï¼ˆ30åˆ†é’Ÿï¼‰
        batch_threshold = timedelta(minutes=30)
        
        batches = []
        current_batch = [papers[0]]
        last_time = datetime.fromisoformat(papers[0]['ingest_at'].replace('Z', '+00:00').replace('+00:00', ''))
        
        for paper in papers[1:]:
            current_time = datetime.fromisoformat(paper['ingest_at'].replace('Z', '+00:00').replace('+00:00', ''))
            
            if current_time - last_time > batch_threshold:
                # å¼€å§‹æ–°æ‰¹æ¬¡
                batches.append(current_batch)
                current_batch = [paper]
            else:
                current_batch.append(paper)
            
            last_time = current_time
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batches.append(current_batch)
        
        # æ„é€ è¿”å›æ•°æ®
        batch_info = []
        for i, batch in enumerate(batches, 1):
            batch_start = datetime.fromisoformat(batch[0]['ingest_at'].replace('Z', '+00:00').replace('+00:00', ''))
            
            batch_info.append({
                'batch_id': i,
                'batch_label': batch_start.strftime('%m-%d %H:%M'),
                'start_time': batch_start.isoformat(),
                'paper_count': len(batch),
                'paper_ids': [p['paper_id'] for p in batch]
            })
        
        return {
            'date': date_str,
            'category': category,
            'total_papers': len(papers),
            'batch_count': len(batches),
            'batches': batch_info
        }
        
    except Exception as e:
        print(f"è·å–æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            'date': date_str,
            'category': category,
            'total_papers': 0,
            'batch_count': 0,
            'batches': []
        }


def get_analysis_status_fast(date: str | dt.date, category: str, prompt_id: str) -> Dict[str, int]:
    """è·å–æŒ‡å®šæ—¥æœŸå’Œåˆ†ç±»çš„åˆ†æè¿›åº¦çŠ¶æ€ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰ã€‚"""
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    print(f"ğŸš€ [æ€§èƒ½ä¼˜åŒ–] å¼€å§‹è·å–åˆ†æçŠ¶æ€: date={date_str}, category={category}")
    start_time = time.time()
    
    try:
        # ğŸš€ ä¼˜åŒ–ç­–ç•¥ï¼šä½¿ç”¨é«˜æ•ˆçš„JOINæŸ¥è¯¢ä¸€æ¬¡æ€§è·å–æ•°æ®
        # ä½¿ç”¨å†…è”JOINè·å–è¯¥æ—¥æœŸ+åˆ†ç±»çš„æ‰€æœ‰è®ºæ–‡ï¼Œå·¦è¿æ¥åˆ†æç»“æœ
        print(f"â±ï¸  [æ€§èƒ½ä¼˜åŒ–] å¼€å§‹JOINæŸ¥è¯¢...")
        
        joined_rows = (
            db.from_("paper_categories")
            .select("papers!inner(paper_id), analysis_results!left(analysis_id)")
            .eq("category_id", category_id)
            .eq("papers.update_date", date_str)
            .eq("analysis_results.prompt_id", prompt_id)
            .range(0, 9999)  # æ”¯æŒæœ€å¤š10000ç¯‡è®ºæ–‡
            .execute()
            .data
        )
        
        print(f"â±ï¸  [æ€§èƒ½ä¼˜åŒ–] JOINæŸ¥è¯¢å®Œæˆï¼Œè¿”å›: {len(joined_rows)} æ¡")
        
        # ç»Ÿè®¡ç»“æœ
        total = len(joined_rows)
        completed = 0
        
        for row in joined_rows:
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
            analysis_results = row.get("analysis_results")
            if analysis_results and len(analysis_results) > 0:
                completed += 1
        
        pending = max(total - completed, 0)
        result = {"total": total, "completed": completed, "pending": pending}
        
        total_elapsed = time.time() - start_time
        print(f"âœ… [æ€§èƒ½ä¼˜åŒ–] JOINæŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.3f}s | ç»“æœ: {result}")
        
        return result
        
    except Exception as e:
        print(f"âŒ [æ€§èƒ½ä¼˜åŒ–] JOINæŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
        return get_analysis_status_legacy(date, category, prompt_id)


def get_analysis_status_legacy(date: str | dt.date, category: str, prompt_id: str) -> Dict[str, int]:
    """åŸå§‹çš„åˆ†å—æŸ¥è¯¢ç‰ˆæœ¬ï¼ˆç”¨ä½œæœ€ç»ˆå›é€€ï¼‰ã€‚"""
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    print(f"ğŸ”„ [å›é€€æ¨¡å¼] ä½¿ç”¨åŸå§‹åˆ†å—æŸ¥è¯¢")
    start_time = time.time()
    
    # æ‰€æœ‰è¯¥åˆ†ç±»ä¸‹çš„ paper_idï¼ˆä¼˜åŒ–ï¼šå¢å¤§é¡µé¢å¤§å°ï¼‰
    all_category_paper_ids: List[int] = []
    page_size = 2000  # å¢å¤§é¡µé¢å¤§å°
    offset = 0
    while True:
        rows = (
            db.from_("paper_categories")
            .select("paper_id")
            .eq("category_id", category_id)
            .range(offset, offset + page_size - 1)
            .execute()
            .data
        )
        if not rows:
            break
        all_category_paper_ids.extend([r["paper_id"] for r in rows])
        offset += page_size
        if len(rows) < page_size:
            break
    
    if not all_category_paper_ids:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # è¯¥æ—¥æœŸçš„è®ºæ–‡ï¼ˆä¼˜åŒ–ï¼šå¢å¤§chunk_sizeï¼‰
    date_paper_ids = []
    chunk_size = 200  # å¢å¤§chunk_size
    for i in range(0, len(all_category_paper_ids), chunk_size):
        chunk_ids = all_category_paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("papers")
            .select("paper_id")
            .eq("update_date", date_str)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        date_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    total = len(date_paper_ids)
    if total == 0:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # å·²åˆ†æçš„è®ºæ–‡
    completed_paper_ids = []
    for i in range(0, len(date_paper_ids), chunk_size):
        chunk_ids = date_paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("analysis_results")
            .select("paper_id")
            .eq("prompt_id", prompt_id)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        completed_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    completed = len(completed_paper_ids)
    pending = max(total - completed, 0)
    
    result = {"total": total, "completed": completed, "pending": pending}
    total_elapsed = time.time() - start_time
    print(f"ğŸ”„ [å›é€€æ¨¡å¼] åˆ†å—æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.3f}s | ç»“æœ: {result}")
    
    return result


def get_analysis_status_original(date: str | dt.date, category: str, prompt_id: str) -> Dict[str, int]:
    """åŸå§‹æœªä¼˜åŒ–ç‰ˆæœ¬ï¼Œç”¨äºå¯¹æ¯”éªŒè¯ã€‚"""
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    print(f"ğŸ” [åŸå§‹ç‰ˆæœ¬] å¼€å§‹è·å–åˆ†æçŠ¶æ€: date={date_str}, category={category}")
    start_time = time.time()
    
    # æ‰€æœ‰è¯¥åˆ†ç±»ä¸‹çš„ paper_id
    all_category_paper_ids: List[int] = []
    page_size = 1000
    offset = 0
    while True:
        rows = (
            db.from_("paper_categories")
            .select("paper_id")
            .eq("category_id", category_id)
            .range(offset, offset + page_size - 1)
            .execute()
            .data
        )
        if not rows:
            break
        all_category_paper_ids.extend([r["paper_id"] for r in rows])
        offset += page_size
        if len(rows) < page_size:
            break
    
    print(f"ğŸ” [åŸå§‹ç‰ˆæœ¬] åˆ†ç±»ä¸‹æ€»paper_idæ•°: {len(all_category_paper_ids)}")
    
    if not all_category_paper_ids:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # è¯¥æ—¥æœŸ + è¯¥åˆ†ç±»çš„ paper_id é›†åˆ
    date_paper_ids = []
    chunk_size = 100  # è½»å¾®ä¼˜åŒ–ï¼šä»50å¢åŠ åˆ°100
    for i in range(0, len(all_category_paper_ids), chunk_size):
        chunk_ids = all_category_paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("papers")
            .select("paper_id")
            .eq("update_date", date_str)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        date_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    total = len(date_paper_ids)
    print(f"ğŸ” [åŸå§‹ç‰ˆæœ¬] è¯¥æ—¥æœŸä¸‹paper_idæ•°: {total}")
    
    if total == 0:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # å·²å®Œæˆï¼ˆä»…ç»Ÿè®¡è¯¥æ—¥æœŸé›†åˆï¼‰
    completed_paper_ids = []
    # ä½¿ç”¨ç¨å¤§çš„chunk_sizeç”¨äºåˆ†æç»“æœæŸ¥è¯¢
    analysis_chunk_size = 150  # å¯¹åˆ†æç»“æœæŸ¥è¯¢ä½¿ç”¨æ›´å¤§çš„chunk
    for i in range(0, len(date_paper_ids), analysis_chunk_size):
        chunk_ids = date_paper_ids[i:i + analysis_chunk_size]
        chunk_rows = (
            db.from_("analysis_results")
            .select("paper_id")
            .eq("prompt_id", prompt_id)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        completed_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    completed = len(completed_paper_ids)
    pending = max(total - completed, 0)
    
    result = {"total": total, "completed": completed, "pending": pending}
    total_elapsed = time.time() - start_time
    print(f"ğŸ” [åŸå§‹ç‰ˆæœ¬] åˆ†æçŠ¶æ€è·å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.3f}s | ç»“æœ: {result}")
    
    return result


def get_analysis_status_optimized(date: str | dt.date, category: str, prompt_id: str) -> Dict[str, int]:
    """ä¼˜åŒ–ç‰ˆæœ¬ï¼šä¿æŒåŸé€»è¾‘ä½†æå‡æ€§èƒ½ã€‚"""
    db = app_schema()
    date_str = _ensure_date(date)
    category_id = upsert_category(category)
    
    print(f"ğŸš€ [ä¼˜åŒ–ç‰ˆæœ¬] å¼€å§‹è·å–åˆ†æçŠ¶æ€: date={date_str}, category={category}")
    start_time = time.time()
    
    # æ‰€æœ‰è¯¥åˆ†ç±»ä¸‹çš„ paper_idï¼ˆä¼˜åŒ–ï¼šå¢å¤§é¡µé¢å¤§å°ï¼‰
    all_category_paper_ids: List[int] = []
    page_size = 2000  # ä»1000å¢åŠ åˆ°2000
    offset = 0
    while True:
        rows = (
            db.from_("paper_categories")
            .select("paper_id")
            .eq("category_id", category_id)
            .range(offset, offset + page_size - 1)
            .execute()
            .data
        )
        if not rows:
            break
        all_category_paper_ids.extend([r["paper_id"] for r in rows])
        offset += page_size
        if len(rows) < page_size:
            break
    
    print(f"ğŸš€ [ä¼˜åŒ–ç‰ˆæœ¬] åˆ†ç±»ä¸‹æ€»paper_idæ•°: {len(all_category_paper_ids)}")
    
    if not all_category_paper_ids:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # è¯¥æ—¥æœŸ + è¯¥åˆ†ç±»çš„ paper_id é›†åˆï¼ˆä¼˜åŒ–ï¼šå¢å¤§chunk_sizeï¼‰
    date_paper_ids = []
    chunk_size = 200  # ä»50å¢åŠ åˆ°200
    for i in range(0, len(all_category_paper_ids), chunk_size):
        chunk_ids = all_category_paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("papers")
            .select("paper_id")
            .eq("update_date", date_str)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        date_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    total = len(date_paper_ids)
    print(f"ğŸš€ [ä¼˜åŒ–ç‰ˆæœ¬] è¯¥æ—¥æœŸä¸‹paper_idæ•°: {total}")
    
    if total == 0:
        return {"total": 0, "completed": 0, "pending": 0}
    
    # å·²å®Œæˆï¼ˆä»…ç»Ÿè®¡è¯¥æ—¥æœŸé›†åˆï¼‰ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨ç›¸åŒçš„å¤§chunk_sizeï¼‰
    completed_paper_ids = []
    for i in range(0, len(date_paper_ids), chunk_size):
        chunk_ids = date_paper_ids[i:i + chunk_size]
        chunk_rows = (
            db.from_("analysis_results")
            .select("paper_id")
            .eq("prompt_id", prompt_id)
            .in_("paper_id", chunk_ids)
            .execute()
            .data
        )
        completed_paper_ids.extend([r["paper_id"] for r in chunk_rows])
    
    completed = len(completed_paper_ids)
    pending = max(total - completed, 0)
    
    result = {"total": total, "completed": completed, "pending": pending}
    total_elapsed = time.time() - start_time
    print(f"âœ… [ä¼˜åŒ–ç‰ˆæœ¬] åˆ†æçŠ¶æ€è·å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.3f}s | ç»“æœ: {result}")
    
    return result


def get_analysis_status(date: str | dt.date, category: str, prompt_id: str) -> Dict[str, int]:
    """è·å–æŒ‡å®šæ—¥æœŸå’Œåˆ†ç±»çš„åˆ†æè¿›åº¦çŠ¶æ€ï¼ˆå…¥å£å‡½æ•°ï¼‰ã€‚"""
    # å›é€€åˆ°åŸå§‹ç‰ˆæœ¬ï¼Œç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§ä¼˜å…ˆ
    return get_analysis_status_original(date, category, prompt_id)


def list_available_dates() -> List[str]:
    """Return distinct update_date values (as ISO strings) sorted desc."""
    db = app_schema()
    rows = db.from_("papers").select("update_date").order("update_date", desc=True).execute().data
    seen: set[str] = set()
    result: List[str] = []
    for r in rows:
        d = r.get("update_date")
        if not d:
            continue
        # supabase returns 'YYYY-MM-DD' string for date
        if d not in seen:
            seen.add(d)
            result.append(d)
    return result


# =====================
# æ‰¹é‡å†™å…¥/æŸ¥è¯¢ åŠ é€Ÿæ¥å£
# =====================

def get_papers_by_arxiv_ids(arxiv_ids: List[str]) -> List[Dict[str, Any]]:
    db = app_schema()
    if not arxiv_ids:
        return []
    # PostgREST in() æœ€å¤šå‚æ•°å¯èƒ½æœ‰é™åˆ¶ï¼Œåšä¸€ä¸‹åˆ†å—
    result: List[Dict[str, Any]] = []
    chunk_size = 500
    for i in range(0, len(arxiv_ids), chunk_size):
        chunk = arxiv_ids[i:i + chunk_size]
        rows = (
            db.from_("papers")
            .select("paper_id, arxiv_id")
            .in_("arxiv_id", chunk)
            .execute()
            .data
        )
        result.extend(rows)
    return result


def upsert_papers_bulk(rows: List[Dict[str, Any]]) -> Dict[str, int]:
    """æ‰¹é‡ upsert papersï¼Œè¿”å› arxiv_id -> paper_id çš„æ˜ å°„ã€‚

    ç­–ç•¥ï¼š
      1) å…ˆæŸ¥å·²æœ‰ arxiv_id â†’ paper_idï¼ˆä¸€æ¬¡æˆ–åˆ†å—ï¼‰
      2) ä»…å¯¹ç¼ºå¤±çš„æ‰§è¡Œæ‰¹é‡ upsert/insertï¼ˆå¸¦ returningï¼‰ï¼Œå†æ•´ä½“ select ä¸€æ¬¡è·å¾—å®Œæ•´æ˜ å°„
    """
    import time
    db = app_schema()
    if not rows:
        return {}

    all_arxiv_ids = [r["arxiv_id"] for r in rows if r.get("arxiv_id")]
    existing = get_papers_by_arxiv_ids(all_arxiv_ids)
    arxiv_to_id: Dict[str, int] = {r["arxiv_id"]: r["paper_id"] for r in existing}

    missing_rows = [r for r in rows if r["arxiv_id"] not in arxiv_to_id]
    if missing_rows:
        # ğŸš€ å¯¹å¤§é‡è®ºæ–‡è¿›è¡Œåˆ†å—å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
        chunk_size = 50  # å‡å°è®ºæ–‡å†™å…¥çš„åˆ†å—å¤§å°
        
        if len(missing_rows) > chunk_size:
            print(f"[è®ºæ–‡å†™å…¥] åˆ†å—å¤„ç† {len(missing_rows)} æ¡æ–°è®ºæ–‡")
        
        for i in range(0, len(missing_rows), chunk_size):
            chunk = missing_rows[i:i + chunk_size]
            chunk_start = time.time()
            
            # æ·»åŠ é‡è¯•æœºåˆ¶
            retry_count = 0
            max_retries = 3
            while retry_count < max_retries:
                try:
                    db.from_("papers").upsert(chunk, on_conflict="arxiv_id").execute()
                    break
                except Exception as e:
                    retry_count += 1
                    if "Connection reset by peer" in str(e) and retry_count < max_retries:
                        print(f"[è®ºæ–‡å†™å…¥] å— {i//chunk_size + 1} è¿æ¥é‡ç½®ï¼Œé‡è¯• {retry_count}/{max_retries}")
                        time.sleep(2)
                        continue
                    elif retry_count >= max_retries:
                        # æœ€åå°è¯•ä½¿ç”¨insert
                        try:
                            db.from_("papers").insert(chunk).execute()
                            break
                        except Exception:
                            raise e
                    else:
                        raise e
            
            chunk_time = time.time() - chunk_start
            if chunk_time > 3:
                print(f"[è®ºæ–‡å†™å…¥] å— {i//chunk_size + 1} å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f}s")

    # ç»Ÿä¸€å† select ä¸€æ¬¡ï¼Œå¾—åˆ°å®Œæ•´æ˜ å°„
    final_rows = get_papers_by_arxiv_ids(all_arxiv_ids)
    final_map: Dict[str, int] = {r["arxiv_id"]: r["paper_id"] for r in final_rows}
    return final_map


def get_categories_by_names(names: List[str]) -> List[Dict[str, Any]]:
    db = app_schema()
    if not names:
        return []
    result: List[Dict[str, Any]] = []
    chunk_size = 1000
    for i in range(0, len(names), chunk_size):
        chunk = names[i:i + chunk_size]
        rows = (
            db.from_("categories")
            .select("category_id, category_name")
            .in_("category_name", chunk)
            .execute()
            .data
        )
        result.extend(rows)
    return result


def upsert_categories_bulk(names: List[str]) -> Dict[str, int]:
    db = app_schema()
    if not names:
        return {}
    uniq = sorted({n for n in names if n})
    existing = get_categories_by_names(uniq)
    name_to_id: Dict[str, int] = {r["category_name"]: r["category_id"] for r in existing}

    missing = [n for n in uniq if n not in name_to_id]
    if missing:
        rows = [{"category_name": n} for n in missing]
        try:
            db.from_("categories").upsert(rows, on_conflict="category_name").execute()
        except Exception:
            db.from_("categories").insert(rows).execute()

    # fetch all
    final_rows = get_categories_by_names(uniq)
    final_map: Dict[str, int] = {r["category_name"]: r["category_id"] for r in final_rows}
    return final_map


def upsert_paper_categories_bulk(pairs: List[Tuple[int, int]]) -> None:
    """æ‰¹é‡ upsert paper_categoriesï¼Œpairs ä¸º (paper_id, category_id)ã€‚"""
    import time
    db = app_schema()
    if not pairs:
        return
    rows = [{"paper_id": p, "category_id": c} for p, c in pairs]
    
    # ğŸš€ ä¼˜åŒ–ï¼šå¯¹å¤§é‡å…³è”è¿›è¡Œåˆ†å—å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
    chunk_size = 100  # è¿›ä¸€æ­¥å‡å°åˆ†å—ï¼Œé¿å…è¶…æ—¶
    total_chunks = (len(rows) + chunk_size - 1) // chunk_size
    
    if len(rows) > chunk_size:
        print(f"[æ‰¹å¤„ç†] åˆ† {total_chunks} å—å¤„ç† {len(rows)} ä¸ªåˆ†ç±»å…³è”")
    
    for i in range(0, len(rows), chunk_size):
        chunk = rows[i:i + chunk_size]
        chunk_start = time.time()
        try:
            # æ·»åŠ è¶…æ—¶é‡è¯•æœºåˆ¶
            retry_count = 0
            max_retries = 3
            while retry_count < max_retries:
                try:
                    db.from_("paper_categories").upsert(chunk, on_conflict="paper_id,category_id").execute()
                    break
                except Exception as e:
                    retry_count += 1
                    if "Connection reset by peer" in str(e) and retry_count < max_retries:
                        print(f"[æ‰¹å¤„ç†] å— {i//chunk_size + 1} è¿æ¥é‡ç½®ï¼Œé‡è¯• {retry_count}/{max_retries}")
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                        continue
                    else:
                        raise e
            
            chunk_time = time.time() - chunk_start
            if chunk_time > 5:  # å¦‚æœå•å—è€—æ—¶è¶…è¿‡5ç§’ï¼Œè®°å½•æ—¥å¿—
                print(f"[æ‰¹å¤„ç†] å— {i//chunk_size + 1}/{total_chunks} å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f}s")
                
        except Exception as e:
            # é€€åŒ–ï¼šé€æ¡æ’å…¥ï¼Œé‡åˆ°é‡å¤åˆ™å¿½ç•¥
            print(f"[æ‰¹å¤„ç†] å— {i//chunk_size + 1} upsertå¤±è´¥ï¼Œé€€åŒ–ä¸ºé€æ¡æ’å…¥: {e}")
            for row in chunk:
                try:
                    db.from_("paper_categories").insert([row]).execute()
                except Exception:
                    # å¿½ç•¥é‡å¤é”™è¯¯
                    pass




