"""
æ™ºèƒ½æœç´¢æœåŠ¡æ¨¡å—
åŸºäºarXiv IDæ–‡æœ¬è§£æå’Œæ‰¹é‡è·å–è®ºæ–‡ä¿¡æ¯
"""

import re
import urllib.request
import urllib.parse
from typing import List, Dict, Any, Tuple
import xml.etree.ElementTree as ET
from datetime import datetime
from ..db import repo as db_repo

def extract_arxiv_ids(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰arXiv ID
    æ”¯æŒæ ¼å¼: arXiv:2508.21824 æˆ– arXiv:2508.21824v1
    
    Args:
        text: åŒ…å«arXiv IDçš„æ–‡æœ¬
        
    Returns:
        å»é‡åçš„arXiv IDåˆ—è¡¨
    """
    # åŒ¹é… arXiv:xxxx.xxxxx æ ¼å¼ (å¯èƒ½å¸¦ç‰ˆæœ¬å·å¦‚v1, v2ç­‰)
    pattern = r'arXiv:(\d{4}\.\d{4,5}(?:v\d+)?)'
    matches = re.findall(pattern, text)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_ids = []
    for match in matches:
        # å»æ‰ç‰ˆæœ¬å·ï¼Œåªä¿ç•™åŸºç¡€ID
        base_id = match.split('v')[0]
        if base_id not in seen:
            seen.add(base_id)
            unique_ids.append(base_id)
    
    return unique_ids

def parse_arxiv_xml(xml_content: str) -> Dict[str, Any]:
    """
    è§£æarXiv APIè¿”å›çš„XMLï¼Œæå–è®ºæ–‡ä¿¡æ¯
    
    Args:
        xml_content: arXiv APIè¿”å›çš„XMLå†…å®¹
        
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        root = ET.fromstring(xml_content)
        
        # å®šä¹‰å‘½åç©ºé—´
        namespaces = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }
        
        # æŸ¥æ‰¾entryå…ƒç´ 
        entry = root.find('atom:entry', namespaces)
        if entry is None:
            return None
            
        # æå–åŸºæœ¬ä¿¡æ¯
        paper_info = {}
        
        # æ ‡é¢˜
        title_elem = entry.find('atom:title', namespaces)
        if title_elem is not None:
            paper_info['title'] = title_elem.text.strip().replace('\n', ' ')
        
        # arXiv ID
        id_elem = entry.find('atom:id', namespaces)
        if id_elem is not None:
            arxiv_url = id_elem.text
            clean_id = arxiv_url.split('/')[-1].replace('v1', '').replace('v2', '').replace('v3', '')
            paper_info['arxiv_id'] = clean_id
            paper_info['id'] = clean_id  # å…¼å®¹å‰ç«¯å­—æ®µå
            paper_info['paper_id'] = None  # æ™ºèƒ½æœç´¢çš„æ–‡ç« æ²¡æœ‰æ•°æ®åº“ID
        
        # æ‘˜è¦ - ä¿®æ”¹å­—æ®µåä¸ºabstractä»¥åŒ¹é…æ•°æ®åº“ç»“æ„
        summary_elem = entry.find('atom:summary', namespaces)
        if summary_elem is not None:
            paper_info['abstract'] = summary_elem.text.strip().replace('\n', ' ')
        
        # ä½œè€… - è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ŒåŒ¹é…æ•°æ®åº“ç»“æ„
        authors = []
        for author in entry.findall('atom:author', namespaces):
            name_elem = author.find('atom:name', namespaces)
            if name_elem is not None:
                authors.append(name_elem.text)
        paper_info['authors'] = ', '.join(authors) if authors else ''
        
        # å‘å¸ƒæ—¥æœŸ
        published_elem = entry.find('atom:published', namespaces)
        if published_elem is not None:
            paper_info['published'] = published_elem.text
        
        # æ›´æ–°æ—¥æœŸ - è§£æä¸ºupdate_dateå­—æ®µ
        updated_elem = entry.find('atom:updated', namespaces)
        if updated_elem is not None:
            paper_info['updated'] = updated_elem.text
            # è§£ææ—¥æœŸå¹¶æ ¼å¼åŒ–ä¸ºYYYY-MM-DDæ ¼å¼
            try:
                # arXivæ—¶é—´æ ¼å¼ï¼š2025-08-25T17:41:27Z
                dt = datetime.fromisoformat(updated_elem.text.replace('Z', '+00:00'))
                paper_info['update_date'] = dt.strftime('%Y-%m-%d')
                print(f"ğŸ“… [æ—¥æœŸè§£æ] {paper_info.get('arxiv_id', 'N/A')}: {updated_elem.text} -> {paper_info['update_date']}")
            except Exception as e:
                print(f"âš ï¸  [æ—¥æœŸè§£æ] å¤±è´¥ {updated_elem.text}: {e}")
                paper_info['update_date'] = datetime.now().strftime('%Y-%m-%d')
        else:
            # æ²¡æœ‰æ›´æ–°æ—¥æœŸæ—¶ä½¿ç”¨ä»Šå¤©
            paper_info['update_date'] = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ†ç±»
        categories = []
        for category in entry.findall('atom:category', namespaces):
            term = category.get('term')
            if term:
                categories.append(term)
        paper_info['categories'] = categories
        
        # PDFé“¾æ¥ - æ·»åŠ linkå­—æ®µä»¥åŒ¹é…æ•°æ®åº“ç»“æ„
        for link in entry.findall('atom:link', namespaces):
            if link.get('title') == 'pdf':
                paper_info['pdf_url'] = link.get('href')
                paper_info['link'] = link.get('href')  # åŒ¹é…æ•°æ®åº“ç»“æ„
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°PDFé“¾æ¥ï¼Œä½¿ç”¨arXivä¸»é¡µé“¾æ¥
        if 'link' not in paper_info and 'arxiv_id' in paper_info:
            paper_info['link'] = f"https://arxiv.org/abs/{paper_info['arxiv_id']}"
        
        # æœºæ„ä¿¡æ¯ - æ™ºèƒ½æœç´¢æš‚æ—¶ä¸åŒ…å«
        paper_info['author_affiliation'] = ''
        
        return paper_info
        
    except Exception as e:
        print(f"è§£æXMLå¤±è´¥: {e}")
        return None

def fetch_arxiv_papers_batch(arxiv_ids: List[str], timeout: int = 30) -> Dict[str, Any]:
    """
    é€šè¿‡arXiv APIæ‰¹é‡è·å–å¤šç¯‡è®ºæ–‡è¯¦ç»†ä¿¡æ¯
    
    Args:
        arxiv_ids: arXivè®ºæ–‡IDåˆ—è¡¨
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        åŒ…å«çŠ¶æ€å’Œå†…å®¹çš„å­—å…¸
    """
    if not arxiv_ids:
        return {
            'status': 'error',
            'message': 'æ²¡æœ‰æä¾›arXiv ID',
            'found_papers': [],
            'not_exist_ids': [],
            'error_ids': []
        }
    
    # æ„å»ºæ‰¹é‡API URL
    base_url = "http://export.arxiv.org/api/query?"
    # ä½¿ç”¨id_listå‚æ•°è¿›è¡Œæ‰¹é‡æŸ¥è¯¢
    id_list = ','.join(arxiv_ids)
    params = {
        'id_list': id_list,
        'start': 0,
        'max_results': min(len(arxiv_ids), 500)  # æ‰©å±•åˆ°500ç¯‡è®ºæ–‡
    }
    
    url = base_url + urllib.parse.urlencode(params)
    print(f"ğŸš€ [æ‰¹é‡æŸ¥è¯¢] å¼€å§‹æ‰¹é‡è·å– {len(arxiv_ids)} ç¯‡è®ºæ–‡ä¿¡æ¯")
    print(f"ğŸ“¡ [APIè¯·æ±‚] {url}")
    
    try:
        # å‘é€æ‰¹é‡è¯·æ±‚
        with urllib.request.urlopen(url, timeout=timeout) as response:
            xml_content = response.read().decode('utf-8')
        
        # è§£æXMLè·å–æ‰€æœ‰è®ºæ–‡ä¿¡æ¯
        return parse_arxiv_batch_xml(xml_content, arxiv_ids)
        
    except Exception as e:
        print(f"âŒ [æ‰¹é‡æŸ¥è¯¢] æ‰¹é‡è¯·æ±‚å¤±è´¥: {e}")
        return {
            'status': 'error',
            'message': str(e),
            'found_papers': [],
            'not_exist_ids': [],
            'error_ids': [{'arxiv_id': id, 'error': str(e)} for id in arxiv_ids]
        }

def parse_arxiv_batch_xml(xml_content: str, requested_ids: List[str]) -> Dict[str, Any]:
    """
    è§£æarXiv APIè¿”å›çš„æ‰¹é‡XMLï¼Œæå–æ‰€æœ‰è®ºæ–‡ä¿¡æ¯
    
    Args:
        xml_content: arXiv APIè¿”å›çš„XMLå†…å®¹
        requested_ids: è¯·æ±‚çš„arXiv IDåˆ—è¡¨
        
    Returns:
        åŒ…å«æ‰€æœ‰è®ºæ–‡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        root = ET.fromstring(xml_content)
        
        # å®šä¹‰å‘½åç©ºé—´
        namespaces = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰entryå…ƒç´ 
        entries = root.findall('atom:entry', namespaces)
        
        found_papers = []
        found_ids = set()
        
        print(f"ğŸ” [æ‰¹é‡è§£æ] æ‰¾åˆ° {len(entries)} ä¸ªXMLæ¡ç›®")
        
        for entry in entries:
            # è§£æå•ä¸ªè®ºæ–‡ä¿¡æ¯
            paper_info = parse_single_entry_xml(entry, namespaces)
            if paper_info and paper_info.get('arxiv_id'):
                found_papers.append(paper_info)
                found_ids.add(paper_info['arxiv_id'])
        
        # ç¡®å®šå“ªäº›IDæ²¡æœ‰æ‰¾åˆ°
        requested_ids_set = set(requested_ids)
        not_exist_ids = list(requested_ids_set - found_ids)
        
        print(f"âœ… [æ‰¹é‡è§£æ] æˆåŠŸ: {len(found_papers)}ç¯‡ï¼Œæœªæ‰¾åˆ°: {len(not_exist_ids)}ç¯‡")
        
        return {
            'status': 'success',
            'found_papers': found_papers,
            'not_exist_ids': not_exist_ids,
            'error_ids': []
        }
        
    except Exception as e:
        print(f"âŒ [æ‰¹é‡è§£æ] XMLè§£æå¤±è´¥: {e}")
        return {
            'status': 'parse_error',
            'message': f'XMLè§£æå¤±è´¥: {e}',
            'found_papers': [],
            'not_exist_ids': [],
            'error_ids': [{'arxiv_id': id, 'error': 'XMLè§£æå¤±è´¥'} for id in requested_ids]
        }

def parse_single_entry_xml(entry, namespaces: Dict[str, str]) -> Dict[str, Any]:
    """
    è§£æå•ä¸ªXML entryå…ƒç´ ï¼Œæå–è®ºæ–‡ä¿¡æ¯
    
    Args:
        entry: XML entryå…ƒç´ 
        namespaces: XMLå‘½åç©ºé—´å­—å…¸
        
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # æå–åŸºæœ¬ä¿¡æ¯
        paper_info = {}
        
        # æ ‡é¢˜
        title_elem = entry.find('atom:title', namespaces)
        if title_elem is not None:
            paper_info['title'] = title_elem.text.strip().replace('\n', ' ')
        
        # arXiv ID
        id_elem = entry.find('atom:id', namespaces)
        if id_elem is not None:
            arxiv_url = id_elem.text
            clean_id = arxiv_url.split('/')[-1].replace('v1', '').replace('v2', '').replace('v3', '')
            paper_info['arxiv_id'] = clean_id
            paper_info['id'] = clean_id  # å…¼å®¹å‰ç«¯å­—æ®µå
            paper_info['paper_id'] = None  # æ™ºèƒ½æœç´¢çš„æ–‡ç« æ²¡æœ‰æ•°æ®åº“ID
        
        # æ‘˜è¦ - ä¿®æ”¹å­—æ®µåä¸ºabstractä»¥åŒ¹é…æ•°æ®åº“ç»“æ„
        summary_elem = entry.find('atom:summary', namespaces)
        if summary_elem is not None:
            paper_info['abstract'] = summary_elem.text.strip().replace('\n', ' ')
        
        # ä½œè€… - è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ŒåŒ¹é…æ•°æ®åº“ç»“æ„
        authors = []
        for author in entry.findall('atom:author', namespaces):
            name_elem = author.find('atom:name', namespaces)
            if name_elem is not None:
                authors.append(name_elem.text)
        paper_info['authors'] = ', '.join(authors) if authors else ''
        
        # å‘å¸ƒæ—¥æœŸ
        published_elem = entry.find('atom:published', namespaces)
        if published_elem is not None:
            paper_info['published'] = published_elem.text
        
        # æ›´æ–°æ—¥æœŸ - è§£æä¸ºupdate_dateå­—æ®µ
        updated_elem = entry.find('atom:updated', namespaces)
        if updated_elem is not None:
            paper_info['updated'] = updated_elem.text
            # è§£ææ—¥æœŸå¹¶æ ¼å¼åŒ–ä¸ºYYYY-MM-DDæ ¼å¼
            try:
                # arXivæ—¶é—´æ ¼å¼ï¼š2025-08-25T17:41:27Z
                dt = datetime.fromisoformat(updated_elem.text.replace('Z', '+00:00'))
                paper_info['update_date'] = dt.strftime('%Y-%m-%d')
                print(f"ğŸ“… [æ—¥æœŸè§£æ] {paper_info.get('arxiv_id', 'N/A')}: {updated_elem.text} -> {paper_info['update_date']}")
            except Exception as e:
                print(f"âš ï¸  [æ—¥æœŸè§£æ] å¤±è´¥ {updated_elem.text}: {e}")
                paper_info['update_date'] = datetime.now().strftime('%Y-%m-%d')
        else:
            # æ²¡æœ‰æ›´æ–°æ—¥æœŸæ—¶ä½¿ç”¨ä»Šå¤©
            paper_info['update_date'] = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ†ç±»
        categories = []
        for category in entry.findall('atom:category', namespaces):
            term = category.get('term')
            if term:
                categories.append(term)
        paper_info['categories'] = categories
        
        # PDFé“¾æ¥ - æ·»åŠ linkå­—æ®µä»¥åŒ¹é…æ•°æ®åº“ç»“æ„
        for link in entry.findall('atom:link', namespaces):
            if link.get('title') == 'pdf':
                paper_info['pdf_url'] = link.get('href')
                paper_info['link'] = link.get('href')  # åŒ¹é…æ•°æ®åº“ç»“æ„
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°PDFé“¾æ¥ï¼Œä½¿ç”¨arXivä¸»é¡µé“¾æ¥
        if 'link' not in paper_info and 'arxiv_id' in paper_info:
            paper_info['link'] = f"https://arxiv.org/abs/{paper_info['arxiv_id']}"
        
        # æœºæ„ä¿¡æ¯ - æ™ºèƒ½æœç´¢æš‚æ—¶ä¸åŒ…å«
        paper_info['author_affiliation'] = ''
        
        return paper_info
        
    except Exception as e:
        print(f"âŒ [å•æ¡è§£æ] è§£æå•ä¸ªentryå¤±è´¥: {e}")
        return None

def fetch_arxiv_paper_info(arxiv_id: str, timeout: int = 10) -> Dict[str, Any]:
    """
    é€šè¿‡arXiv APIè·å–å•ç¯‡è®ºæ–‡è¯¦ç»†ä¿¡æ¯
    
    Args:
        arxiv_id: arXivè®ºæ–‡ID
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        åŒ…å«çŠ¶æ€å’Œå†…å®¹çš„å­—å…¸
    """
    # æ„å»ºAPI URL
    base_url = "http://export.arxiv.org/api/query?"
    query = f"id:{arxiv_id}"
    params = {
        'search_query': query,
        'start': 0,
        'max_results': 1
    }
    
    url = base_url + urllib.parse.urlencode(params)
    
    try:
        # å‘é€è¯·æ±‚
        with urllib.request.urlopen(url, timeout=timeout) as response:
            xml_content = response.read().decode('utf-8')
        
        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°è®ºæ–‡
        if '<entry>' in xml_content and '</entry>' in xml_content:
            # è§£æXMLè·å–è¯¦ç»†ä¿¡æ¯
            paper_info = parse_arxiv_xml(xml_content)
            
            if paper_info:
                return {
                    'status': 'found',
                    'paper_info': paper_info,
                    'raw_xml': xml_content,
                    'arxiv_id': arxiv_id
                }
            else:
                return {
                    'status': 'parse_error',
                    'message': 'XMLè§£æå¤±è´¥',
                    'arxiv_id': arxiv_id
                }
        else:
            return {
                'status': 'not_exist',
                'message': 'è®ºæ–‡ä¸å­˜åœ¨',
                'arxiv_id': arxiv_id
            }
    
    except Exception as e:
        return {
            'status': 'error',
            'message': str(e),
            'arxiv_id': arxiv_id
        }

def batch_save_papers_to_db(found_papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ä¿å­˜è®ºæ–‡åˆ°æ•°æ®åº“
    
    Args:
        found_papers: ä»arXivè·å–çš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        
    Returns:
        æ›´æ–°äº†paper_idçš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
    """
    if not found_papers:
        return []
    
    print(f"ğŸ’¾ [æ‰¹é‡ä¿å­˜] å¼€å§‹æ‰¹é‡ä¿å­˜ {len(found_papers)} ç¯‡è®ºæ–‡åˆ°æ•°æ®åº“")
    
    try:
        today_str = datetime.now().strftime('%Y-%m-%d')  # ä»…ä½œä¸ºfallback
        
        # 1. å‡†å¤‡è®ºæ–‡æ•°æ®
        paper_rows = []
        for paper in found_papers:
            # ä½¿ç”¨è®ºæ–‡çš„å®é™…æ›´æ–°æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä»Šå¤©
            update_date = paper.get('update_date', today_str)
            
            paper_row = {
                'arxiv_id': paper['arxiv_id'],
                'title': paper['title'],
                'authors': paper['authors'],
                'abstract': paper['abstract'],
                'link': paper['link'],
                'update_date': update_date,
                'primary_category': paper.get('categories', [''])[0] if paper.get('categories') else None,
                'author_affiliation': paper.get('author_affiliation', '')
            }
            paper_rows.append(paper_row)
        
        # 2. æ‰¹é‡upsertè®ºæ–‡ï¼Œè·å–arxiv_id -> paper_idæ˜ å°„
        arxiv_to_paper_id = db_repo.upsert_papers_bulk(paper_rows)
        print(f"âœ… [æ‰¹é‡ä¿å­˜] è®ºæ–‡æ‰¹é‡upsertå®Œæˆï¼Œå…±å¤„ç† {len(arxiv_to_paper_id)} æ¡")
        
        # 3. æ”¶é›†æ‰€æœ‰åˆ†ç±»
        all_categories = set()
        for paper in found_papers:
            if paper.get('categories'):
                all_categories.update(paper.get('categories', []))
        
        # 4. æ‰¹é‡upsertåˆ†ç±»
        if all_categories:
            category_list = [cat for cat in all_categories if cat]
            name_to_category_id = db_repo.upsert_categories_bulk(category_list)
            print(f"âœ… [æ‰¹é‡ä¿å­˜] åˆ†ç±»æ‰¹é‡upsertå®Œæˆï¼Œå…±å¤„ç† {len(name_to_category_id)} ä¸ªåˆ†ç±»")
            
            # 5. å‡†å¤‡è®ºæ–‡-åˆ†ç±»å…³è”æ•°æ®
            paper_category_pairs = []
            for paper in found_papers:
                arxiv_id = paper['arxiv_id']
                paper_id = arxiv_to_paper_id.get(arxiv_id)
                if paper_id and paper.get('categories'):
                    for category in paper.get('categories', []):
                        if category and category in name_to_category_id:
                            category_id = name_to_category_id[category]
                            paper_category_pairs.append((paper_id, category_id))
            
            # 6. æ‰¹é‡å…³è”è®ºæ–‡åˆ†ç±»
            if paper_category_pairs:
                db_repo.upsert_paper_categories_bulk(paper_category_pairs)
                print(f"âœ… [æ‰¹é‡ä¿å­˜] è®ºæ–‡åˆ†ç±»å…³è”å®Œæˆï¼Œå…±å¤„ç† {len(paper_category_pairs)} ä¸ªå…³è”")
        
        # 7. æ›´æ–°paper_idåˆ°found_papersä¸­ï¼ˆä¿ç•™åŸæœ‰çš„update_dateï¼‰
        for paper in found_papers:
            paper['paper_id'] = arxiv_to_paper_id.get(paper['arxiv_id'])
            # ä¸è¦è¦†ç›–paper['update_date']ï¼Œä¿æŒarXiv APIè¿”å›çš„çœŸå®æ—¥æœŸ
        
        print(f"ğŸ‰ [æ‰¹é‡ä¿å­˜] æ‰¹é‡ä¿å­˜å…¨éƒ¨å®Œæˆï¼")
        return found_papers
        
    except Exception as e:
        print(f"âŒ [æ‰¹é‡ä¿å­˜] æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
        # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›ç»“æœï¼Œpaper_idä¿æŒä¸ºNone
        return found_papers

def smart_search_papers_optimized(text_content: str) -> Dict[str, Any]:
    """
    æ™ºèƒ½æœç´¢è®ºæ–‡ï¼šè§£ææ–‡æœ¬ä¸­çš„arXiv IDå¹¶æ‰¹é‡è·å–è®ºæ–‡ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        text_content: åŒ…å«arXiv IDçš„æ–‡æœ¬å†…å®¹
        
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯
    """
    start_time = datetime.now()
    print(f"ğŸ” [æ™ºèƒ½æœç´¢ä¼˜åŒ–ç‰ˆ] å¼€å§‹å¤„ç†æ™ºèƒ½æœç´¢è¯·æ±‚ï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
    
    # 1. æå–arXiv ID
    arxiv_ids = extract_arxiv_ids(text_content)
    
    if not arxiv_ids:
        return {
            'success': False,
            'message': 'æœªåœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ä»»ä½•arXiv ID',
            'articles': [],
            'total': 0,
            'performance': {
                'total_time': 0,
                'total_processed': 0,
                'found_count': 0,
                'not_exist_count': 0,
                'error_count': 0
            },
            'not_exist_ids': [],
            'error_details': []
        }
    
    print(f"ğŸ“‹ [æ™ºèƒ½æœç´¢ä¼˜åŒ–ç‰ˆ] æå–åˆ° {len(arxiv_ids)} ä¸ªarXiv ID: {arxiv_ids}")
    
    # 2. æ‰¹é‡è·å–è®ºæ–‡ä¿¡æ¯ï¼ˆä¸€æ¬¡APIè°ƒç”¨ï¼‰
    batch_start = datetime.now()
    batch_result = fetch_arxiv_papers_batch(arxiv_ids)
    batch_end = datetime.now()
    api_time = (batch_end - batch_start).total_seconds()
    
    print(f"â±ï¸  [APIæ€§èƒ½] æ‰¹é‡APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {api_time:.2f}s")
    
    if batch_result['status'] != 'success':
        return {
            'success': False,
            'message': batch_result.get('message', 'æ‰¹é‡è·å–è®ºæ–‡ä¿¡æ¯å¤±è´¥'),
            'articles': [],
            'total': 0,
            'performance': {
                'total_time': api_time,
                'total_processed': len(arxiv_ids),
                'found_count': 0,
                'not_exist_count': 0,
                'error_count': len(arxiv_ids)
            },
            'not_exist_ids': [],
            'error_details': batch_result.get('error_ids', [])
        }
    
    # 3. æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
    db_start = datetime.now()
    found_papers = batch_save_papers_to_db(batch_result['found_papers'])
    db_end = datetime.now()
    db_time = (db_end - db_start).total_seconds()
    
    print(f"â±ï¸  [DBæ€§èƒ½] æ‰¹é‡æ•°æ®åº“æ“ä½œå®Œæˆï¼Œè€—æ—¶: {db_time:.2f}s")
    
    # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    found_count = len(found_papers)
    not_exist_count = len(batch_result['not_exist_ids'])
    error_count = len(batch_result['error_ids'])
    
    print(f"âœ… [æ™ºèƒ½æœç´¢ä¼˜åŒ–ç‰ˆ] æœç´¢å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}s")
    print(f"   ğŸ“Š ç»Ÿè®¡: æ€»è®¡{len(arxiv_ids)}ç¯‡ï¼ŒæˆåŠŸ{found_count}ç¯‡ï¼Œå¤±è´¥{not_exist_count}ç¯‡ï¼Œé”™è¯¯{error_count}ç¯‡")
    print(f"   âš¡ æ€§èƒ½: APIè°ƒç”¨{api_time:.2f}s + DBæ“ä½œ{db_time:.2f}s")
    
    # æ„å»ºä¸search_articlesä¸€è‡´çš„è¿”å›ç»“æ„
    return {
        'success': True,
        'articles': found_papers,
        'total': found_count,
        'search_type': 'smart_search_optimized',
        'performance': {
            'total_time': round(total_time, 2),
            'api_time': round(api_time, 2),
            'db_time': round(db_time, 2),
            'total_processed': len(arxiv_ids),
            'found_count': found_count,
            'not_exist_count': not_exist_count,
            'error_count': error_count
        },
        'not_exist_ids': batch_result['not_exist_ids'],
        'error_details': batch_result['error_ids'],
        'message': f'æ™ºèƒ½æœç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {found_count} ç¯‡è®ºæ–‡ä¿¡æ¯'
    }

def smart_search_papers(text_content: str, delay: float = None) -> Dict[str, Any]:
    """
    æ™ºèƒ½æœç´¢è®ºæ–‡ï¼šè§£ææ–‡æœ¬ä¸­çš„arXiv IDå¹¶æ‰¹é‡è·å–è®ºæ–‡ä¿¡æ¯
    ä¿ç•™åŸå‡½æ•°åä»¥ä¿æŒå‘åå…¼å®¹æ€§ï¼Œå†…éƒ¨è°ƒç”¨ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        text_content: åŒ…å«arXiv IDçš„æ–‡æœ¬å†…å®¹
        delay: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼Œä¼˜åŒ–ç‰ˆæœ¬ä¸­å·²å¿½ç•¥ï¼‰
        
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯
    """
    return smart_search_papers_optimized(text_content)