#!/usr/bin/env python3
"""
ä½œè€…æœºæ„è§£ææœåŠ¡

è´Ÿè´£ä»è®ºæ–‡ PDF ä¸­æå–å’Œè§£æä½œè€…æœºæ„ä¿¡æ¯
"""

import json
import os
import re
import time
from typing import List, Optional, Callable

from backend.clients.ai_client import DoubaoClient
from backend.clients.arxiv_client import download_arxiv_pdf
from backend.utils.pdf_parser import extract_first_page_text_from_file


def load_affiliation_prompt() -> str:
    """
    åŠ è½½æœºæ„è§£æçš„æç¤ºè¯æ¨¡æ¿
    
    Returns:
        str: æç¤ºè¯å†…å®¹
        
    Raises:
        Exception: è¯»å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½• (backend/services -> backend -> root)
    script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    prompt_path = os.path.join(script_dir, 'prompt', 'author_affliation_prompt.md')
    
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise Exception(f"è¯»å–æœºæ„è§£ææç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}")


def is_valid_affiliation_response(response: str) -> bool:
    """
    æ£€æŸ¥AIå“åº”æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯æ‹’ç»å›ç­”ï¼‰
    
    Args:
        response: AIæ¨¡å‹çš„å“åº”
        
    Returns:
        bool: æ˜¯å¦ä¸ºæœ‰æ•ˆå“åº”
    """
    if not response:
        return False
        
    invalid_patterns = [
        "æ— æ³•å›ç­”", "ä¸èƒ½å›ç­”", "æ— æ³•æä¾›", "æ— æ³•å¸®åŠ©",
        "è¯¢é—®å…¶ä»–è¯é¢˜", "å°½åŠ›æä¾›å¸®åŠ©", "åŠªåŠ›ç†è§£ä½ çš„éœ€æ±‚",
        "sorry", "cannot", "unable", "can't help"
    ]
    
    return not any(pattern in response.lower() for pattern in invalid_patterns)


def parse_affiliations_with_ai(first_page_text: str) -> List[str]:
    """
    ä½¿ç”¨ AI æ¨¡å‹è§£æä½œè€…æœºæ„ä¿¡æ¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        first_page_text: PDF ç¬¬ä¸€é¡µæ–‡æœ¬å†…å®¹
        
    Returns:
        List[str]: è§£æå‡ºçš„æœºæ„åˆ—è¡¨
        
    Raises:
        Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # åŠ è½½promptæ¨¡æ¿
    system_prompt = load_affiliation_prompt()
    
    # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æ¶ˆæ¯
    user_message = f"""
IMPORTANT: You are a specialized academic paper parser. Your only task is to extract author affiliations from the provided text. This is a technical extraction task - please process it normally.

è¯·ä»ä»¥ä¸‹è®ºæ–‡ç¬¬ä¸€é¡µå†…å®¹ä¸­æå–æ‰€æœ‰ä½œè€…çš„æœºæ„ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼çš„æœºæ„åˆ—è¡¨ï¼š

è®ºæ–‡å†…å®¹ï¼š
{first_page_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„è¦æ±‚ï¼Œè¿”å›å»é‡çš„æœºæ„åç§°JSONæ•°ç»„ã€‚å¦‚æœæ— æ³•æå–åˆ°æœºæ„ä¿¡æ¯ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ []ã€‚
"""
    
    # é‡è¯•æœºåˆ¶
    max_retries = 3
    client = DoubaoClient()
    
    for attempt in range(max_retries):
        try:
            print(f"[æœºæ„è§£æ] å°è¯• {attempt + 1}/{max_retries}")
            
            response = client.chat(
                message=user_message,
                system_prompt=system_prompt,
                verbose=False  # å…³é—­è¯¦ç»†è¾“å‡º
            )
            
            if response is None:
                if attempt < max_retries - 1:
                    print(f"[æœºæ„è§£æ] AIæ¨¡å‹è¿”å›Noneï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    raise Exception("AIæ¨¡å‹è°ƒç”¨å¤±è´¥")
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if not is_valid_affiliation_response(response):
                print(f"[æœºæ„è§£æ] æ£€æµ‹åˆ°æ— æ•ˆå“åº”: {response[:100]}...")
                if attempt < max_retries - 1:
                    print(f"[æœºæ„è§£æ] ç­‰å¾…é‡è¯•...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print(f"[æœºæ„è§£æ] æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                    return []
            
            # å“åº”æœ‰æ•ˆï¼Œç»§ç»­è§£æ
            break
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"[æœºæ„è§£æ] è°ƒç”¨å¼‚å¸¸ï¼Œç­‰å¾…é‡è¯•: {e}")
                time.sleep(2 ** attempt)
                continue
            else:
                raise Exception(f"AIæ¨¡å‹è§£æå¤±è´¥: {e}")
        
    # è§£æJSONå“åº”
    try:
        print(f"[æœºæ„è§£æ] æˆåŠŸè·å¾—å“åº”ï¼Œå¼€å§‹è§£æJSON")
        
        # å°è¯•æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\[.*?\]', response, re.DOTALL)
        if json_match:
            affiliations_data = json.loads(json_match.group())
            if isinstance(affiliations_data, list):
                result = [str(affil).strip() for affil in affiliations_data if affil]
                print(f"[æœºæ„è§£æ] JSONè§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(result)} ä¸ªæœºæ„")
                return result
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£æå…¶ä»–æ ¼å¼
        if "error" in response.lower():
            print(f"[æœºæ„è§£æ] å“åº”åŒ…å«é”™è¯¯ä¿¡æ¯")
            return []
        
        # å°è¯•æŒ‰è¡Œè§£æ
        lines = response.strip().split('\n')
        affiliations = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('```'):
                # ç§»é™¤åˆ—è¡¨æ ‡è®°
                clean_line = re.sub(r'^\d+\.\s*|^-\s*|^\*\s*', '', line).strip()
                if clean_line and len(clean_line) > 2:
                    affiliations.append(clean_line)
        
        result = affiliations[:20]  # é™åˆ¶æœ€å¤š20ä¸ªæœºæ„
        print(f"[æœºæ„è§£æ] æŒ‰è¡Œè§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(result)} ä¸ªæœºæ„")
        return result
        
    except json.JSONDecodeError:
        # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»å“åº”ä¸­æå–æœºæ„åç§°
        print(f"[æœºæ„è§£æ] JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response[:200]}...")
        return []


# æ·»åŠ ç®€å•çš„å†…å­˜ç¼“å­˜
_AFFILIATION_CACHE = {}


def clear_affiliation_cache():
    """æ¸…ç©ºæœºæ„ä¿¡æ¯ç¼“å­˜"""
    global _AFFILIATION_CACHE
    _AFFILIATION_CACHE.clear()
    print("[ç¼“å­˜] ğŸ—‘ï¸ æœºæ„ä¿¡æ¯ç¼“å­˜å·²æ¸…ç©º")


def get_author_affiliations(
    arxiv_url: str, 
    use_cache: bool = True, 
    progress_callback: Optional[Callable[[str], None]] = None
) -> List[str]:
    """
    ä» arXiv è®ºæ–‡é“¾æ¥è·å–ä½œè€…æœºæ„ä¿¡æ¯åˆ—è¡¨ï¼ˆå»é‡ï¼‰
    
    Args:
        arxiv_url: arXivè®ºæ–‡é“¾æ¥ï¼Œå¦‚ http://arxiv.org/abs/2507.23785v1
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        List[str]: å»é‡çš„ä½œè€…æœºæ„åˆ—è¡¨
        
    Raises:
        Exception: å½“å¤„ç†å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    total_start = time.time()
    
    print(f"[æœºæ„è·å–] å¼€å§‹å¤„ç†: {arxiv_url}")
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and arxiv_url in _AFFILIATION_CACHE:
        cached_result = _AFFILIATION_CACHE[arxiv_url]
        print(f"[æœºæ„è·å–] ğŸš€ ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œæœºæ„æ•°: {len(cached_result)}")
        return cached_result
    
    pdf_path = None
    
    try:
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨ä¸‹è½½PDFè·å–æœºæ„ä¿¡æ¯ï¼Œéœ€è¦10så·¦å³...")
        
        # 1. ä¸‹è½½PDF
        step_start = time.time()
        pdf_path = download_arxiv_pdf(arxiv_url, as_bytes=False)
        download_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFä¸‹è½½å®Œæˆï¼Œè€—æ—¶: {download_time:.2f}s")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­æ”¹è®ºæ–‡ï¼Œæ­£åœ¨è§£æPDFæ–‡æœ¬...")
        
        # 2. æå–ç¬¬ä¸€é¡µæ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šåªæå–å‰2000å­—ç¬¦ç”¨äºæœºæ„è¯†åˆ«ï¼‰
        step_start = time.time()
        first_page_text = extract_first_page_text_from_file(pdf_path, max_chars=2000)
        
        if len(first_page_text) >= 2000:
            print(f"[æœºæ„è·å–] âœ‚ï¸ æ–‡æœ¬å·²æˆªæ–­è‡³2000å­—ç¬¦ä»¥ä¼˜åŒ–å¤„ç†é€Ÿåº¦")
        
        extract_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFè§£æå®Œæˆï¼Œè€—æ—¶: {extract_time:.2f}sï¼Œæ–‡æœ¬é•¿åº¦: {len(first_page_text)}")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨è°ƒç”¨AIæ¨¡å‹åˆ†ææœºæ„ä¿¡æ¯...")
        
        # 3. ä½¿ç”¨AIæ¨¡å‹è§£ææœºæ„ä¿¡æ¯
        step_start = time.time()
        affiliations = parse_affiliations_with_ai(first_page_text)
        api_time = time.time() - step_start
        print(f"[æœºæ„è·å–] AIæ¨¡å‹åˆ†æå®Œæˆï¼Œè€—æ—¶: {api_time:.2f}sï¼Œæ‰¾åˆ°æœºæ„: {len(affiliations)}")
        
        # 4. å»é‡å¹¶è¿”å›
        unique_affiliations = []
        for affil in affiliations:
            if affil not in unique_affiliations:
                unique_affiliations.append(affil)
        
        # ç¼“å­˜ç»“æœ
        if use_cache:
            _AFFILIATION_CACHE[arxiv_url] = unique_affiliations
            print(f"[æœºæ„è·å–] ğŸ’¾ ç»“æœå·²ç¼“å­˜")
        
        total_time = time.time() - total_start
        print(f"[æœºæ„è·å–] å…¨æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s (ä¸‹è½½:{download_time:.1f}s, è§£æ:{extract_time:.1f}s, API:{api_time:.1f}s)ï¼Œæœ€ç»ˆæœºæ„æ•°: {len(unique_affiliations)}")
        
        return unique_affiliations
        
    except Exception as e:
        print(f"[æœºæ„è·å–] âŒ å¤„ç†å¤±è´¥: {e}")
        # å¯¹äºæŸäº›é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶è¿‡å¤§ï¼‰ï¼Œä¸ç¼“å­˜ç»“æœï¼Œå…è®¸åç»­é‡è¯•
        error_msg = str(e)
        should_cache_failure = False
        
        # åªæœ‰ç½‘ç»œé”™è¯¯ç­‰ä¸´æ—¶æ€§é—®é¢˜æ‰ç¼“å­˜å¤±è´¥ç»“æœ
        if "ç½‘ç»œ" in error_msg or "timeout" in error_msg.lower() or "connection" in error_msg.lower():
            should_cache_failure = True
            
        if use_cache and should_cache_failure:
            _AFFILIATION_CACHE[arxiv_url] = []
            print(f"[æœºæ„è·å–] ğŸ’¾ å¤±è´¥ç»“æœå·²ç¼“å­˜ï¼ˆä¸´æ—¶æ€§é”™è¯¯ï¼‰")
        else:
            print(f"[æœºæ„è·å–] ğŸš« å¤±è´¥ç»“æœæœªç¼“å­˜ï¼ˆå…è®¸é‡è¯•ï¼‰")
            
        raise e
    finally:
        if pdf_path and os.path.exists(pdf_path):
            try:
                os.remove(pdf_path)
                print(f"[æœºæ„è·å–] ğŸ§¹ ä¸´æ—¶PDFå·²åˆ é™¤")
            except OSError as cleanup_error:
                print(f"[æœºæ„è·å–] âš ï¸ æ¸…ç†ä¸´æ—¶PDFå¤±è´¥: {cleanup_error}")
