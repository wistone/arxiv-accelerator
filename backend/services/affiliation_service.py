#!/usr/bin/env python3
"""
ä½œè€…æœºæ„è§£ææœåŠ¡

è´Ÿè´£ä»è®ºæ–‡ PDF ä¸­æå–å’Œè§£æä½œè€…æœºæ„ä¿¡æ¯
"""

import json
import os
import re
import time
from typing import List, Optional, Callable

from backend.clients.ai_client import DoubaoClient
from backend.clients.arxiv_client import download_arxiv_pdf
from backend.utils.pdf_parser import extract_first_page_text


def load_affiliation_prompt() -> str:
    """
    åŠ è½½æœºæ„è§£æçš„æç¤ºè¯æ¨¡æ¿
    
    Returns:
        str: æç¤ºè¯å†…å®¹
        
    Raises:
        Exception: è¯»å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½• (backend/services -> backend -> root)
    script_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    prompt_path = os.path.join(script_dir, 'prompt', 'author_affliation_prompt.md')
    
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise Exception(f"è¯»å–æœºæ„è§£ææç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}")


def parse_affiliations_with_ai(first_page_text: str) -> List[str]:
    """
    ä½¿ç”¨ AI æ¨¡å‹è§£æä½œè€…æœºæ„ä¿¡æ¯
    
    Args:
        first_page_text: PDF ç¬¬ä¸€é¡µæ–‡æœ¬å†…å®¹
        
    Returns:
        List[str]: è§£æå‡ºçš„æœºæ„åˆ—è¡¨
        
    Raises:
        Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # åŠ è½½promptæ¨¡æ¿
    system_prompt = load_affiliation_prompt()
    
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_message = f"""
è¯·ä»ä»¥ä¸‹è®ºæ–‡ç¬¬ä¸€é¡µå†…å®¹ä¸­æå–æ‰€æœ‰ä½œè€…çš„æœºæ„ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼çš„æœºæ„åˆ—è¡¨ï¼š

è®ºæ–‡å†…å®¹ï¼š
{first_page_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„è¦æ±‚ï¼Œè¿”å›å»é‡çš„æœºæ„åç§°JSONæ•°ç»„ã€‚
"""
    
    # è°ƒç”¨AIæ¨¡å‹
    try:
        client = DoubaoClient()
        response = client.chat(
            message=user_message,
            system_prompt=system_prompt,
            verbose=False  # å…³é—­è¯¦ç»†è¾“å‡º
        )
        
        if response is None:
            raise Exception("AIæ¨¡å‹è°ƒç”¨å¤±è´¥")
        
        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                affiliations_data = json.loads(json_match.group())
                if isinstance(affiliations_data, list):
                    return [str(affil).strip() for affil in affiliations_data if affil]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£æå…¶ä»–æ ¼å¼
            if "error" in response.lower():
                return []
            
            # å°è¯•æŒ‰è¡Œè§£æ
            lines = response.strip().split('\n')
            affiliations = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('```'):
                    # ç§»é™¤åˆ—è¡¨æ ‡è®°
                    clean_line = re.sub(r'^\d+\.\s*|^-\s*|^\*\s*', '', line).strip()
                    if clean_line and len(clean_line) > 2:
                        affiliations.append(clean_line)
            
            return affiliations[:20]  # é™åˆ¶æœ€å¤š20ä¸ªæœºæ„
            
        except json.JSONDecodeError:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»å“åº”ä¸­æå–æœºæ„åç§°
            print(f"JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response}")
            return []
            
    except Exception as e:
        raise Exception(f"AIæ¨¡å‹è§£æå¤±è´¥: {e}")


# ä½¿ç”¨é™åˆ¶å¤§å°çš„ç¼“å­˜æ›¿ä»£æ— é™å¢é•¿çš„å­—å…¸
from backend.utils.memory_manager import LimitedCache
_AFFILIATION_CACHE = LimitedCache(max_size=200)  # æœ€å¤šç¼“å­˜200ä¸ªæœºæ„ç»“æœ


def clear_affiliation_cache():
    """æ¸…ç©ºæœºæ„ä¿¡æ¯ç¼“å­˜"""
    global _AFFILIATION_CACHE
    _AFFILIATION_CACHE.clear()
    print("[ç¼“å­˜] ğŸ—‘ï¸ æœºæ„ä¿¡æ¯ç¼“å­˜å·²æ¸…ç©º")


def get_author_affiliations(
    arxiv_url: str, 
    use_cache: bool = True, 
    progress_callback: Optional[Callable[[str], None]] = None
) -> List[str]:
    """
    ä» arXiv è®ºæ–‡é“¾æ¥è·å–ä½œè€…æœºæ„ä¿¡æ¯åˆ—è¡¨ï¼ˆå»é‡ï¼‰
    
    Args:
        arxiv_url: arXivè®ºæ–‡é“¾æ¥ï¼Œå¦‚ http://arxiv.org/abs/2507.23785v1
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        List[str]: å»é‡çš„ä½œè€…æœºæ„åˆ—è¡¨
        
    Raises:
        Exception: å½“å¤„ç†å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    total_start = time.time()
    
    print(f"[æœºæ„è·å–] å¼€å§‹å¤„ç†: {arxiv_url}")
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache:
        cached_result = _AFFILIATION_CACHE.get(arxiv_url)
        if cached_result is not None:
            print(f"[æœºæ„è·å–] ğŸš€ ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œæœºæ„æ•°: {len(cached_result)}")
            return cached_result
    
    try:
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨ä¸‹è½½PDFè·å–æœºæ„ä¿¡æ¯ï¼Œéœ€è¦10så·¦å³...")
        
        # 1. ä¸‹è½½PDF
        step_start = time.time()
        pdf_content = download_arxiv_pdf(arxiv_url)
        download_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFä¸‹è½½å®Œæˆï¼Œè€—æ—¶: {download_time:.2f}s")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­æ”¹è®ºæ–‡ï¼Œæ­£åœ¨è§£æPDFæ–‡æœ¬...")
        
        # 2. æå–ç¬¬ä¸€é¡µæ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šåªæå–å‰2000å­—ç¬¦ç”¨äºæœºæ„è¯†åˆ«ï¼‰
        step_start = time.time()
        first_page_text = extract_first_page_text(pdf_content, max_chars=2000)
        
        if len(first_page_text) >= 2000:
            print(f"[æœºæ„è·å–] âœ‚ï¸ æ–‡æœ¬å·²æˆªæ–­è‡³2000å­—ç¬¦ä»¥ä¼˜åŒ–å¤„ç†é€Ÿåº¦")
        
        extract_time = time.time() - step_start
        print(f"[æœºæ„è·å–] PDFè§£æå®Œæˆï¼Œè€—æ—¶: {extract_time:.2f}sï¼Œæ–‡æœ¬é•¿åº¦: {len(first_page_text)}")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback("åˆ†æåå·²é€‰ä¸­è¯¥è®ºæ–‡ï¼Œæ­£åœ¨è°ƒç”¨AIæ¨¡å‹åˆ†ææœºæ„ä¿¡æ¯...")
        
        # 3. ä½¿ç”¨AIæ¨¡å‹è§£ææœºæ„ä¿¡æ¯
        step_start = time.time()
        affiliations = parse_affiliations_with_ai(first_page_text)
        api_time = time.time() - step_start
        print(f"[æœºæ„è·å–] AIæ¨¡å‹åˆ†æå®Œæˆï¼Œè€—æ—¶: {api_time:.2f}sï¼Œæ‰¾åˆ°æœºæ„: {len(affiliations)}")
        
        # 4. å»é‡å¹¶è¿”å›
        unique_affiliations = []
        for affil in affiliations:
            if affil not in unique_affiliations:
                unique_affiliations.append(affil)
        
        # ç¼“å­˜ç»“æœ
        if use_cache:
            _AFFILIATION_CACHE.put(arxiv_url, unique_affiliations)
            print(f"[æœºæ„è·å–] ğŸ’¾ ç»“æœå·²ç¼“å­˜")
        
        total_time = time.time() - total_start
        print(f"[æœºæ„è·å–] å…¨æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s (ä¸‹è½½:{download_time:.1f}s, è§£æ:{extract_time:.1f}s, API:{api_time:.1f}s)ï¼Œæœ€ç»ˆæœºæ„æ•°: {len(unique_affiliations)}")
        
        return unique_affiliations
        
    except Exception as e:
        print(f"[æœºæ„è·å–] âŒ å¤„ç†å¤±è´¥: {e}")
        # å¯¹äºæŸäº›é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶è¿‡å¤§ï¼‰ï¼Œä¸ç¼“å­˜ç»“æœï¼Œå…è®¸åç»­é‡è¯•
        error_msg = str(e)
        should_cache_failure = False
        
        # åªæœ‰ç½‘ç»œé”™è¯¯ç­‰ä¸´æ—¶æ€§é—®é¢˜æ‰ç¼“å­˜å¤±è´¥ç»“æœ
        if "ç½‘ç»œ" in error_msg or "timeout" in error_msg.lower() or "connection" in error_msg.lower():
            should_cache_failure = True
            
        if use_cache and should_cache_failure:
            _AFFILIATION_CACHE.put(arxiv_url, [])
            print(f"[æœºæ„è·å–] ğŸ’¾ å¤±è´¥ç»“æœå·²ç¼“å­˜ï¼ˆä¸´æ—¶æ€§é”™è¯¯ï¼‰")
        else:
            print(f"[æœºæ„è·å–] ğŸš« å¤±è´¥ç»“æœæœªç¼“å­˜ï¼ˆå…è®¸é‡è¯•ï¼‰")
            
        raise e
